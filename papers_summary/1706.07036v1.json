{
    "basic_info": {
        "title": "Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction",
        "authors": [
            "Chen-Hsuan Lin",
            "Chen Kong",
            "Simon Lucey"
        ],
        "paper_id": "1706.07036v1",
        "published_year": 2017,
        "references": []
    },
    "raw_chunks": [
        {
            "text": "3 Approach\n\nOur goal is to generate 3D predictions that compactly shape the surface geometry with dense point clouds. The overall pipeline is illustrated in Fig. 1. We start with an encoder that maps the input data to a latent representation space. The encoder may take on various forms of data depending on the application; in our experiments, we focus on encoding RGB images for single-image 3D reconstruction tasks. From the latent representation, we propose to generate the dense point clouds using a structure generator based on 2D convolutions with a joint 2D projection criterion, described in detail as follows.\n\n3.1 Structure Generator\n\nThe structure generator predicts the 3D structure of the object at V different viewpoints (along with their binary masks), i.e. the 3D coordinates &; = [%; 9 ait at each pixel location. Pixel values in natural images can be synthesized through convolutional generative models mainly due to their exhibition of strong local spatial dependencies; similar phenomenons can be observed for point clouds when treating them as (x, y, z) multi-channel images on a 2D grid. Based on this insight, the structure generator is mainly based on 2D convolutional operations to predict the (x, y, z) images representing the 3D surface geometry. This approach circumvents the need of time-consuming and memory-expensive 3D convolutional operations for volumetric predictions. The evidence of such validity is verified in our experimental results.\n\nAssuming the 3D rigid transformation matrices of the N viewpoints (R1,t1)...(RN,tN) are given a priori, each 3D point ˆxi at viewpoint n can be transformed to the canonical 3D coordinates as ˆpi via\n\nVi,\n\nˆpi = R−1\n\n(1)\n\nn\n\nwhere K is the predeﬁned camera intrinsic matrix. This deﬁnes the relationship between the predicted 3D points and the fused collection of point clouds in the canonical 3D coordinates, which is the outcome of our network.\n\n3.2 Joint 2D Projection Optimization\n\nTo learn point cloud generation using the provided 3D CAD models as supervision, the standard approach would be to optimize over a 3D-based metric that deﬁnes the distance between the point cloud and the ground-truth CAD model (e.g. Chamfer distance [4]). Such metric usually involves computing surface projections for every generated point, which can be computationally expensive for very dense predictions, making it intractable.\n\n3\n\nGround truth\n\nFigure 2: Concept of pseudo-rendering. Multiple transformed 3D points may correspond to projection on the same pixels in the image space. (a) Collision could easily occur if (4%, 9) were directly discretized. (b) Upsampling the target image increases the precision of the projection locations and thus alleviates the collision effect. A max-pooling operation on the inverse depth values follows as to obtain the original resolution while maintaining the effective depth value at each pixel. (c) Examples of pseudo-rendered depth images with various upsampling factors U (only valid depth values without collision are shown). Pseudo-rendering achieves closer performance to true rendering with a higher value of U.\n\nWe overcome this issue by alternatively optimizing over the joint 2D projection error of novel viewpoints. Instead of using only projected binary masks as supervision [31, 5, 22], we conjecture that a well-generated 3D shape should also have the ability to render reasonable depth images from any viewpoint. To realize this concept, we introduce the pseudo-renderer, a differentiable module to approximate true rendering, to synthesize novel depth images from dense point clouds.\n\nPseudo-rendering. Given the 3D rigid transformation matrix of a novel viewpoint (Rx, t;), each canonical 3D point p; can be further transformed to x; back in the image coordinates via",
            "section": "methodology",
            "section_idx": 0,
            "citations": [
                "4",
                "31, 5, 22"
            ]
        },
        {
            "text": "4 Experiments\n\nWe evaluate our proposed method by analyzing its performance in the application of single-image 3D reconstruction and comparing against state-of-the-art methods.\n\nData preparation. We train and evaluate all networks using the ShapeNet database [1], which contains a large collection of categorized 3D CAD models. For each CAD model, we pre-render 100 depth/mask image pairs of size 128×128 at random novel viewpoints as the ground truth of the loss function. We consider the entire space of possible 3D rotations (including in-plane rotation) for the viewpoints and assume identity translation for simplicity. The input images are objects pre-rendered from a ﬁxed elevation and 24 different azimuth angles.\n\nArchitectural details. The structure generator follows the structure of conventional deep generative models, consisting of linear layers followed by 2D convolution layers (with kernel size 3×3). The dimensions of all feature maps are halved after each encoder convolution and doubled after each generator convolution. Details of network dimensions are listed in Table 1. At the end of the decoder, we add an extra convolution layer with ﬁlters of size 1×1 to encourage individuality of the generated pixels. Batch normalization [10] and ReLU activations are added between all layers.\n\nThe generator predicts N = 8 images of size 128×128 with 4 channels (x, y, z and the binary mask), where the ﬁxed viewpoints are chosen from the 8 corners of a centered cube. Orthographic projection is assumed in the transformation in (1) and (2). We use U = 5 for the upsampling factor of the pseudo-renderer in our experiments.\n\nTraining details. All networks are optimized using the Adam optimizer [13]. We take a two-stage training procedure: the structure generator is ﬁrst pretrained to predict the depth images from the N viewpoints (with a constant learning rate of 1e-2), and then the entire network is ﬁne-tuned with joint 2D projection optimization (with a constant learning rate of 1e-4). For the training parameters, we set λ = 1.0 and K = 5.\n\nQuantitative metrics. We measure using the average point-wise 3D Euclidean distance between two 3D models: for each point p; in the source model, the distance to the target model S is defined as &; = minp,es ||pi — pj||. This metric is defined bidirectionally as the distance from the predicted point cloud to the ground-truth CAD model and vice versa. It is necessary to report both metrics for they represent different aspects of quality — the former measures 3D shape similarity and the latter measures surface coverage [15]. We represent the ground-truth CAD models as collections of uniformly densified 3D points on the surfaces (100K densified points in our settings).",
            "section": "methodology",
            "section_idx": 1,
            "citations": [
                "1",
                "10",
                "13",
                "15"
            ]
        },
        {
            "text": "5 Conclusion\n\nIn this paper, we introduced a framework for generating 3D shapes in the form of dense point clouds. Compared to conventional volumetric prediction methods using 3D ConvNets, it is more efﬁcient to utilize 2D convolutional operations to predict surface information of 3D shapes. We showed that by introducing a pseudo-renderer, we are able to synthesize approximate depth images from novel viewpoints to optimize the 2D projection error within a backpropagation framework. Experimental results for single-image 3D reconstruction tasks showed that we generate more accurate and much denser 3D shapes than state-of-the-art 3D reconstruction methods.\n\n8\n\nReferences\n\n[1] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n\n[2] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-r2n2: A uniﬁed approach for single and multi- view 3d object reconstruction. In European Conference on Computer Vision, pages 628–644. Springer, 2016.\n\n[3] A. Dosovitskiy, J. Tobias Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1538–1546, 2015.\n\n[4] H. Fan, H. Su, and L. Guibas. A point set generation network for 3d object reconstruction from a single image. arXiv preprint arXiv:1612.00603, 2016.\n\n[5] M. Gadelha, S. Maji, and R. Wang. 3d shape induction from 2d views of multiple objects. arXiv preprint arXiv:1612.05872, 2016.\n\n[6] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta. Learning a predictable and generative vector representation for objects. In European Conference on Computer Vision, pages 484–499. Springer, 2016.\n\n[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680, 2014.\n\n[8] C. Häne, S. Tulsiani, and J. Malik. Hierarchical surface prediction for 3d object reconstruction. arXiv preprint arXiv:1704.00710, 2017.\n\n[9] V. Hegde and R. Zadeh. Fusionnet: 3d object classiﬁcation using multiple data representations. arXiv preprint arXiv:1607.05695, 2016.\n\n[10] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. pages 448–456, 2015.\n\n[11] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint arXiv:1611.07004, 2016.\n\n[12] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In Advances in Neural Information Processing Systems, pages 2017–2025, 2015.\n\n[13] D. Kingma and J. Ba. Adam: A method for stochastic optimization. 2015.\n\n[14] D. P. Kingma and M. Welling. Auto-encoding variational bayes. 2013.\n\n[15] C. Kong, C.-H. Lin, and S. Lucey. Using locally corresponding cad models for dense 3d reconstructions from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n\n[16] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum. Deep convolutional inverse graphics network. In Advances in Neural Information Processing Systems, pages 2539–2547, 2015.\n\n[17] C.-H. Lin and S. Lucey. Inverse compositional spatial transformer networks. arXiv preprint arXiv:1612.03897, 2016.\n\n[18] D. Maturana and S. Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pages 922–928. IEEE, 2015.\n\n[19] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C. Berg. Transformation-grounded image generation network for novel 3d view synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n\n[20] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\n[21] S. E. Reed, Y. Zhang, Y. Zhang, and H. Lee. Deep visual analogy-making. In Advances in Neural Information Processing Systems, pages 1252–1260, 2015.\n\n[22] D. J. Rezende, S. A. Eslami, S. Mohamed, P. Battaglia, M. Jaderberg, and N. Heess. Unsupervised learning of 3d structure from images. In Advances In Neural Information Processing Systems, pages 4997–5005, 2016.\n\n[23] G. Riegler, A. O. Ulusoys, and A. Geiger. Octnet: Learning deep 3d representations at high resolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n\n[24] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3d models from single images with a convolu- tional network. In European Conference on Computer Vision, pages 322–337. Springer, 2016.\n\n[25] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks: Efﬁcient convolutional architectures for high-resolution 3d outputs. arXiv preprint arXiv:1703.09438, 2017.\n\n[26] S. Tulsiani, T. Zhou, A. A. Efros, and J. Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. arXiv preprint arXiv:1704.06254, 2017.\n\n[27] X. Wang and A. Gupta. Generative image modeling using style and structure adversarial networks. In European Conference on Computer Vision, pages 318–335. Springer, 2016.\n\n9\n\n[28] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural Information Processing Systems, pages 82–90, 2016.\n\n[29] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912–1920, 2015.\n\n[30] X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image: Conditional image generation from visual attributes. In European Conference on Computer Vision, pages 776–791. Springer, 2016.\n\n[31] X. Yan, J. Yang, E. Yumer, Y. Guo, and H. Lee. Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision. In Advances in Neural Information Processing Systems, pages 1696–1704, 2016.\n\n[32] J. Yang, S. E. Reed, M.-H. Yang, and H. Lee. Weakly-supervised disentangling with recurrent transforma- tions for 3d view synthesis. In Advances in Neural Information Processing Systems, pages 1099–1107, 2015.\n\n[33] M. E. Yumer and N. J. Mitra. Learning semantic deformation ﬂows with 3d convolutional networks. In European Conference on Computer Vision, pages 294–311. Springer, 2016.",
            "section": "results",
            "section_idx": 0,
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33"
            ]
        },
        {
            "text": "4.3 Generative Representation Analysis\n\nWe analyze the learned generative representations by observing the 3D predictions from manipulation in the latent space. Previous works have demonstrated that deep generative networks can generate meaningful pixel/voxel predictions by performing linear operations in the latent space [20, 3, 28]; here, we explore the possibility of such manipulation for dense point clouds in an undiscretized space.\n\nWe show in Fig. 5 the resulting dense shapes generated from the embedding vector interpolated in the latent space. The morphing transition is smooth with plausible interpolated shapes, which suggests that our structure generator can generate meaningful 3D predictions from convex combinations of encoded latent vectors. The structure generator is also capable of generating reasonable novel shapes from arithmetic results in the latent space — from Fig. 6) we observe semantic feature replacement of table height/shape as well as chair arms/backs. These results suggest that the high-level semantic information encoded in the latent vectors are manipulable and interpretable of the resulting dense point clouds through the structure generator.\n\n7\n\ncategory airplane\n\ninput oe\n\nground- truth CAD model\n\nbench\n\ncar\n\ndisplay\n\nwatercraft\n\n7454 pts.\n\n24346 pts.\n\n—--22588 pts.\n\n11146 pts.\n\n39923 pts.\n\nFan etal.\n\n1024 pts.\n\n1024 pts.\n\n1024 pts.\n\n1024 pt\n\n1024 pts.\n\n1024 pts.\n\n1024 pts.\n\n1024 pts.\n\n3D-R2N2 (1 view)\n\nRosy\n\n269 pts.\n\n627 pts.\n\n1099 pts.\n\n422 pts.\n\n1996 pts.\n\n1742 pts.\n\n729 pts.\n\nFigure 4: Qualitative results from the multi-category experiment. Our method generates denser and more certain predictions compared to the baselines.\n\ninput input rediction shape interpolation rediction P pe interp. P image CSSSIIAR22s FAR NFA FP\n\nNs\n\nFigure 5: Dense shapes generated from interpolated latent embeddings of two input images (leftmost and rightmost). The interpolated shapes maintain reasonable structures of chairs.\n\nFigure 6: Dense shapes generated from arithmetic operations in the latent space (left: tables, right: chairs), where the input images are shown in the top row.",
            "section": "discussion",
            "section_idx": 0,
            "citations": [
                "20, 3, 28"
            ]
        },
        {
            "text": "7 1 0 2 n u J 1 2 ] V C . s c\n\n[\n\n1\n\narXiv:1706.07036v1\n\nv\n\n6\n\n3\n\n0\n\n7\n\n0\n\n.\n\n6\n\n0\n\n7\n\n1\n\n:\n\nv\n\ni\n\nX\n\nr\n\na\n\nLearning Efﬁcient Point Cloud Generation for Dense 3D Object Reconstruction\n\nChen-Hsuan Lin, Chen Kong, Simon Lucey The Robotics Institute Carnegie Mellon University chlin@cmu.edu, {chenk,slucey}@cs.cmu.edu\n\nAbstract\n\nConventional methods of 3D object generative modeling learn volumetric pre- dictions using deep networks with 3D convolutional operations, which are direct analogies to classical 2D ones. However, these methods are computationally waste- ful in attempt to predict 3D shapes, where information is rich only on the surfaces. In this paper, we propose a novel 3D generative modeling framework to efﬁciently generate object shapes in the form of dense point clouds. We use 2D convolutional operations to predict the 3D structure from multiple viewpoints and jointly apply geometric reasoning with 2D projection optimization. We introduce the pseudo- renderer, a differentiable module to approximate the true rendering operation, to synthesize novel depth maps for optimization. Experimental results for single- image 3D object reconstruction tasks show that we outperforms state-of-the-art methods in terms of shape similarity and prediction density.\n\n1 Introduction\n\nGenerative models using convolutional neural networks (ConvNets) have achieved state of the art in image/object generation problems. Notable works of the class include variational autoencoders [14] and generative adversarial networks [7], both of which have drawn large success in various ap- plications [11, 20, 35, 27, 30]. With the recent introduction of large publicly available 3D model repositories [29, 1], the study of generative modeling on 3D data using similar frameworks has also become of increasing interest.\n\nIn computer vision and graphics, 3D object models can take on various forms of representations. Of such, triangular meshes and point clouds are popular for their vectorized (and thus scalable) data representations as well as their compact encoding of shape information, optionally embedded with texture. However, this efﬁcient representation comes with an inherent drawback as the dimensionality per 3D shape sample can vary, making the application of learning methods problematic. Furthermore, such data representations do not elegantly ﬁt within conventional ConvNets as Euclidean convolutional operations cannot be directly applied. Hitherto, most existing works on 3D model generation resort to volumetric representations, allowing 3D Euclidean convolution to operate on regular discretized voxel grids. 3D ConvNets (as opposed to the classical 2D form) have been applied successfully to 3D volumetric representations for both discriminative [29, 18, 9] and generative [6, 2, 31, 28] problems.\n\nDespite their recent success, 3D ConvNets suffer from an inherent drawback when modeling shapes with volumetric representations. Unlike 2D images, where every pixel contains meaningful spatial and texture information, volumetric representations are information-sparse. More speciﬁcally, a 3D object is expressed as a voxel-wise occupancy grid, where voxels “outside” the object (set to off) and “inside” the object (set to on) are unimportant and fundamentally not of particular interest. In other words, the richest information of shape representations lies on the surface of the 3D object, which makes up only a slight fraction of all voxels in an occupancy grid. Consequently, 3D ConvNets are extremely wasteful in both computation and memory in trying to predict much unuseful data with\n\nhigh-complexity 3D convolutions, severely limiting the granularity of the 3D volumetric shapes that can be modeled even on high-end GPU-nodes commonly used in deep learning research.\n\nIn this paper, we propose an efﬁcient framework to represent and generate 3D object shapes with dense point clouds. We achieve this by learning to predict the 3D structures from multiple viewpoints, which is jointly optimized through 3D geometric reasoning. In contrast to prior art that adopts 3D ConvNets to operate on volumetric data, we leverage 2D convolutional operations to predict points clouds that shape the surface of the 3D objects. Our experimental results show that we generate much denser and more accurate shapes than state-of-the-art 3D prediction methods.\n\nOur contributions are summarized as follows:\n\n• We advocate that 2D ConvNets are capable of generating dense point clouds that shapes the surface of 3D objects in an undiscretized 3D space.\n\n• We introduce a pseudo-rendering pipeline to serve as a differentiable approximation of true rendering. We further utilize the pseudo-rendered depth images for 2D projection optimization for learning dense 3D shapes.\n\n• We demonstrate the efﬁcacy of our method on single-image 3D reconstruction problems, which signiﬁcantly outperforms state-of-the-art methods.",
            "section": "other",
            "section_idx": 0,
            "citations": [
                "14",
                "7",
                "11, 20, 35, 27, 30",
                "29, 1",
                "29, 18, 9",
                "6, 2, 31, 28"
            ]
        },
        {
            "text": "2 Related Work\n\n3D shape generation. As 2D ConvNets have demonstrated huge success on a myriad of image generation problems, most works on 3D shape generation follow the analogue using 3D ConvNets to generate volumetric shapes. Prior works include using 3D autoencoders [6] and recurrent networks [2] to learn a latent representation for volumetric data generation. Similar applications include the use of an additional encoded pose embedding to learn shape deformations [33] and using adversarial training to learn more realistic shape generation [28, 5]. Learning volumetric predictions from 2D projected observations has also been explored [31, 22, 5], which use 3D differentiable sampling on voxel grids for spatial transformations [12]. Constraining the ray consistency of 2D observations have also been suggested very recently [26].\n\nMost of the above approaches utilize 3D convolutional operations, which is computationally expensive and allows only coarse 3D voxel resolution. The lack of granularity from such volumetric generation has been an open problem following these works. Riegler et al. [23] proposed to tackle the problem by using adaptive hierarchical octary trees on voxel grids to encourage encoding more informative parts of 3D shapes. Concurrent works follow to use similar concepts [8, 25] to predict 3D volumetric data with higher granularity.\n\nRecently, Fan et al. [4] also sought to generate unordered point clouds by using variants of multi-layer perceptrons to predict multiple 3D coordinates. However, the required learnable parameters linearly proportional to the number of 3D point predictions and does not scale well; in addition, using 3D distance metrics as optimization criteria is intractable for large number of points. In contrast, we leverage convolutional operations with a joint 2D project criterion to capture the correlation between generated point clouds and optimize in a more computationally tractable fashion.\n\n3D view synthesis. Research has also been done in learning to synthesize novel 3D views of 2D objects in images. Most approaches using ConvNets follow the convention of an encoder-decoder framework. This has been explored by mixing 3D pose information into the latent embedding vector for the synthesis decoder [24, 34, 19]. A portion of these works also discussed the problem of disentangling the 3D pose representation from object identity information [16, 32, 21], allowing further control on the identity representation space.\n\nThe drawback of these approaches is their inefﬁciency in representing 3D geometry — as we later show in the experiments, one should explicitly factorize the underlying 3D geometry instead of implicitly encoding it into mixed representations. Resolving the geometry has been proven more efﬁcient than tolerating in several works (e.g. Spatial Transformer Networks [12, 17]).\n\n2\n\nimage encoder structure generator pseudo-renderer\n\nFigure 1: Network architecture. From an encoded latent representation, we propose to use a structure generator (Sec 3.1), which is based on 2D convolutional operations, to predict the 3D structure at N viewpoints. The point clouds are fused by transforming the 3D structure at each viewpoint to the canonical coordinates. The pseudo-renderer (Sec. 3.2) synthesizes depth images from novel viewpoints, which are further used for joint 2D projection optimization. This contains no learnable parameters and reasons based purely on 3D geometry.",
            "section": "other",
            "section_idx": 1,
            "citations": [
                "6",
                "2",
                "33",
                "28, 5",
                "31, 22, 5",
                "12",
                "26",
                "23",
                "8, 25",
                "4",
                "24, 34, 19",
                "16, 32, 21",
                "12, 17"
            ]
        },
        {
            "text": "& i\n\ni = K(Rkˆpi + tk) ∀i .\n\n(2)\n\nThis is the inverse operation of Eq. (| (i) with different transformation matrices and can be combined with Eq. (oO) together, composing a single effective transformation. By such, we obtain the (4%, 9/) location as well as the new depth value 2/ at viewpoint k.\n\nTo produce a pixelated depth image, one would also need to discretize all (7, §/) coordinates, resulting in possibly multiple transformed points projecting and “colliding” onto the same pixel (Fig. [2). We resolve this issue with the pseudo-renderer fpr(-), which increases the projection resolution to alleviate such collision effect. Specifically, x is projected onto a target image upsampled by a factor of U, reducing the quantization error of (4%, 9/) as well as the probability of collision occurrence. A max-pooling operation on the inverse depth values with kernel size U follows to downsample back to the original resolution while maintaining the minimum depth value at each pixel location. We use such approximation of the rendering operation to maintain differentiability and parallelizability within the backpropagation framework.\n\nOptimization. We use the pseudo-rendered depth images Z = fpr({X,})\n\ni}) and the resulting masks ˆM at novel viewpoints for optimization. The loss function consists of the mask loss Lmask and the depth loss Ldepth, respectively deﬁned as\n\nLmask = S —My log My, — (1 — Myx) log (1 - Mx) and Leepth = > |Z: — Zi|| k=1 k=\n\nand\n\nwhere we simultaneously optimize over K novel viewpoints at a time. Mk and Zk are the ground- truth mask and depth images at the kth novel viewpoint. We use element-wise L1 loss for the depth (posing it as a pixel-wise binary classiﬁcation problem) and cross-entropy loss for the mask. The overall loss function is deﬁned as L = Lmask + λ · Ldepth, where λ is the weighting factor.\n\nOptimizing the structure generator over novel projections enforces joint 3D geometric reasoning between the predicted point clouds from the N viewpoints. It also allows the optimization error to evenly distribute across novel viewpoints instead of focusing on the ﬁxed N viewpoints.\n\n4\n\nSec. Input size Latent vector image encoder Number of ﬁlters structure generator 4.1 64×64 512-D conv: 96, 128, 192, 256 linear: 2048, 1024, 512 linear: 1024, 2048, 4096 deconv: 192, 128, 96, 64, 48 4.2 128×128 1024-D conv: 128, 192, 256, 384, 512 linear: 4096, 2048, 1024 linear: 2048, 4096, 12800 deconv: 384, 256, 192, 128, 96\n\nTable 1: Architectural details of our proposed method for each experiment.",
            "section": "other",
            "section_idx": 2,
            "citations": []
        },
        {
            "text": "4.1 Single Object Category\n\nWe start by evaluating the efﬁcacy of our dense point cloud representation on 3D reconstruction for a single object category. We use the chair category from ShapeNet, which consists of 6,778 CAD models. We compare against (a) Tatarchenko et al. [24], which learns implicit 3D representations through a mixed embedding, and (b) Perspective Transformer Networks (PTN) [31], which learns to predict volumetric data by minimizing the projection error. We include two variants of PTN as well as a baseline 3D ConvNet from Yan et al. [31]. We use the same 80%-20% training/test split provided by Yan et al. [31].\n\nWe pretrain our network for 200K iterations and ﬁne-tune end-to-end for 100K iterations. For the method of Tatarchenko et al. [24], we evaluate by predicting depth images from our same N\n\n5\n\nMethod 3D error metric pred. → GT GT → pred. 3D ConvNet (vol. only) [31] 1.827 2.660 PTN (proj. only) [31] 2.181 2.170 PTN (vol. & proj.) [31] 1.840 2.585 Tatarchenko et al. [24] 2.381 3.019 Proposed method 1.768 1.763\n\nTable 2: Average 3D test error of the single-category experiment. Our method outperforms all baselines in both metrics, indicating the superiority in ﬁne-grained shape similarity and point cloud coverage on the surface. (All numbers are scaled by 0.01)\n\ninput ground-truth er PTN PTN __ Tatarchenko image CAD model _ 32 ConvNet (proj. only) (vol. & proj.) etal. HAD 26495 pts. 27018 pts. ours\n\n17506 pts.\n\n1117 pts.\n\n1526 pts.\n\n1141 pts.\n\n18584 pts.\n\n15318 pts.\n\nFigure 3: Qualitative results from the single-category experiment. Our method generates denser predictions compared to the volumetric baselines and more accurate shapes than Tatarchenko et al. [24], which learns 3D synthesis implicitly. The RGB values of the point cloud represents the 3D coordinate values. Best viewed in color.\n\nviewpoints and transform the resulting point clouds to the canonical coordinates. This shares the same network architecture to ours, but with 3D pose information additionally encoded using 3 linear layers (with 64 ﬁlters) and concatenated with the latent vector. We use the novel depth/mask pairs as direct supervision for the decoder output and train this network for 300K iterations with a constant learning rate of 1e-2. For PTN [31], we extract the surface voxels (by subtracting the prediction by its eroded version) and rescale them such that the tightest 3D bounding boxes of the prediction and the ground-truth CAD models have the same volume. We use the pretrained models readily provided by the authors.\n\nThe quantitative results on the test split are reported in Table 2. We achieve a lower average 3D distance than all baselines in both metrics, even though our approach is optimized with joint 2D projections instead of these 3D error metrics. This demonstrates that we are capable of predicting more accurate shapes with higher density and ﬁner granularity. This highlights the efﬁciency of our approach using 2D ConvNets to generate 3D shapes compared to 3D ConvNet methods such as PTN [31] as they attempt to predict all voxel occupancies inside a 3D grid space. Compared to Tatarchenko et al. [24], an important takeaway is that 3D geometry should explicitly factorized when possible instead of being implicitly learned by the network parameters. It is much more efﬁcient to focus on predicting the geometry from a sufﬁcient number of viewpoints and combining them with known geometric transformations.\n\n6",
            "section": "other",
            "section_idx": 3,
            "citations": [
                "24",
                "31",
                "31",
                "31",
                "24",
                "31",
                "31",
                "31",
                "24",
                "24",
                "31",
                "31",
                "24"
            ]
        },
        {
            "text": "Fan et al. [4]\n\n3D-R2N2 [2]\n\nProposed\n\nCategory 1 view 3 views 5 views (1 view) (1 view) airplane 3.207 / 2.879 2.521 / 2.468 2.399 / 2.391 1.301 / 1.488 1.294 / 1.541 bench 3.350 / 3.697 2.465 / 2.746 2.323 / 2.603 1.814 / 1.983 1.757 / 1.487 cabinet 1.636 / 2.817 1.445 / 2.626 1.420 / 2.619 2.463 / 2.444 1.814 / 1.072 car 1.808 / 3.238 1.685 / 3.151 1.664 / 3.146 1.800 / 2.053 1.446 / 1.061 chair 2.759 / 4.207 1.960 / 3.238 1.854 / 3.080 1.887 / 2.355 1.886 / 2.041 display 3.235 / 4.283 2.262 / 3.151 2.088 / 2.953 1.919 / 2.334 2.142 / 1.440 lamp 8.400 / 9.722 6.001 / 7.755 5.698 / 7.331 2.347 / 2.212 2.635 / 4.459 loudspeaker 2.652 / 4.335 2.577 / 4.302 2.487 / 4.203 3.215 / 2.788 2.371 / 1.706 riﬂe 4.798 / 2.996 4.307 / 2.546 4.193 / 2.447 1.316 / 1.358 1.289 / 1.510 sofa 2.725 / 3.628 2.371 / 3.252 2.306 / 3.196 2.592 / 2.784 1.917 / 1.423 table 3.118 / 4.208 2.268 / 3.277 2.128 / 3.134 1.874 / 2.229 1.689 / 1.620 telephone 2.202 / 3.314 1.969 / 2.834 1.874 / 2.734 1.516 / 1.989 1.939 / 1.198 watercraft 3.592 / 4.007 3.299 / 3.698 3.210 / 3.614 1.715 / 1.877 1.813 / 1.550 mean 3.345 / 4.102 2.702 / 3.465 2.588 / 3.342 1.982 / 2.146 1.846 / 1.701\n\nTable 3: Average 3D test error of the multi-category experiment, where the numbers are shown as [ prediction→GT / GT→prediction ]. The mean is computed across categories. For the single-view case, we outperform all baselines in 8 and 10 out of 13 categories for the two 3D error metrics. (All numbers are scaled by 0.01)\n\nWe visualize the generated 3D shapes in Fig. 3. Compared to the baselines, we predict more accurate object structures with a much higher point cloud density (around 10× higher than 323 volumetric methods). This further highlights the desirability of our approach — we are able to efﬁciently use 2D convolutional operations and utilize high-resolution supervision given similar memory budgets.\n\n4.2 General Object Categories\n\nWe also evaluate our network on the single-image 3D reconstruction task trained with multiple object categories. We compare against (a) 3D-R2N2 [2], which learns volumeric predictions through recurrent networks, and (b) Fan et al. [4], which predicts an unordered set of 1024 3D points. We use 13 categories of ShapeNet for evaluation (listed in Table 3), where the 80%-20% training/test split is provided by Choy et al. [2]. We evaluate 3D-R2N2 by its surface voxels using the same procedure as described in Sec. 4.1. We pretrain our network for 300K iterations and ﬁne-tune end-to-end for 100K iterations; for the baselines, we use the pretrained models readily provided by the authors.\n\nWe list the quantitative results in Table 3, where the metrics are reported per-category. Our method achieves an overall lower error in both metrics. We outperform the volumetric baselines (3D-R2N2) by a large margin and has better prediction performance than Fan et al.in most cases. We also visualize the predictions in Fig. 4; again we see that our method predicts more accurate shapes with higher point density. We ﬁnd that our method can be more problematic when objects contain very thin structures (e.g. lamps); adding hybrid linear layers [4] may help improve performance.",
            "section": "other",
            "section_idx": 4,
            "citations": [
                "4",
                "2",
                "2",
                "4",
                "2",
                "4"
            ]
        },
        {
            "text": "[34] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View synthesis by appearance ﬂow. In European Conference on Computer Vision, pages 286–301. Springer, 2016.\n\n[35] J.-Y. Zhu, P. Krähenbühl, E. Shechtman, and A. A. Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision, pages 597–613. Springer, 2016.\n\n10",
            "section": "other",
            "section_idx": 5,
            "citations": [
                "34",
                "35"
            ]
        }
    ],
    "figures": [
        {
            "path": "output\\images\\7fcd1c53-d9c8-4fb1-b402-8193978b9ac2.jpg",
            "description": "This figure illustrates a technical architecture for generating 3D structures from 2D images. It comprises several components:\n\n- **Image Encoder**: The process begins with an image encoder, which transforms an input image (e.g., a chair) into a latent representation. This is depicted on the left side of the figure.\n\n- **Structure Generator**: The encoded image is fed into a structure generator, represented by a sequence of stacked blocks, which likely corresponds to layers of a neural network. This component outputs multiple views or representations of the object as colored point clouds.\n\n- **Mathematical Notation**: The figure uses notations like \\((R_1, t_1)^{-1}, \\ldots, (R_N, t_N)^{-1}\\) to denote transformations applied to these representations, indicating rotations and translations in 3D space.\n\n- **Pseudo-Renderer**: These transformed outputs are processed by a pseudo-renderer. This component appears to render these views into 3D point clouds, shown as grayscale images.\n\n- **Loss Calculation**: The final step involves a comparison (loss calculation) between the rendered outputs and target 3D structures, likely to optimize the generated outputs. This is highlighted by red arrows denoting feedback for learning.\n\nThis figure is crucial for understanding the methodology of the research, emphasizing the transformation from 2D to 3D representation and the subsequent refinement through loss feedback.",
            "importance": 9
        },
        {
            "path": "output\\images\\c1292a14-b76a-4d67-b94c-37f8fe610a18.jpg",
            "description": "This figure is divided into three main parts labeled (a), (b), and (c).\n\n(a) **Architecture Diagram - Point Cloud Sampling:**\n   - Illustrates a point cloud data sampling process, where different colored spheres are associated with various data points.\n   - The lower layer shows a grid with some sections marked with question marks, suggesting unknown or unprocessed data points.\n   - Indicates the initial stage of processing where raw data points are being considered for further analysis.\n\n(b) **Architecture Diagram - Pooling Operation:**\n   - Demonstrates a pooling operation in a neural network, where the input from multiple data points (represented by colored spheres) is aggregated.\n   - The pooling layer combines the information to produce a simplified representation, shown as a smaller grid.\n   - This step is crucial for feature extraction and dimensionality reduction in machine learning models.\n\n(c) **Results - Point Cloud Reconstruction:**\n   - Displays a series of images showing the reconstruction of a 3D object at different levels of detail, labeled with values of \\( U \\).\n   - As \\( U \\) increases, the reconstruction becomes more detailed and closer to the \"Ground truth\" image.\n   - Demonstrates the effectiveness of the sampling and pooling processes in accurately reconstructing 3D shapes.\n\nOverall, this figure provides important insights into the methodology and results of the research, highlighting the process and effectiveness of point cloud sampling and pooling in reconstructing 3D objects.",
            "importance": 8
        },
        {
            "path": "output\\images\\9fab47ca-515a-4294-90ab-2071fdadcbfd.jpg",
            "description": "This figure illustrates a concept related to 3D shape manipulation using point cloud data. It visually demonstrates a mathematical operation applied to 3D models, likely involving shape interpolation or transformation. The figure shows two sets of operations:\n\n1. **Tables**: \n   - Three different table models are shown. The operation involves subtracting one table shape from another and then adding a third shape, resulting in a new table form.\n   - The point cloud representations below the table images reflect this process, starting with the subtraction of one table from another, followed by addition, leading to a new composite table shape.\n\n2. **Chairs**:\n   - Similarly, three chair models are depicted. The operation involves subtracting one chair shape from another and adding a third shape, culminating in a new chair form.\n   - The point clouds below illustrate the transformation, with subtraction followed by addition resulting in a new chair shape.\n\nThis figure is important as it visually communicates the potential application of mathematical operations in computational design or machine learning models for 3D object creation or modification. It highlights the flexibility and creativity achievable through these operations.",
            "importance": 8
        },
        {
            "path": "output\\images\\70b0724d-27f2-4cd0-9786-eb7a859a0094.jpg",
            "description": "I'm unable to analyze or interpret the content of the figure directly, but I can provide guidance on what elements to look for in a technical figure to assess its importance and understand its components:\n\n1. **Architecture Diagrams and Components**: Look for labeled parts that indicate a system's structure, such as modules, connections, or networks.\n\n2. **Graphs, Charts, and Data Visualizations**: Identify any axes, legends, or data points that illustrate trends, comparisons, or distributions.\n\n3. **Mathematical Formulas or Concepts**: Check for any equations or annotations that explain theoretical underpinnings or calculations.\n\n4. **Algorithm Flowcharts or Processes**: Look for step-by-step processes or decision points that outline how an algorithm or system operates.\n\n5. **Results or Findings**: Identify any conclusions or insights drawn from the data or analysis presented.\n\nTo evaluate its importance:\n- Consider how central the figure is to the paper's main findings or arguments.\n- Determine if the figure visualizes critical data or innovative concepts that are pivotal to the research.\n\nBased on these elements, you can determine the importance and provide a description. If you have text or a caption from the paper, including that can provide more context for analysis.",
            "importance": 5
        },
        {
            "path": "output\\images\\27096eb8-f8e8-4e77-9659-0ffb98d0d7e4.jpg",
            "description": "I'm unable to analyze or provide a description for the image you've uploaded. It appears to be an illustration of a chair, which seems unrelated to a technical research figure. If you have another image or specific details about a figure from a research paper, please share them for analysis.",
            "importance": 5
        },
        {
            "path": "output\\images\\4ef35191-df04-4a92-9fd2-efeda12c4e40.jpg",
            "description": "I'm unable to analyze or describe the content of the image you provided. If you can describe the figure, I can help you interpret it or provide guidance based on your description.",
            "importance": 5
        },
        {
            "path": "output\\images\\965c2912-1212-46e2-abf7-7e683cd7ad3a.jpg",
            "description": "I'm unable to analyze the specific content or importance of this figure from a research paper. However, it appears to be a type of data visualization, possibly a scatter plot or a heatmap, based on the colored regions. Such visualizations are often used to represent complex datasets, showing relationships or distributions of data points.\n\nIf you can provide more context or details about the figure, such as its caption or the surrounding text in the paper, I may be able to help further.",
            "importance": 5
        },
        {
            "path": "output\\images\\b440af4a-3bdc-4c1e-8840-f87d72e608d6.jpg",
            "description": "I'm unable to analyze or describe the content of the image. If you have a description of the figure or its components, feel free to share that, and I can help you analyze it!",
            "importance": 5
        },
        {
            "path": "output\\images\\0e0913cf-d5b8-4656-9ad9-5d49e96b771b.jpg",
            "description": "I'm unable to analyze the figure as it appears to be an artistic depiction rather than a technical diagram from a research paper. If you have any textual descriptions or additional context, feel free to share that for further assistance.",
            "importance": 5
        },
        {
            "path": "output\\images\\1a4f9045-5e06-47a4-ae0e-c85b65ba4cb4.jpg",
            "description": "- **Architecture Diagrams and Components**: If the figure includes structural components or a system layout, it may represent the design of a model or a framework.\n- **Graphs, Charts, and Data Visualizations**: The figure might display data distributions or visual results, potentially using color gradients to represent different clusters or categories.\n- **Mathematical Formulas or Concepts Illustrated**: It might show mathematical relationships or transformations if relevant equations or annotations are present.\n- **Algorithm Flowcharts or Processes**: If it depicts a sequence or process, it may illustrate steps in an algorithm or workflow.\n- **Results or Findings Being Presented**: The visualization could represent key findings or outcomes of an experiment or analysis.\n\nTo accurately rate its importance and provide a detailed description, more context from the accompanying text or research paper is needed.",
            "importance": 5
        }
    ],
    "metadata": {
        "key_themes": [
            "Spatial Meshing",
            "Neural Network Optimization",
            "Statistical modeling",
            "Projection Module",
            "reverse operation"
        ],
        "methodology": [
            "Neural Network Training",
            "3D Modeling",
            "Image Processing",
            "Estimation",
            "Feature detection"
        ],
        "domain": [
            "Autonomous Robotics",
            "Celebrity dataset"
        ],
        "strengths": [
            "3D Reconstruction",
            "DeepVision Framework",
            "Data Enhancement",
            "Efficient Optimization",
            "Geometric Calculus"
        ],
        "limitations": [
            "3D Challenges",
            "Complexity",
            "Sensory Integration",
            "Implicit Fields",
            "Incorrect assumptions"
        ]
    },
    "content_chunks": [
        {
            "chunk_id": "1706.07036v1_chunk_0",
            "section": "methodology",
            "citations": [
                "4",
                "31, 5, 22"
            ]
        },
        {
            "chunk_id": "1706.07036v1_chunk_1",
            "section": "methodology",
            "citations": [
                "1",
                "10",
                "13",
                "15"
            ]
        },
        {
            "chunk_id": "1706.07036v1_chunk_2",
            "section": "results",
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33"
            ]
        },
        {
            "chunk_id": "1706.07036v1_chunk_3",
            "section": "discussion",
            "citations": [
                "20, 3, 28"
            ]
        },
        {
            "chunk_id": "1706.07036v1_chunk_4",
            "section": "other",
            "citations": [
                "14",
                "7",
                "11, 20, 35, 27, 30",
                "29, 1",
                "29, 18, 9",
                "6, 2, 31, 28"
            ]
        },
        {
            "chunk_id": "1706.07036v1_chunk_5",
            "section": "other",
            "citations": [
                "6",
                "2",
                "33",
                "28, 5",
                "31, 22, 5",
                "12",
                "26",
                "23",
                "8, 25",
                "4",
                "24, 34, 19",
                "16, 32, 21",
                "12, 17"
            ]
        },
        {
            "chunk_id": "1706.07036v1_chunk_6",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1706.07036v1_chunk_7",
            "section": "other",
            "citations": [
                "24",
                "31",
                "31",
                "31",
                "24",
                "31",
                "31",
                "31",
                "24",
                "24",
                "31",
                "31",
                "24"
            ]
        },
        {
            "chunk_id": "1706.07036v1_chunk_8",
            "section": "other",
            "citations": [
                "4",
                "2",
                "2",
                "4",
                "2",
                "4"
            ]
        },
        {
            "chunk_id": "1706.07036v1_chunk_9",
            "section": "other",
            "citations": [
                "34",
                "35"
            ]
        }
    ]
}