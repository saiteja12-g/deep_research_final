{
    "basic_info": {
        "title": "Voxel-based 3D Detection and Reconstruction of Multiple Objects from a Single Image",
        "authors": [
            "Feng Liu",
            "Xiaoming Liu"
        ],
        "paper_id": "2111.03098v1",
        "published_year": 2021,
        "references": [
            "1810.13049v2",
            "2004.12989v2"
        ]
    },
    "raw_chunks": [
        {
            "text": "3 Methodology\n\nWe illustrate the overall architecture of our method in Fig. 2, which consists of three key modules: i) 3D voxel feature learning; ii) CenterNet-3D detector; and iii) Coarse-to-ﬁne 3D reconstruction.\n\n3.1 3D Voxel Feature Learning\n\nOur network learns to produce a compact 3D voxel feature representation of the image with com- plementary supervision from both 3D detection and reconstruction, which enables rich 3D context and geometric information and allows the two tasks to beneﬁt each other. We ﬁrst deﬁne a 3D grid V ∈ RX×Y ×Z by partitioning of the scene space into voxels Vi with a voxel size of r. The 3D grid size, Xr × Y r × Zr, is set based on the minimum volume of the scenes in the width, height and length dimensions, that can encompass all annotated instances in the dataset.\n\nFeature Extraction. We utilize a convolutional feature extractor to generate a hierarchy of multi- scale 2D feature maps. Speciﬁcally, the input to the feature extraction network is a RGB image I ∈ RWI×HI×3, where WI and HI are the image size. The convolutions are followed by down scaling the input, creating growing receptive ﬁelds and resulting in D-channel multi-scale 2D feature maps F ∈ RWF ×HF ×D. WF and HF are the width and height of feature F.\n\n3\n\nImage Feature Maps Voxel Features Detection Reconstruction Le RW ips Fc RWrxtrxD Ge RX*Yx2xD CenterNet-3D Detector | \\ | | | 3D | | , \"Heatmap | | | | \\ 1 Lt | 3D” | | | \\ 7) Regression Box @ Sampler @B Coneatenate Operator © Selection Operator (©) Linear Combination Local PCA-SDF Bases Coarse-to-Fine 3D Reconstruction. l l I | Il H + | l Il 7\n\nFigure 2: Overview of our approach. The proposed joint framework is composed of three key modules: 3D voxel feature learning (consists of feature backbone and 2D-to-3D feature lifting), CenterNet-3D detector, and coarse-to-ﬁne 3D reconstruction. 2D feature maps are ﬁrst generated from input image I, which are back-projected into voxel features G using a known camera projection matrix P. The voxel features serve for our novel 3D object detection and reconstruction.\n\nLifting 2D Features to 3D. The lifting layer back-projects 2D feature maps F into 3D voxel space, resulting in initial 3D voxel features G. Formally, the objective of the lifting operator is to populate the 3D voxel features G(x,y,z) with the projected 2D features F{(u,v)}, where {·} denotes bilinear interpolation on the 2D feature maps. We assume a full perspective camera model. Any voxel center (x,y,z) can be projected to image plane via a camera projection matrix P ∈ R3×4: [u·d,v ·d,d]T = P[x,y,z,1]T. Here u and v are the 2D position of the projection and d is its depth from the camera. The resulting voxel features G ∈ RX×Y ×Z×D provide a scene representation that is free from the effects of perspective projection.\n\n3D Voxel Features Aggregation. The lifting mechanism we use is similar to the one in [24,25,27], which has a major weakness that all voxels along a camera ray will receive the same 2D feature. This feature smearing issue increases the difﬁculties of 3D detection and reconstruction. To mitigate this issue, we propose to employ the positional encoding (PE) [70] strategy that adds 3D voxel center position to the voxel features: RX×Y ×Z×D PE−−→ RX×Y ×Z×(D+3), which helps the voxel features to be more discriminative and position-embedded. We further utilize a 3D convolutional hourglass (U-Net) network [71], comprised of a series of down- and upsampling convolutions with skip connections, to integrate both local and global information. The ﬁnal voxel features are thus G : RX×Y ×Z×(D+3) U−Net −−−−−→ RX×Y ×Z×(D+3). This voxel features serve as the cornerstone for the downstream tasks of 3D detection and reconstruction.",
            "section": "methodology",
            "section_idx": 0,
            "citations": [
                "24,25,27",
                "70",
                "71"
            ]
        },
        {
            "text": "3.4 Loss Functions and Implementation Details\n\nThe training data of one example consists of RGB image I, ground-truth 3D bounding boxes B∗ of objects, coarse-level voxelization V∗, and a set of regular SDF pairs {(idxj,sj)}K j=1 sampled from the surface. Here, each set of SDFs sj ∈ Rk×k×k, idxj is the idxj-th voxel of the holistic grid. During training, we jointly optimize the parameters of 2D feature extraction network, 3D U-Net, detection and reconstruction modules by minimizing three losses: 3D keypoint classiﬁcation loss Lcls, regression loss Lreg, and 3D reconstruction loss Lrecon, i.e.,\n\nL = Lcls + Lreg + Lrecon. (2)\n\nLoss Functions. We generate the target heatmaps Y∗ by splatting the ground truth 3D center points using a Gaussian kernel (please refer to Supp for details). If two Gaussians of the same class overlap, we take the element-wise maximum. The 3D keypoint branch is trained with a penalty-reduced focal loss [28,73] in a point-wise manner on the 3D heatmap.\n\n(3) lo @yZe (1 — Veyze)\"log(Veyze) if Viyze = 1 ( 1—Dryzc)” Vryse)\"l0g(1—Veyee) otherwise\n\nwhere N is the number of objects per image, µ = 2 and σ = 4 are hyper-parameters of the focal loss.\n\nWe deﬁne the 3D bounding box regression loss as the L1 distance between the predicted transform B and the ground truth B∗: Lreg = 1 N ||B − B∗||1. The reconstruction loss consists of cross-entropy classiﬁcation loss Lv for coarse-level voxelization and ﬁne-level SDF regression loss.\n\nK Lrecon = Ly(V,V\") + $°||Sp2j — 8)||3, 2 = MLP(Gias,)- (4) J\n\nImplementation Detail. We use a ResNet-34 network as our 2D feature extractor. We extract features immediately before the ﬁnal three downsampling layers, resulting in a set of feature maps at 1/8, 1/16, 1/32 scales of the input resolution. We then resize them to the original size of the input images via bilinear interpolation. Convolutional layers with 1 × 1 kernels are used to map these feature maps to a common channel size of 64. The set of feature maps is summarized as F before\n\n6\n\nTable 1: Multiple object reconstruction comparison. We report per-class and mean IoU over all classes, and class-agnostic global IoU on 1283 voxel grid.",
            "section": "methodology",
            "section_idx": 1,
            "citations": [
                "28,73"
            ]
        },
        {
            "text": "Method\n\n| ShapeNet-triplets || ShapeNet-pairs | bottle bowl chair mug sofa table | mean global || mean global 34.8 | 43.9 19.8 | 36.4 36.1 | 46.0\n\nCoReNet [22] 61.8 Points2Objects [23] | 63.5 Proposed | 63.3\"\n\n36.2 30.2 38.5\n\n30.1 18.9 318°\n\n48.0 52.9 41.5 44.5 51.7 54.3\n\n49.8 44.7\n\n43.1\n\n=\n\n|| 46.7\n\n52.3,\n\n52.7\n\n55.1\n\nInput\n\n3 Z oO m% i} 1S)\n\nProposed\n\nFigure 4: Qualitative results on ShapeNet-triplets dataset. We compare to CoReNet [22] in two different viewpoints. Our model more accurately reconstructs details and hallucinates occluded parts.\n\npassing to 3D lifting layers. For the main experiments, we train our models with a batch size of 8 on a GTX 1080Ti GPU for 200 epochs. The learning rate is set at 5 × 10−4 and drops at 50 and 100 epochs by a factor of 10. For more details, please refer to Sec. 4 or Supp.\n\n4 Experiments\n\n4.1 Multiple Object Detection and Reconstruction on ShapeNet-pairs and -triplets\n\nDatasets. Following the experimental setting of CoReNet [22] and Points2Objects [23], we evaluate multiple object detection and reconstruction on ShapeNet-pairs and ShapeNet-triplets datasets [22]. These datasets contain 256×256 photorealistic renderings of either pairs or triplets of ShapeNet [46] objects placed on a ground plane with random scale, rotation, and camera viewpoint. The ShapeNet- pairs has several pairs of object classes: bed-pillow, bottle-bowl, bottle-mug, chair-table, display-lamp, guitar-piano, motorcycle-car, which contains 365,600 images on trainval and 91,200 on test. The ShapeNet-triplets is with bottle-bowl-mug and chair-sofa-table, which includes 91,400 on trainval and 22,000 on test.\n\nExperimental Settings. In this experiment, we set a voxel grid of size X×Y ×Z = 40×25×25 (r = 0.1), which is sufﬁcient to enclose all objects in the datasets. We randomly select 200 surfaces from the training set to generate NS ≈ 200,000 occupied voxels. In this experiment, we set k = 11, lB = 64, C = 6 (ShapeNet-triplets) or C = 13 (ShapeNet-pairs). We compare with SOTA methods for multiple object reconstruction: CoReNet [22] and Points2Objects [23]. As CoReNet doesn’t perform 3D detection, we only compare with Points2Objects on detection. Following [23], we use mean average precision (mAP) as the detection metric with 3D box intersection-over-union (IoU) thresholds 0.25 and 0.5. Following [22,23], the metric for reconstruction is IoU on a 1283 voxel grid.\n\nResults. We ﬁrst report the 3D detection results. Our method achieves a higher detection accuracy than Points2Objects [23]: 51.5% vs. 48.6% (threshold@0.5) and 80.3% vs. 77.2% (threshold@0.25), which demonstrates that voxel features perform better than image-based features for monocular 3D detection. For reconstruction, we report the mean over the per-class IoU, as well as the global IoU of all object instances within a scene, which does not concern predicted class labels. As compared in Tab. 1, our method signiﬁcantly outperforms two SOTA baselines on both datasets. On ShapeNet- triplets, our method achieves relative 5.0% global IoUs gains while 4.8% mean IoUs gains, which\n\n7\n\nInput Image Points2Objects Proposed a aap s\n\n12345678910 Ip 60 3 3 3 S iE} Pa 6 5 50 Eq 2 — 0.16m —0.32m —0.64m",
            "section": "methodology",
            "section_idx": 2,
            "citations": [
                "22",
                "23",
                "22",
                "22",
                "23",
                "22",
                "46",
                "22",
                "23",
                "23",
                "22,23",
                "23"
            ]
        },
        {
            "text": "4.4 3D Shape Representation Power of PCA-SDF\n\nWhile PCA-SDF has demonstrated its advantage in 2D to 3D reconstruction, this is rooted from its ability in representing 3D shapes. To quantify its 3D shape representation power, we design the following experiment and compare with DeepSDF [59] and DeepLS [30], without involving 2D image inputs. Following the setting of [30], we utilize 1,000 ShapeNet shapes (200 each from 5 categories) to compute our PCA-SDF bases. Each 3D shape is split by a 32×32×32 grids (r = 1 During training, both DeepSDF and DeepLS optimize the latent codes and decoder weights through backpropagation, to best represent the training shapes. In inference, decoder weights are ﬁxed, and the optimal latent code is estimated given a testing shape. In contrast, we compute the latent codes whose multiplication with PCA-SDF bases can best approximate the ground-truth SDF. 32).\n\nWe evaluate 3D shape reconstruction accuracy on various categories. As shown in Tab. 4, PCA-SDF has lower reconstruction error than DeepLS, even with a smaller number of representation parameters. Moreover, our inference is 10× more efﬁcient (infer at 2563 resolution) which meets the real-time requirement for downstream tasks.\n\nTo study whether the PCA-based representation is sensitive to tiny geometry perturbation, we apply minimal translation (±0.1r, ±0.05r) and rotation (±4◦, ±2◦) to testing surfaces of the 5 categories and evaluate surface reconstruction error on these data, while no data augmentation was applied to the training data of shape bases computing. As shown in Fig. 5(c), the error is stable in a very small range, which illustrates that PCA-SDF is robust to translation and rotation variations.\n\nGeneralization to Unseen Category. In order to investigate the generalization of PCA-SDF, we design an experiment to compute PCA-SDF from a single category (200 shapes), and reconstruct 3D shapes from the other four unseen categories. We repeat the training/testing 5 times across 5 categories. As reported in Tab. 4, PCA-SDF trained on unseen categories achieves a comparable performance (0.028 vs. 0.022) with the one trained on seen categories when lB = 125, which indicates that local 3D shapes at the voxel level are indeed similar to each other, even across different categories.",
            "section": "methodology",
            "section_idx": 3,
            "citations": [
                "59",
                "30",
                "30"
            ]
        },
        {
            "text": "5 Conclusion\n\nWe present a voxel-based 3D detection and reconstruction framework for predicting 3D bounding boxes and shapes of multiple objects from a single image. Speciﬁcally, we ﬁrst learn a regular grid of 3D voxel features for the input images. Based on the voxel features, we devise a novel CenterNet-3D detector to detect and regress 3D bounding boxes in the 3D space. With a coarse-level voxelization and a ﬁne-level local PCA-SDF representation, our reconstruction module provides highly efﬁcient and accurate reconstructions. The comprehensive experiments show the superiority of the proposed method in 3D detection and reconstruction, as well as shape representation power. The same as CoReNet and Points2Objects, one limitation of our approach is that it requires the camera calibration matrix as input which might limit its application to real images. Therefore, one future direction is to invest the necessity of this requirement and/or integrate with auto-calibration methods.\n\n10\n\nReferences\n\n[1] Joris Guerry, Alexandre Boulch, Bertrand Le Saux, Julien Moras, Aurélien Plyer, and David Filliat. Snapnet-r: Consistent 3D multi-view semantic labeling for robotics. In ICCV, 2017.\n\n[2] Keisuke Tateno, Federico Tombari, Iro Laina, and Nassir Navab. CNN-SLAM: Real-time dense monocular slam with learned depth prediction. In CVPR, 2017.\n\n[3] Lei Han, Tian Zheng, Yinheng Zhu, Lan Xu, and Lu Fang. Live semantic 3D perception for immersive augmented reality. TVCG, 2020.\n\n[4] Aseem Behl, Omid Hosseini Jafari, Siva Karthik Mustikovela, Hassan Abu Alhaija, Carsten Rother, and Andreas Geiger. Bounding boxes, segmentations and object coordinates: How important is recognition for 3D scene ﬂow estimation in autonomous driving scenarios? In ICCV, 2017.\n\n[5] Yiping Chen, Jingkang Wang, Jonathan Li, Cewu Lu, Zhipeng Luo, Han Xue, and Cheng Wang. LiDAR-video driving dataset: Learning driving policies effectively. In CVPR, 2018.\n\n[6] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In CVPR, 2012.\n\n[7] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel Urtasun. Monocular 3D object detection for autonomous driving. In CVPR, 2016.\n\n[8] Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, and Song-Chun Zhu. Coop- erative holistic scene understanding: Unifying 3D object, layout, and camera pose estimation. In NeurIPS, 2018.\n\n[9] Garrick Brazil and Xiaoming Liu. M3D-RPN: Monocular 3D region proposal network for object detection. In ICCV, 2019.\n\n[10] Yongjian Chen, Lei Tai, Kai Sun, and Mingyang Li. Monopair: Monocular 3D object detection using pairwise spatial relationships. In CVPR, 2020.\n\n[11] Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, and Bernt Schiele. Kinematic 3d object detection in monocular video. In CVPR, 2020.\n\n[12] Abhinav Kumar, Garrick Brazil, and Xiaoming Liu. Groomed-nms: Grouped mathematically differentiable nms for monocular 3D object detection. In CVPR, 2021.\n\n[13] Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, Bill Freeman, and Josh Tenenbaum. Marrnet: 3D shape reconstruction via 2.5D sketches. In NeurIPS, 2017.\n\n[14] Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Josh Tenen- baum, and Bill Freeman. Visual object networks: Image generation with disentangled 3D representations. In NeurIPS, 2018.\n\n[15] Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for generative shape modeling. In CVPR, 2019.\n\n[16] Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. Atlasnet: A papier-mâché approach to learning 3D surface generation. In CVPR, 2018.\n\n[17] Chao Wen, Yinda Zhang, Zhuwen Li, and Yanwei Fu. Pixel2Mesh++: multi-view 3D mesh generation via deformation. In ICCV, 2019.\n\n[18] Abhijit Kundu, Yin Li, and James M Rehg. 3D-RCNN: Instance-level 3D object reconstruction via render-and-compare. In CVPR, 2018.\n\n[19] Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian Jun Zhang. Total3DUnderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image. In CVPR, 2020.\n\n[20] Martin Runz, Kejie Li, Meng Tang, Lingni Ma, Chen Kong, Tanner Schmidt, Ian Reid, Lourdes Agapito, Julian Straub, Steven Lovegrove, et al. FroDO: From detections to 3D objects. In CVPR, 2020.\n\n[21] Justin Johnson Georgia Gkioxari, Jitendra Malik. Mesh R-CNN. In ICCV, 2019.\n\n[22] Stefan Popov, Pablo Bauszat, and Vittorio Ferrari. CoReNet: Coherent 3D scene reconstruction from a single RGB image. In ECCV, 2020.\n\n11\n\n[23] Francis Engelmann, Konstantinos Rematas, Bastian Leibe, and Vittorio Ferrari. From points to multi-object 3D reconstruction. In CVPR, 2021.\n\n[24] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhofer. DeepVoxels: Learning persistent 3D feature embeddings. In CVPR, 2019.\n\n[25] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Orthographic feature transform for monocular 3D object detection. In BMVC, 2018.\n\n[26] Cody Reading, Ali Harakeh, Julia Chae, and Steven L Waslander. Categorical depth distribution network for monocular 3D object detection. In CVPR, 2021.\n\n[27] Benoit Guillard, Edoardo Remelli, and Pascal Fua. UCLID-Net: Single view reconstruction in object space. In NeurIPS, 2020.\n\n[28] Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. arXiv preprint arXiv:1904.07850, 2019.\n\n[29] Francis Engelmann, Jörg Stückler, and Bastian Leibe. Joint object pose estimation and shape reconstruction in urban street scenes using 3D shape priors. In GCPR, 2016.\n\n[30] Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, and Richard Newcombe. Deep local shapes: Learning local SDF priors for detailed 3D reconstruction. In ECCV, 2020.\n\n[31] Muhammad Zeeshan Zia, Michael Stark, and Konrad Schindler. Are cars just 3D boxes?-jointly estimating the 3d shape of multiple objects. In CVPR, 2014.\n\n[32] Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, Céline Teuliere, and Thierry Chateau. Deep MANTA: A coarse-to-ﬁne many-task network for joint 2D and 3D vehicle analysis from monocular image. In CVPR, 2017.\n\n[33] Chi Li, M Zeeshan Zia, Quoc-Huy Tran, Xiang Yu, Gregory D Hager, and Manmohan Chan- draker. Deep supervision with shape concepts for occlusion-aware 3D object parsing. In CVPR, 2017.\n\n[34] Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas. Render for CNN: Viewpoint estimation in images using cnns trained with rendered 3D model views. In ICCV, 2015.\n\n[35] Shubham Tulsiani and Jitendra Malik. Viewpoints and keypoints. In CVPR, 2015.\n\n[36] Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese. Data-driven 3D voxel patterns for object category recognition. In CVPR, 2015.\n\n[37] M Zeeshan Zia, Michael Stark, and Konrad Schindler. Towards scene understanding with detailed 3D object representations. IJCV, 112(2):188–203, 2015.\n\n[38] Patrick Poirson, Phil Ammirato, Cheng-Yang Fu, Wei Liu, Jana Kosecka, and Alexander C Berg. Fast single shot detection and pose estimation. In 3DV, 2016.",
            "section": "methodology",
            "section_idx": 4,
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33",
                "34",
                "35",
                "36",
                "37",
                "38"
            ]
        },
        {
            "text": "1\n\n2021\n\n2\n\n0\n\n2\n\nv o N 4 ] V C . s c [ 1 v 8 9 0 3 0 . 1 1\n\n1\n\n2\n\n:\n\nv\n\ni\n\nX\n\nr\n\na\n\nVoxel-based 3D Detection and Reconstruction of Multiple Objects from a Single Image\n\nFeng Liu Xiaoming Liu\n\nDepartment of Computer Science and Engineering Michigan State University, East Lansing MI 48824 {liufeng6, liuxm}@msu.edu\n\nAbstract\n\nInferring 3D locations and shapes of multiple objects from a single 2D image is a long-standing objective of computer vision. Most of the existing works either predict one of these 3D properties or focus on solving both for a single object. One fundamental challenge lies in how to learn an effective representation of the image that is well-suited for 3D detection and reconstruction. In this work, we propose to learn a regular grid of 3D voxel features from the input image which is aligned with 3D scene space via a 3D feature lifting operator. Based on the 3D voxel features, our novel CenterNet-3D detection head formulates the 3D detection as keypoint detection in the 3D space. Moreover, we devise an efﬁcient coarse-to-ﬁne reconstruction module, including coarse-level voxelization and a novel local PCA- SDF shape representation, which enables ﬁne detail reconstruction and one order of magnitude faster inference than prior methods. With complementary supervision from both 3D detection and reconstruction, one enables the 3D voxel features to be geometry and context preserving, beneﬁting both tasks. The effectiveness of our approach is demonstrated through 3D detection and reconstruction in single object and multiple object scenarios. Code is available at http://cvlab.cse. msu.edu/project-mdr.html.\n\n1 Introduction\n\nAs a fundamental computer vision task, instance-level 3D scene understanding from a single image has drawn substantial attention from researchers due to its importance in applications such as robotics [1,2], AR/VR [3] and autonomous driving [4–6]. The important 3D properties include 3D bounding box (pose, size, location) and 3D shape of object instances. In this work, we aim to design a framework to infer all these 3D properties of multiple objects from a single 2D image.\n\nIn recent years, various monocular methods are proposed to predict either 3D boxes [7–12] or 3D shapes [13–17]. However, only a few studies [18–23] consider both 3D detection and reconstruction for a total 3D scene understanding. The complexity of real-world scenarios and diverse category variations make it challenging to fully reconstruct the scene context (both semantics and geometry) at the instance level from a single image. Moreover, those methods primarily assign 3D semantic labels to pixels. Yet, such a 2D representation with depth ambiguity is insufﬁcient for 3D geometry and context reasoning. It is thus crucial to develop an effective representation of the image that is relevant to 3D geometry and spatial information for performing accurate 3D detection and reconstruction.\n\nIn light of this, attempts like grid-based representation have been made for tasks such as rendering [24], detection [25,26], or reconstruction [27]. OFT [25] proposes to sample and transform image features into a BEV grid representation, which enables holistic reasoning of the 3D scene conﬁguration. CaDDN [26] extends the BEV grid representation with a categorical depth prior, leading to higher 3D detection accuracies. DeepVoxels [24] and UCLID-Net [27] build voxel features by back-projecting\n\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\n\nFigure 1: Given a single image as input, our proposed approach jointly predicts 3D object bounding boxes and surfaces.\n\n2D features to 3D space for respective rendering or single object reconstruction purposes. Inspired by this line of works, we propose a novel voxel-based 3D detection and reconstruction framework for predicting 3D bounding boxes and surfaces of multiple objects from a single image (see Fig. 1).\n\nSpeciﬁcally, we ﬁrst divide a 3D scene space into a regular grid of voxels. For each voxel, we assign 3D features by sampling from the image plane via a 2D-to-3D feature lifting operator and the known camera projection matrix. As multiple voxels can be projected to the same position, this leads to similar features along the camera ray and increased difﬁculty for downstream tasks. To remedy this, we use a positional encoding strategy to make our voxel features position-aware and more discriminative. Based on the intermediate voxel features, we carefully devise our detection and reconstruction modules. For detection, we introduce a novel CenterNet-3D detector head. Instead of formulating the 3D detection as 2D keypoint detection problem as conventional CenterNet-based methods [28,29], each object is directly represented by its 3D keypoint. Predicting a class-speciﬁc 3D heatmap can show probabilities of 3D object centers in the pre-deﬁned voxel space, leading to improved 3D center accuracy. For reconstruction, we propose a multi-level shape representation with two components: coarse-level occupancy representation and ﬁne-level local PCA-SDF representation. The coarse-level voxel grid represents the whole 3D scene with continuous occupancy values. At a ﬁne level, we represent the occupied voxels with a PCA-based signed distance function (SDF) by assuming that the local shapes of different voxels are similar either within an object instance, or across different objects.\n\nIn summary, the contributions of this work include:\n\n© We propose a novel voxel-based 3D detection and reconstruction framework, which infers the 3D locations and 3D surfaces for multiple object instances with only a 2D image as input.\n\n© We present a novel CenterNet-3D detector, where each object is represented by its center point in a partitioned 3D grid space. CenterNet-3D avoids estimating depth directly from image features, leading to increased detection performance.\n\n© We propose a novel local PCA-SDF shape representation, which provides finer reconstruction and order of magnitude faster inference than SOTA local implicit function methods like DeepLS [30].\n\n© We demonstrate the superiority of our method in multiple object 3D reconstruction and detection, as well as 3D shape representation. We assemble a 3D detection and reconstruction benchmark with 18, 000 real images, annotated with 3D models and bounding boxes of 19 object categories.",
            "section": "other",
            "section_idx": 0,
            "citations": [
                "1,2",
                "3",
                "24",
                "25,26",
                "27",
                "25",
                "26",
                "24",
                "27",
                "28,29",
                "30"
            ]
        },
        {
            "text": "2 Related Work\n\n3D Scene Understanding and Single Object Reconstruction. Tremendous efforts have been devoted to instance-level 3D scene understanding [7,9,18,28,31–37] over the last decade. However, most of these approaches estimate object orientation [34,35,38] or 3D bounding boxes [7–9,39–45]. Since describing objects with boxes only offers a coarse information of 3D objects in images, the usage of 3D models as shape priors can complement and enrich 3D scene understanding. Yet, scene understanding at the instance level remains challenging due to the large number of objects with various categories. With the substantial growth in the number of publicly available 3D models, datasets such as ShapeNet [46] have allowed neural networks to train on the 3D shape reconstruction task from single images [47–52]. To further leverage real-world images in 3D modeling, as Liu et al. [53] propose a semi-supervised learning framework for generic objects. However, most of these methods estimate 3D shapes in the object-centric coordinate system, which differs from the shape prediction of multiple instances at scene-level 3D reconstruction – the focus of our work.\n\n2\n\nMultiple Object 3D Reconstruction A common characteristic amongst aforementioned single object 3D reconstruction approaches is that they usually treat objects as isolated geometries without considering the scene context, such as object locations, and instance-to-instance interactions. Recently, there is progress in multiple object 3D reconstruction. 3D-RCNN [18] exploits the idea of using inverse graphics to map image regions to the 3D shape and pose of object instances. The shape is represented by a simple linear subspace which limits its application for objects with large intra-class variability. Mesh R-CNN [21] augments Mask R-CNN [54] with a mesh predictions branch that estimates a 3D mesh for each detected in an image. Total3DUnderstanding [19] presents a framework that predicts room layout, 3D object bounding boxes, and meshes for all objects in an image based on the known 2D bounding boxes. However, these three methods ﬁrst detect objects in the 2D image, and then independently produce their 3D shapes with single object reconstruction modules. This could be problematic when 3D boxes of objects intersect, such as a chair is pushed under a table.\n\nRecently, CoReNet [22] performs multiple object reconstructions in a ﬁxed 1283 voxel grid without recovering 3D position information in the world space. Points2Objects [23] combines a 3D object detector and shape retrieval to detect and reconstruct 3D objects from an image. However, it suffers from two limitations: 1) Its CenterNet-based 3D detector reasons 3D boxes directly in the 2D image domain, which is inherently challenging due to the lack of reliable depth cue. 2) Retrieval-based methods depend on the size and diversity of the pre-deﬁned CAD model pool. Moreover, [22,23] train on synthetic renderings, which limits their applicability to real-world scenarios. Instead of relying on 2D feature for 3D detection or reconstruction, we propose to learn a geometry and context preserving voxel feature representation, which is well suited for 3D detection and reconstruction. Moreover, we validate our method on real-world images from 19 object categories.\n\nLocal Shape Priors Many neural architectures are proposed to model 3D objects via geometric representations, e.g., point clouds [55], meshes [16,56], voxels [13,57], or implicit functions [15, 58–60]. Recently, neural implicit functions have demonstrated their effectiveness by encoding geometry in latent vectors and network weights, which parameterize surfaces through level-sets. Instead of an object-level representation, some follow-up works learn patch-level or primitive-level representations of surfaces, e.g., PatchNet [61], CvxNet [62], BSP-Net [63]. To further leverage local geometric priors, another line of works learn implicit geometry on sparse regular [64,65] or 3D voxel grids [30,66]. A latent code of each voxel is responsible for representing implicit geometry in a small neighborhood, enabling ﬁne-grained reconstruction. However, these methods often suffer from inefﬁcient inference as each point needs a forward pass through of the implicit function network. Instead, we build a local PCA-SDF shape representation, which represents each local shape as a linear combination of implicit volumetric prototypes, leading to ﬁner details and order of magnitude faster inference than prior works. Similar eigenanalysis of SDF has been applied for either global shape representation [67] or geometry compression [68,69]. However, none of them develop their algorithms from our perspective of local shape priors, which is motivated from the assumption that local shapes at the voxel level share similarities.",
            "section": "other",
            "section_idx": 1,
            "citations": [
                "34,35,38",
                "46",
                "53",
                "18",
                "21",
                "54",
                "19",
                "22",
                "23",
                "22,23",
                "55",
                "16,56",
                "13,57",
                "61",
                "62",
                "63",
                "64,65",
                "30,66",
                "67",
                "68,69"
            ]
        },
        {
            "text": "3.2 Monocular CenterNet-3D Detector\n\nConventional CenterNet-based [28] monocular 3D detection methods such as Points2Objects [23] formulate 3D detection as a projected 2D keypoint detection problem. In contrast, we propose a novel CenterNet-3D detection head, where each object is directly represented by its 3D keypoint. Then 3D properties such as object size and orientation can be intuitively inferred from the 3D voxel features at the center location by a regression branch. Compared to [23], CenterNet-3D avoids estimating depth values of 3D boxes directly from 2D features, leading to improved detection accuracy.\n\n4\n\n02 | 00s | 03 k Ady Local PCA-SDF bases\n\n(a)\n\n(b)\n\n(c)\n\nFigure 3: 2D examples of (a) DeepSDF [59], (b) DeepLS [30], and (c) our local PCA-SDF shape representation. DeepSDF describes the surfaces with global shape codes. The SDF function f in DeepLS outputs a scalar value conditional on the local latent code zi and local coordinate x. However, its inference is computationally expensive since it requires forward pass through f for every x. Our shape representation consists of coarse-level voxelization and ﬁne-level local PCA-SDF. The coarse-level voxelization holistically represents the whole 3D surface with binary values. To further represent ﬁne-level surfaces, we propose a novel local PCA-SDF model, representing any occupied voxel as a linear combination of regular SDF function bases, which enables a more efﬁcient and accurate representation than DeepLS.\n\n3D Keypoint Branch. The 3D keypoint branch takes the voxel features G as input and predicts a 3D heatmap Y ∈ RX×Y ×Z×C (see Fig. 2), where C is the number of object categories. The values of each voxel in Y indicates how likely the 3D centroid of a certain object category exists at the voxel center. By computing the local maxima and ﬁltering via a threshold, we obtain a preliminary estimation of the 3D centroids, denoted as ˜c3d = [xc,yc,zc]T. To remedy the discretization error of voxels, the regression branch additionally predicts a local offset to ˜c3d, which is discussed next.\n\nRegression Branch. The regression branch predicts the essential properties to construct a 3D bound- ing box for each voxel of the 3D heatmap. We parameterize a 3D box as the prior works [8,19] and set up the world system located at the camera center with its vertical (y-) axis perpendicular to the ﬂoor and its forward (z-) axis toward the camera, such as the pitch and roll angles could be included in the camera pose. Speciﬁcally the 3D box is encoded as a 8-tuple τ = [δxc,δyc,δzc,δh,δw,δl,sinθ,cosθ]. Here ∆c3d = [δxc,δyc,δzc]T denotes the 3D center offset compensating voxel discretization. [l,h,w]T = [¯l · eδl,¯h · eδh, ¯w · eδw]T represents the object size, where [¯l,¯h, ¯w]T is a pre-calculated category-wise average box size, [δh,δw,δl] represents the corresponding transformations. θ denotes the rotation angle around y-axis. Here the network estimates the vectorial representation of rotation angle θ [72]. The output size of the regression branch is thus X × Y × Z × 8. Given the outputs of keypoint and regression branches, the 3D bounding box B ∈ R3×8 can be restored as 8 corners:\n\nf 1/2\n\nB = Rθ ±h/2 + c3d, c3d = ˜c3d + ∆c3d, ±w/2 (1)\n\nwhere Rθ ∈ R3×3 is the rotation matrix.",
            "section": "other",
            "section_idx": 2,
            "citations": [
                "28",
                "23",
                "23",
                "59",
                "30",
                "8,19",
                "72"
            ]
        },
        {
            "text": "3.3 Coarse-to-Fine 3D Reconstruction\n\nOur reconstruction module is based on a coarse-to-ﬁne shape representation, which consists of two components: coarse-level voxelization and ﬁne-level local PCA-SDF.\n\nCoarse-Level Voxelization. Based on the extracted 3D voxel features G, we ﬁrst estimate a coarse-level voxelization ˜V by a speciﬁc branch. The coarse-level voxelization holistically represents the whole 3D surface with binary occupancy values, where the unoccupied voxels cover “air\" in the scene, and occupied voxels can be either fully occupied ones inside the object, or voxels intersecting with the object’s surface. For the occupied voxels of both types, we further reconstruct a ﬁne-level local shape via the local PCA-SDF.\n\nLocal PCA-SDF Shape Representation. Recent works such as DeepSDF [59] aims to learn global implicit functions to represent shapes (see Fig. 3(a)). However, representing the entire objects with a single latent code often results in loss of details, which limits its application for scene-level object reconstruction. DeepLS [30] represents 3D surfaces by a set of independent latent codes on a regular\n\n5\n\ngrid (see Fig. 3(b)). Each latent code zi, concatenated with any point location x, can be decoded into a SDF value si by the learned implicit network f: si = f(zi,x). However, this shape representation has two limitations. i) Inference is inefﬁcient (in the order of seconds) since every point of a test voxel (e.g., 2563 points) is required to be sent to f for SDF calculation, making it unsuitable for real-time applications. ii) Our key observation is that local voxels, either within an object instance or across different categories, share similar local shapes, e.g., voxels across a table’s surface all have planar shapes. However, DeepLS treats voxels as independent training samples, without fully leveraging such local shape priors in training. To address these issues, we propose a novel local PCA-SDF shape representation, which represents each voxel shape as a linear combination of a set of implicit volumetric prototypes, leading to signiﬁcantly ﬁner reconstruction and 10× faster inference speed than DeepLS (see Tab. 4).\n\nFormally, as shown in Fig. 3(c), for each occupied voxel V, we deﬁne a regular lattice q ∈ Rk×k×k×3\n\nand compute their SDFs s ∈ Rk×k×k×1 toward the surface. By collecting SDFs of NS occupied voxels from the training surfaces, we apply Principal Component Analysis (PCA) to ﬁnd lB (lB << k3) local shape bases, SB ∈ Rk×k×k×lb. As such, given the learned SB and the latent code zi, any local shape Si of the underlying surface can be implicitly represented by Si = SBzi. The latent code zi for Si can be generated by the corresponding voxel feature Gi via zi = MLP(Gi). MLP is a mapping network, implemented with two fully-connected layers. By combining the contributions of all the occupied voxels, we can infer a global iso-surface from the SDF ﬁeld. Similar to DeepLS [30], we apply a 1.5 times receptive ﬁeld strategy to mitigate the inconsistent surface predictions at the voxel boundaries (Fig. 3(c)). Accordingly, during inference, we could apply average pooling to combine the SDF values for the boundary area. It is worth mentioning that our local PCA-SDF also allows reconstruction at resolutions higher than the one used during training by simply applying trilinear interpolation on the learned local shape bases SB.",
            "section": "other",
            "section_idx": 3,
            "citations": [
                "59",
                "30",
                "30"
            ]
        },
        {
            "text": "Rotation\n\nTranslation\n\n0dr -0.05r\n\n0\n\n_—0.05r__O.1r\n\n4° 0.0209 | 0.0204 | 0.0210 | 0.0213 | 0.0271\n\n2 0.0231 | 0.0202 | 0.0254 | 0.0215 | 0.0199\n\n0.0205 | 0.0188 | 0.0248 | 0.0203 | 0.0214 oe\n\n» 0.0206 } 0.0235 | 0.0205 | 0.0211 | 0.0214\n\nfa 0.0209 | 0.0211 | 0.0210 | 0.0196 | 0.0226\n\n(a)\n\n(b)\n\n(c)\n\nFigure 5: (a) Qualitative comparison on Pix3D. Our reconstructions closely match objects’ genuine shape, e.g., the table legs and chair arms. (b) Explained variation of our PCA-SDF representation with three voxel sizes r. (c) Reconstruction errors (Chamfer Distance-L2) of PCA-SDF w.r.t. translated and rotated 3D shapes.\n\nInput CoReNet PHS Proposed Ground-truth\n\nFigure 6: Qualitative results on real images from ScanNet-MDR. Our reconstructions closely match the objects than CoReNet [22]. Moreover, our method performs better for reconstruction of the truncated objects.\n\nindicates that our model performs well on reconstructing the overall shapes of objects. Qualitative results of detection and reconstruction are shown in Fig. 4.\n\n4.2 Single Object Reconstruction on Pix3D\n\nWe further compare to CoReNet [22] and Points2Objects [23] on the real image database, Pix3D [74] in the same protocol (splits S1 and S2) as in [21]. In this experiment, we train our model with the same experimental setting and the same pre-computed PCA-SDF bases as in Sec. 4.1. On average IoU over all 9 object classes, we achieve 38.6% vs. 34.1% (CoReNet) vs. 33.3% (Points2Objects) on S1 and 28.6% vs. 26.3% (CoReNet) vs. 23.6% (Points2Objects) on S2. The results demonstrate that our approach improves over baselines on real images. Qualitative results are shown in Fig. 5(a).\n\n4.3 Multiple Object Detection and Reconstruction on ScanNet-MDR\n\nDataset. Since there is no benchmark providing both 3D CAD models and 3D bounding boxes for multiple objects within a single real image, we assemble a dataset with 18,000 real images from the\n\n8\n\nTable 2: Comparisons of 3D object detec- tion and reconstruction on ScanNet-MDR dataset. [Key: D=CenterNet, @=CenterNet-3D], D=DeepLS, @z=Local PCA-SDF]\n\nTable 3: Effect of the voxel size r and latent code size lB in detection and reconstruction on ScanNet-MDR dataset (mAP/IoU), and inference time per image.\n\nMethod Detection | Recon. Evaluation _ ® @ |@® © | maP(@o.15) | tov ; 'a}) 16 | 32 | 64 on ms CoReNet [22] - = - 35.2 = = Proposed-I v = 36.5 0.64m 17.5/ 177] 18.1/ 44.2 Proposed-2 v 20.7 - 30.7 | 31.1 | 31.4 Proposed-3 v v 19.5 36.9 032m | 1-4/7] 18:87] 18-67 | s 6 Proposed-4 v |v 20.0 35.4 : 34.1 | 34.8 | 35.3 . Proposed-w/o PE v v 21.2 37.2 21.9/ | 22.2/ | 22.87 |). Proposed v v 22.8 38.2 916m | ‘371 | 376 | 382 |) 7!\n\nScanNet [75], termed ScanNet Monocular Detection and Reconstruction (ScanNet-MDR) dataset. For each object in an image, its CAD model is produced by [76]. We then generate the corresponding 3D bounding box label and camera calibration matrix. Unlike the ShapeNet-pairs or ShapeNet-triplets datasets, all the 3D objects in ScanNet-MDR are at absolute scale. Additionally, this dataset contains greater diversity including 19 object categories (bag, basket, bathtub, bed, bench, bookshelf, cabinet, chair, display, ﬁle, lamp, microwave, piano, printer, sofa, stove, table, trash and washer). We split the data into 80% for training and 20% for testing.\n\nExperimental Settings. In this dataset, we use a voxel grid of size 10.28 × 3.2 × 6.4m (X = 64,Y = 20,Z = 40, r = 0.16m), a minimum to encompass all annotated 3D objects in the dataset. The PCA-SDF is pre-computed with NS ≈ 560,000 occupied voxels from 200 training surfaces. We set k = 17, C = 19 and lB = 64. For comparison, we train CoReNet [22] using the released code on our training data. We use mAP with 3D box IoU threshold of 0.15 as detection metric [19], and global 3D IoU on a 1283 voxel grid as reconstruction metric.\n\nResults and Ablation Studies. We report detection and reconstruction results on the testing set. As shown in Tab. 2, our method signiﬁcantly improves over CoReNet [22] on 3D reconstruction and advances the ablated versions on both detection and reconstruction. Qualitative results are shown in Fig. 6. CoReNet, as an image-to-voxel reconstruction network without special design for feature smearing issue, cannot handle truncated objects in the input image.\n\nJoint Framework vs. Separate Modules. Moreover, Tab. 2 shows the ablation results of our models without detection (Proposed-1) or reconstruction (Proposed-2) modules, where one can conclude that joint framework in this work performs better than solving either task exclusively.\n\nCenterNet-3D and PCA-SDF. To further validate the effectiveness of the proposed CenterNet-3D detector over the conventional CenterNet, we train a model (Proposed-3) by combining the recon- struction module with the conventional CenterNet, which formulates 3D detection as a problem of 2D keypoint detection directly from the pixel-based image features. As compared in Tab. 2, our model outperforms Proposed-3 in both detection and reconstruction. To compare PCA-SDF with DeepLS [30] in the joint detection and reconstruction framework, we train a model (Proposed-4) by using DeepLS representation as our ﬁne-level reconstruction module. Tab. 2 shows that both detection and reconstruction performances are worse than ours.\n\nEffect on Voxel Size r Latent Code Size lB. The validity of local shape pair, expressed by PCA-SDF in our work, depends on the voxel size. For instance, we show the percentage of explained variation for three voxel sizes in Fig. 5(b). As larger voxel sizes are used, the ﬁrst few bases could explain less variation, due to the diminished local shape similarity among larger voxels, i.e., weakened local shape prior. This is also validated by the ablation of voxel size r and latent code size lB in Tab. 3, where larger voxel sizes lead to lower detection and reconstruction accuracies. On the other hand, a larger latent code size results in better representation power (Fig. 5(b)) but not necessarily reconstruction, since it imposes a more challenging task for the network to predict a higher-dim code.\n\nEffect on Positional Encoding. To investigate the effect of positional encoding operator on 3D detection and reconstruction, we retrain a model without the positional encoding (Proposed-w/o PE). As compared in Tab. 2, both detection and reconstruction accuracies are worse than ours, which indicates that the positional encoding indeed enhances the voxel feature representation.\n\n9\n\nTable 4: Comparison of reconstructing 3D shapes from ShapeNet test set, evaluated by Chamfer Distance-L2 (multiplied by 103). PCA-SDF achieves higher accuracy and efﬁciency than DeepLS even with fewer decoder and representation parameters. Decoder para. refer to the decoder network parameters for DeepSDF or DeepLS, and PCA bases for our PCA-SDF. [Key: Best, Second Best]\n\nMethod Chair Plane Table Lamp Sofa Mean Unseen #Decoder Para. (M) #Represent. Para. (K) DeepSDF [59] 0.204 0.143 0.553 0.832 0.132 0.372 - 1.8 0.3 DeepLS [30] (lB=125) 0.030 0.018 0.032 0.078 0.044 0.040 - 0.05 4096 PCA-SDF (lB=32) 0.031 0.016 0.033 0.035 0.032 0.029 0.111 0.02 1049 PCA-SDF (lB=64) 0.027 0.012 0.030 0.027 0.030 0.025 0.059 0.05 2097 PCA-SDF (lB=125) 0.026 0.010 0.029 0.016 0.029 0.022 0.028 0.09 4096 Inference Time (s) 6.9626 0.8081 0.0126 0.0129 0.0132\n\nComputation Time. Tab. 3 validates our inference time per image with different voxel sizes on a GTX 1080Ti GPU. Since lB does not affect the runtime much, we show the average time across three lB.",
            "section": "other",
            "section_idx": 4,
            "citations": [
                "22",
                "22",
                "23",
                "74",
                "21",
                "22",
                "75",
                "76",
                "22",
                "19",
                "22",
                "30",
                "59",
                "30"
            ]
        },
        {
            "text": "[39] Yousef Atoum, Joseph Roth, Michael Bliss, Wende Zhang, and Xiaoming Liu. Monocular video-based trailer coupler detection using multiplexer convolutional neural network. In ICCV, 2017.\n\n[40] Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G Berneshawi, Huimin Ma, Sanja Fidler, and Raquel Urtasun. 3D object proposals for accurate object class detection. In NeurIPS, 2015.\n\n[41] Bin Xu and Zhenzhong Chen. Multi-level fusion based 3D object detection from monocular images. In CVPR, 2018.\n\n[42] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Manuel López-Antequera, and Peter Kontschieder. Disentangling monocular 3D object detection. In ICCV, 2019.\n\n[43] Lijie Liu, Jiwen Lu, Chunjing Xu, Qi Tian, and Jie Zhou. Deep ﬁtting degree scoring network for monocular 3D object detection. In CVPR, 2019.\n\n[44] Wongun Choi, Yu-Wei Chao, Caroline Pantofaru, and Silvio Savarese. Understanding indoor scenes using 3D geometric phrases. In CVPR, 2013.\n\n[45] Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, and Song-Chun Zhu. Holistic 3D scene parsing and reconstruction from a single RGB image. In ECCV, 2018.\n\n[46] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An information-rich 3D model repository. arXiv preprint arXiv:1512.03012, 2015.\n\n12\n\n[47] Zi-Hang Jiang, Qianyi Wu, Keyu Chen, and Juyong Zhang. Disentangled representation learning for 3D face shape. In CVPR, 2019.\n\n[48] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J Black. Generating 3D faces using convolutional mesh autoencoders. In ECCV, 2018.\n\n[49] Feng Liu, Luan Tran, and Xiaoming Liu. 3D face modeling from diverse raw scan data. In ICCV, 2019.\n\n[50] Timur Bagautdinov, Chenglei Wu, Jason Saragih, Pascal Fua, and Yaser Sheikh. Modeling facial geometry using compositional VAEs. In CVPR, 2018.\n\n[51] Angela Dai, Charles Ruizhongtai Qi, and Matthias Nießner. Shape completion using 3D- encoder-predictor CNNs and shape synthesis. In CVPR, 2017.\n\n[52] David Stutz and Andreas Geiger. Learning 3D shape completion from laser scan data with weak supervision. In CVPR, 2018.\n\n[53] Feng Liu, Luan Tran, and Xiaoming Liu. Fully understanding generic objects: Modeling, segmentation, and reconstruction. In CVPR, 2021.\n\n[54] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In ICCV, 2017.\n\n[55] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3D classiﬁcation and segmentation. In CVPR, 2017.\n\n[56] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3D mesh models from single RGB images. In ECCV, 2018.\n\n[57] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3D-R2N2: A uniﬁed approach for single and multi-view 3D object reconstruction. In ECCV, 2016.\n\n[58] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D reconstruction in function space. In CVPR, 2019.\n\n[59] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representation. In CVPR, 2019.\n\n[60] Feng Liu and Xiaoming Liu. Learning implicit functions for topology-varying dense 3D shape correspondence. In NeurIPS, 2020.\n\n[61] Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In ICCV, 2019.\n\n[62] Boyang Deng, Kyle Genova, Soroosh Yazdani, Soﬁen Bouaziz, Geoffrey Hinton, and Andrea Tagliasacchi. CvxNet: Learnable convex decomposition. In CVPR, 2020.\n\n[63] Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. BSP-Net: Generating compact meshes via binary space partitioning. In CVPR, 2020.\n\n[64] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nießner, Thomas Funkhouser, et al. Local implicit grid representations for 3D scenes. In CVPR, 2020.\n\n[65] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. In CVPR, 2021.\n\n[66] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel ﬁelds. In NeurIPS, 2020.\n\n[67] Mateusz Michalkiewicz, Eugene Belilovsky, Mahsa Baktashmotlagh, and Anders Eriksson. A simple and scalable shape representation for 3d reconstruction. In BMVC, 2020.\n\n[68] Daniel Ricao Canelhas, Erik Schaffernicht, Todor Stoyanov, Achim J Lilienthal, and Andrew J Davison. Compressed voxel-based mapping using unsupervised learning. Robotics, 2017.\n\n[69] Danhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan Taylor, Sean Fanello, Cem Keskin, Adarsh Kowdle, Soﬁen Bouaziz, et al. Real-time compression and streaming of 4D performances. TOG, 2018.\n\n[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\n13\n\n[71] Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3D U-Net: learning dense volumetric segmentation from sparse annotation. In MICCAI, 2016.\n\n[72] Zechen Liu, Zizhang Wu, and Roland Tóth. Smoke: Single-stage monocular 3d object detection via keypoint estimation. In CVPRW, 2020.\n\n[73] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In ICCV, 2017.\n\n[74] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum, and William T Freeman. Pix3D: Dataset and methods for single-image 3D shape modeling. In CVPR, 2018.\n\n[75] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3D reconstructions of indoor scenes. In CVPR, 2017.",
            "section": "other",
            "section_idx": 5,
            "citations": [
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45",
                "46",
                "47",
                "48",
                "49",
                "50",
                "51",
                "52",
                "53",
                "54",
                "55",
                "56",
                "57",
                "58",
                "59",
                "60",
                "61",
                "62",
                "63",
                "64",
                "65",
                "66",
                "67",
                "68",
                "69",
                "70",
                "71",
                "72",
                "73",
                "74",
                "75"
            ]
        },
        {
            "text": "[76] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X Chang, and Matthias Nießner. Scan2CAD: Learning CAD model alignment in RGB-D scans. In CVPR, 2019.\n\n14",
            "section": "other",
            "section_idx": 6,
            "citations": [
                "76"
            ]
        }
    ],
    "figures": [
        {
            "path": "output\\images\\915321dc-4b45-4253-ae51-c1e59a7a396e.jpg",
            "description": "This figure illustrates the architecture and process flow of a 3D reconstruction system from 2D images. It is crucial for understanding the methodology used in the research. The figure is divided into several key components:\n\n- **Image Input:** Starts with an input image \\( I \\) with dimensions \\( W \\times H \\times 3 \\).\n\n- **Feature Backbone:** Extracts feature maps \\( F \\) from the input image, transforming it into a higher-dimensional space \\( W_F \\times H_F \\times D \\).\n\n- **2D-to-3D Feature Lifting:** Converts 2D feature maps into 3D voxel features \\( G \\), using a 3D U-Net with positional encoding and camera calibration.\n\n- **CenterNet-3D Detector:** Performs object detection using 3D heatmaps and regression to identify 3D bounding boxes.\n\n- **Coarse-to-Fine 3D Reconstruction:** Refines the detected objects through coarse-level voxelization and local PCA-SDF representation, resulting in detailed 3D models.\n\nThe figure uses a mix of algorithmic flowcharts, mathematical notations, and visual representations to convey the system's architecture, making it an important visualization for understanding the core methodology and innovation presented in the research.",
            "importance": 9
        },
        {
            "path": "output\\images\\22939fd3-7b96-4fa6-9c27-2128138589a1.jpg",
            "description": "The figure presents a comparison between real-world images and their corresponding 3D models, focusing on object detection and 3D bounding box representations. The images on the left side of each pair show real indoor scenes with furniture, such as sofas and chairs, highlighted by colored bounding boxes. The images on the right depict the 3D models of these objects with aligned bounding boxes, likely indicating how the model interprets and reconstructs these objects in a virtual space. This visualization is crucial for understanding the effectiveness of the algorithm in translating 2D images into 3D spaces, demonstrating the system's capability in object recognition and spatial reconstruction.",
            "importance": 9
        },
        {
            "path": "output\\images\\9f77d504-353f-4d21-be9e-e1404210bd20.jpg",
            "description": "This figure illustrates a comparison between two methods, \"Points2Objects\" and a \"Proposed\" method, for generating 3D models from input images. It consists of two rows, each with three columns. The first column shows the \"Input Image,\" which is a photograph of an object—a chair in the first row and a table in the second row. The middle column shows the 3D model output of the \"Points2Objects\" method, represented within a bounding box. The third column displays the 3D model produced by the \"Proposed\" method, also within a bounding box. The comparisons suggest that the proposed method may offer improvements or differences in the reconstruction of the 3D models from the input images, potentially highlighting the effectiveness or innovation of the new approach. This figure is likely critical for understanding the key advancements or contributions of the research, making it an important part of the paper's methodology and results discussion.",
            "importance": 9
        },
        {
            "path": "output\\images\\eb39134c-1290-46b3-8cdb-9a840f506bbb.jpg",
            "description": "This figure illustrates a method involving voxel-based 3D representation using Principal Component Analysis (PCA) for Signed Distance Function (SDF) bases. On the left, a grid represents the division of a shape into multiple voxels. The highlighted \"Voxel i\" is shown in detail on the right, indicating its local neighborhood with a finer grid and overlaid data points (red dots). Below, the vector \\( Z_i \\) contains coefficients (0.2, 0.04, 0.3, etc.) that are used in combination with local PCA-SDF bases to reconstruct the shape within the voxel. The summation symbol suggests a mathematical operation, possibly combining these coefficients with the PCA bases to approximate the local shape. This figure is important for understanding the methodology of using PCA for efficient 3D shape representation and reconstruction in localized regions.",
            "importance": 8
        },
        {
            "path": "output\\images\\5c9d725e-288c-4409-a3ca-2cd014506eec.jpg",
            "description": "This figure illustrates a method for calculating a Signed Distance Function (SDF) using a voxel-based approach. Here's a breakdown of the components:\n\n- **Architecture Diagrams and Components**: \n  - The left part of the figure shows a grid of voxels, with a specific voxel highlighted. The grid likely represents a spatial partitioning of a 3D space.\n  - The inset zooms into a specific voxel, labeled \"Voxel i,\" showing its intersection with a surface (orange shape).\n\n- **Mathematical Formulas or Concepts**:\n  - The formula \\( s_i = f(z_i, x) \\) is presented, indicating that the SDF \\( s_i \\) is a function of both a latent vector \\( z_i \\) and a position \\( x \\).\n  - The function \\( f \\) is depicted as a blue triangle, representing a computational process that outputs the SDF value.\n\n- **Algorithm Flowcharts or Processes**:\n  - The flow from \\( x \\) and \\( z_i \\) through the function \\( f \\) to produce the SDF is shown, indicating the transformation process.\n\n- **Results or Findings**:\n  - The diagram likely explains how the SDF is computed for different points within a voxel, contributing to understanding the spatial characteristics of the object represented by the grid.\n\nThis figure is important as it visualizes a key methodological component, demonstrating how the SDF is derived from voxel-based data, which is crucial for understanding the paper's approach to spatial representation.",
            "importance": 8
        },
        {
            "path": "output\\images\\7434c97a-1462-4751-ab85-959078eb80c8.jpg",
            "description": "This figure illustrates a process involving a Signed Distance Function (SDF) represented by a mathematical formula \\( s = f(z, x) \\). On the left side, a geometric shape (a crescent) is shown, with a point labeled \\( x \\) and a distance \\( s \\) to the shape's boundary. The right side depicts a diagram with inputs \\( x \\) (a point in space) and \\( z \\) (possibly a latent vector or parameter), which are fed into a function \\( f \\). The output is the SDF value, indicating the distance from the point \\( x \\) to the surface of the shape. This figure is important for understanding the methodology of using SDFs within the context of shape representation or analysis, likely a key component of the research.",
            "importance": 8
        },
        {
            "path": "output\\images\\50c098e0-5abf-4b54-bf7c-849933315883.jpg",
            "description": "The figure appears to showcase a series of 3D models or visualizations, likely related to computer vision or object recognition research. Each sub-image seems to focus on different objects, possibly highlighting segmentation or classification processes. The use of distinct colors for different parts might indicate the identification and differentiation of various components within a scene or object. This visualization is potentially important for understanding how the research addresses the segmentation or recognition of objects in a 3D space, which is a key aspect of methodologies in fields like autonomous navigation, robotics, or augmented reality.",
            "importance": 7
        },
        {
            "path": "output\\images\\81f1ddb8-2dd4-4efc-ab16-e9c8f6febc97.jpg",
            "description": "This figure is a graph depicting the percentage of explained variation as a function of a variable denoted as \\( l_B \\). The x-axis represents \\( l_B \\), which seems to range from 1 to 10, while the y-axis shows the percentage of explained variation ranging from 20% to 100%. Three distinct curves are plotted, each corresponding to different values: 0.16m, 0.32m, and 0.64m, as indicated by the legend. The curves show how the explained variation increases with \\( l_B \\), with higher initial percentages for smaller values and converging near 100% as \\( l_B \\) increases. This suggests that the variable \\( l_B \\) has a significant impact on the explained variation, and the comparison of these three curves could be essential for understanding the influence of different conditions or parameters in the study.",
            "importance": 7
        },
        {
            "path": "output\\images\\af792a1b-2523-4566-ae4e-79b42d8ea9cb.jpg",
            "description": "I'm unable to analyze or provide a description of this image as it appears to be a collection of photographs rather than a technical figure from a research paper. If you have a specific technical figure or diagram related to research, please provide that for analysis.",
            "importance": 5
        },
        {
            "path": "output\\images\\3ce1d368-b7b9-4909-8a7b-a51eee290992.jpg",
            "description": "I'm unable to view or analyze the content of specific images, including technical figures from research papers. However, I can help with general guidance on how to analyze a figure like this. Consider the following aspects:\n\n1. **Architecture Diagrams and Components**: Look for labeled parts that show system architecture or components, such as network layers or modules.\n\n2. **Graphs, Charts, and Data Visualizations**: Identify any charts or graphs indicating performance metrics, comparisons, or trends.\n\n3. **Mathematical Formulas or Concepts**: Check for equations or annotations that explain mathematical modeling or theories.\n\n4. **Algorithm Flowcharts or Processes**: Look for step-by-step processes or decision-making paths that illustrate algorithm functionality.\n\n5. **Results or Findings**: Note any data or visuals that summarize experimental results or key findings.\n\nFor rating importance, you should consider how central the figure is to the paper’s main thesis or findings.\n\nIf you provide a description of the figure, I can help you further analyze its importance and content.",
            "importance": 5
        }
    ],
    "metadata": {
        "key_themes": [
            "PCA-SDF Shape Representation",
            "Synthetic dataset generation",
            "Cross-category generalization",
            "Multivariable calculus",
            "RGB-D Data",
            "Distance Metrics",
            "3D Reconstruction"
        ],
        "methodology": [
            "Linear regression",
            "Distance Metrics",
            "Generative Deep Learning",
            "Feature Fusion",
            "Interpolation",
            "Variational Autoencoder (VAE)",
            "Categorical Cross Entropy Loss",
            "Multi-scale encoding",
            "Geometric Regression",
            "Scan2CAD dataset",
            "Principal Component Analysis (PCA)",
            "3D IoU",
            "Average Error",
            "Reconstruction error",
            "Machine learning",
            "Gradient-based optimization",
            "3D Shape Estimation",
            "Camera superpixel processing",
            "Loss Function"
        ],
        "domain": [
            "3D Computer Vision",
            "Autonomous technology",
            "Vehicle detection"
        ],
        "strengths": [
            "Efficiently outperforms existing methods",
            "Local feature learning",
            "Point cloud optimization",
            "Geometric invariance",
            "Deep Learning Toolbox",
            "Gaussian Surface Normal Estimation",
            "MultiBin Optimization",
            "Generalization across diverse object classes",
            "3D Shape Reconstruction",
            "Enhanced performance in various evaluation metrics"
        ],
        "limitations": [
            "Overfitting",
            "Limitation in diversity detection",
            "Resolution limitations",
            "Limited image supervision",
            "CAD model dependency",
            "Computational complexity",
            "Ambiguity in 3D reconstruction",
            "Implicit Representation",
            "Limited dataset requirements",
            "Generative Model Inefficiency"
        ]
    },
    "content_chunks": [
        {
            "chunk_id": "2111.03098v1_chunk_0",
            "section": "methodology",
            "citations": [
                "24,25,27",
                "70",
                "71"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_1",
            "section": "methodology",
            "citations": [
                "28,73"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_2",
            "section": "methodology",
            "citations": [
                "22",
                "23",
                "22",
                "22",
                "23",
                "22",
                "46",
                "22",
                "23",
                "23",
                "22,23",
                "23"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_3",
            "section": "methodology",
            "citations": [
                "59",
                "30",
                "30"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_4",
            "section": "methodology",
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33",
                "34",
                "35",
                "36",
                "37",
                "38"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_5",
            "section": "other",
            "citations": [
                "1,2",
                "3",
                "24",
                "25,26",
                "27",
                "25",
                "26",
                "24",
                "27",
                "28,29",
                "30"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_6",
            "section": "other",
            "citations": [
                "34,35,38",
                "46",
                "53",
                "18",
                "21",
                "54",
                "19",
                "22",
                "23",
                "22,23",
                "55",
                "16,56",
                "13,57",
                "61",
                "62",
                "63",
                "64,65",
                "30,66",
                "67",
                "68,69"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_7",
            "section": "other",
            "citations": [
                "28",
                "23",
                "23",
                "59",
                "30",
                "8,19",
                "72"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_8",
            "section": "other",
            "citations": [
                "59",
                "30",
                "30"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_9",
            "section": "other",
            "citations": [
                "22",
                "22",
                "23",
                "74",
                "21",
                "22",
                "75",
                "76",
                "22",
                "19",
                "22",
                "30",
                "59",
                "30"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_10",
            "section": "other",
            "citations": [
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45",
                "46",
                "47",
                "48",
                "49",
                "50",
                "51",
                "52",
                "53",
                "54",
                "55",
                "56",
                "57",
                "58",
                "59",
                "60",
                "61",
                "62",
                "63",
                "64",
                "65",
                "66",
                "67",
                "68",
                "69",
                "70",
                "71",
                "72",
                "73",
                "74",
                "75"
            ]
        },
        {
            "chunk_id": "2111.03098v1_chunk_11",
            "section": "other",
            "citations": [
                "76"
            ]
        }
    ]
}