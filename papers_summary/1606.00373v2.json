{
    "basic_info": {
        "title": "Deeper Depth Prediction with Fully Convolutional Residual Networks",
        "authors": [
            "Iro Laina",
            "Christian Rupprecht",
            "Vasileios Belagiannis",
            "Federico Tombari",
            "Nassir Navab"
        ],
        "paper_id": "1606.00373v2",
        "published_year": 2016,
        "references": [
            "1411.6387v2",
            "1502.03167v3"
        ]
    },
    "detailed_references": {
        "ref_1": {
            "text": "R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua,\nand S. Susstrunk. SLIC superpixels compared to\nstate-of-the-art superpixel methods. Pattern Anal-\nysis and Machine Intelligence, IEEE Transactions\non, 34(11):2274–2282, 2012. 2",
            "type": "numeric",
            "number": "1",
            "arxiv_id": null
        },
        "ref_2": {
            "text": "V . Belagiannis, C. Rupprecht, G. Carneiro, and\nN. Navab. Robust optimization for deep regres-\nsion. Proceedings of the International Conference\non Computer Vision , 2015. 6",
            "type": "numeric",
            "number": "2",
            "arxiv_id": null
        },
        "ref_3": {
            "text": "N. Dalal and B. Triggs. Histograms of oriented\ngradients for human detection. In Proc. Conf.\nComputer Vision and Pattern Recognition (CVPR) ,\npages 886–893, 2005. 2",
            "type": "numeric",
            "number": "3",
            "arxiv_id": null
        },
        "ref_4": {
            "text": "A. Dosovitskiy, J. Tobias Springenberg, and\nT. Brox. Learning to generate chairs with convo-\nlutional neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition , pages 1538–1546, 2015. 4",
            "type": "numeric",
            "number": "4",
            "arxiv_id": null
        },
        "ref_5": {
            "text": "D. Eigen and R. Fergus. Predicting depth, surface\nnormals and semantic labels with a common multi-\nscale convolutional architecture. In Proc. Int. Conf.\nComputer Vision (ICCV) , 2015. 1, 2, 3, 4, 6, 7, 8, 9",
            "type": "numeric",
            "number": "5",
            "arxiv_id": null
        },
        "ref_6": {
            "text": "D. Eigen, C. Puhrsch, and R. Fergus. Prediction\nfrom a single image using a multi-scale deep net-\nwork. In Proc. Conf. Neural Information Process-\ning Systems (NIPS) , 2014. 2, 3, 4, 6, 7",
            "type": "numeric",
            "number": "6",
            "arxiv_id": null
        },
        "ref_7": {
            "text": "K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-\nual learning for image recognition. arXiv preprint\narXiv:1512.03385 , 2015. 2, 4, 5",
            "type": "numeric",
            "number": "7",
            "arxiv_id": "1512.03385"
        },
        "ref_8": {
            "text": "D. Hoiem, A. Efros, M. Hebert, et al. Geometric\ncontext from a single image. In Computer Vision,\n2005. ICCV 2005. Tenth IEEE International Con-\nference on , volume 1, pages 654–661. IEEE, 2005.\n2",
            "type": "numeric",
            "number": "8",
            "arxiv_id": null
        },
        "ref_9": {
            "text": "S. Ioffe and C. Szegedy. Batch normalization: Ac-\ncelerating deep network training by reducing inter-\nnal covariate shift. In Proceedings of The 32nd\nInternational Conference on Machine Learning ,\npages 448–456, 2015. 4",
            "type": "numeric",
            "number": "9",
            "arxiv_id": null
        },
        "ref_10": {
            "text": "K. Karsch, C. Liu, and S. B. Kang. Depth extrac-\ntion from video using non-parametric sampling. In\nProc. Europ. Conf. Computer Vision (ECCV) , pages\n775–788, 2012. 2, 3, 7, 9",
            "type": "numeric",
            "number": "10",
            "arxiv_id": null
        },
        "ref_11": {
            "text": "M. Keller, D. Leﬂoch, M. Lambers, S. Izadi,\nT. Weyrich, and A. Kolb. Real-time 3d reconstruc-\ntion in dynamic scenes using point-based fusion. In\nProc. Int. Conf. 3D Vision(3DV) , pages 1–8, 2013.\n10",
            "type": "numeric",
            "number": "11",
            "arxiv_id": null
        },
        "ref_12": {
            "text": "C. Kerl, J. Sturm, and D. Cremers. Robust odom-\netry estimation for RGB-D cameras. In Proc. Int.\nConf. on Robotics and Automation (ICRA) , 2013.\n10",
            "type": "numeric",
            "number": "12",
            "arxiv_id": null
        },
        "ref_13": {
            "text": "J. Konrad, M. Wang, and P. Ishwar. 2d-to-3d image\nconversion by learning depth from examples. In\nProc. Conf. Computer Vision and Pattern Recogni-\ntion Workshops , pages 16–22, 2012. 2, 3",
            "type": "numeric",
            "number": "13",
            "arxiv_id": null
        },
        "ref_14": {
            "text": "A. Krizhevsky, I. Sutskever, and G. E. Hinton. Ima-\ngenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in neural information pro-\ncessing systems , pages 1097–1105, 2012. 3, 4",
            "type": "numeric",
            "number": "14",
            "arxiv_id": null
        },
        "ref_15": {
            "text": "L. Ladicky, J. Shi, and M. Pollefeys. Pulling things\nout of perspective. In Proc. Conf. Computer Vi-\nsion and Pattern Recognition (CVPR) , pages 89–\n96, 2014. 2, 7",
            "type": "numeric",
            "number": "15",
            "arxiv_id": null
        },
        "ref_16": {
            "text": "B. Li, C. Shen, Y . Dai, A. V . den Hengel, and\nM. He. Depth and surface normal estimation\nfrom monocular images using regression on deep\nfeatures and hierarchical CRFs. In Proc. Conf.\nComputer Vision and Pattern Recognition (CVPR) ,\npages 1119–1127, 2015. 2, 3, 7, 9",
            "type": "numeric",
            "number": "16",
            "arxiv_id": null
        },
        "ref_17": {
            "text": "B. Liu, S. Gould, and D. Koller. Single im-\nage depth estimation from predicted semantic la-\nbels. In Computer Vision and Pattern Recognition\n(CVPR), 2010 IEEE Conference on , pages 1253–\n1260. IEEE, 2010. 2",
            "type": "numeric",
            "number": "17",
            "arxiv_id": null
        },
        "ref_18": {
            "text": "C. Liu, J. Yuen, and A. Torralba. Sift ﬂow: Dense\ncorrespondence across scenes and its applications.\nPattern Analysis and Machine Intelligence, IEEE\nTransactions on , 33(5):978–994, 2011. 2, 3Published at IEEE International Conference on 3D Vision (3DV) 2016",
            "type": "numeric",
            "number": "18",
            "arxiv_id": null
        },
        "ref_19": {
            "text": "F. Liu, C. Shen, and G. Lin. Deep convolutional\nneural ﬁelds for depth estimation from a single im-\nage. In Proc. Conf. Computer Vision and Pattern\nRecognition (CVPR) , pages 5162–5170, 2015. 2,\n3, 7, 9",
            "type": "numeric",
            "number": "19",
            "arxiv_id": null
        },
        "ref_20": {
            "text": "M. Liu, M. Salzmann, and X. He. Discrete-\ncontinuous depth estimation from a single image.\nInProc. Conf. Computer Vision and Pattern Recog-\nnition (CVPR) , pages 716–723, 2014. 2, 3, 7, 9, 10",
            "type": "numeric",
            "number": "20",
            "arxiv_id": null
        },
        "ref_21": {
            "text": "J. Long, E. Shelhamer, and T. Darrell. Fully con-\nvolutional networks for semantic segmentation. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pages 3431–3440,\n2015. 4",
            "type": "numeric",
            "number": "21",
            "arxiv_id": null
        },
        "ref_22": {
            "text": "R. Memisevic and C. Conrad. Stereopsis via deep\nlearning. In NIPS Workshop on Deep Learning ,\nvolume 1, 2011. 2",
            "type": "numeric",
            "number": "22",
            "arxiv_id": null
        },
        "ref_23": {
            "text": "P. K. Nathan Silberman, Derek Hoiem and R. Fer-\ngus. Indoor segmentation and support inference\nfrom RGBD images. In ECCV , 2012. 1, 2, 4, 6,\n7",
            "type": "numeric",
            "number": "23",
            "arxiv_id": null
        },
        "ref_24": {
            "text": "A. Oliva and A. Torralba. Modeling the shape of\nthe scene: A holistic representation of the spatial\nenvelope. Int. Journal of Computer Vision (IJCV) ,\npages 145–175, 2014. 2",
            "type": "numeric",
            "number": "24",
            "arxiv_id": null
        },
        "ref_25": {
            "text": "A. B. Owen. A robust hybrid of lasso and ridge\nregression. Contemporary Mathematics , 443:59–\n72, 2007. 6",
            "type": "numeric",
            "number": "25",
            "arxiv_id": null
        },
        "ref_26": {
            "text": "X. Ren, L. Bo, and D. Fox. Rgb-(d) scene label-\ning: Features and algorithms. In Computer Vision\nand Pattern Recognition (CVPR), 2012 IEEE Con-\nference on , pages 2759–2766. IEEE, 2012. 1",
            "type": "numeric",
            "number": "26",
            "arxiv_id": null
        },
        "ref_27": {
            "text": "A. Roy and S. Todorovic. Monocular depth estima-\ntion using neural regression forest. In Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition CVPR (CVPR) , 2016. 2, 3, 6,\n7",
            "type": "numeric",
            "number": "27",
            "arxiv_id": null
        },
        "ref_28": {
            "text": "O. Russakovsky, J. Deng, H. Su, J. Krause,\nS. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, A. C. Berg, and L. Fei-\nFei. ImageNet Large Scale Visual Recognition\nChallenge. International Journal of Computer Vi-\nsion (IJCV) , 115(3):211–252, 2015. 3, 6",
            "type": "numeric",
            "number": "28",
            "arxiv_id": null
        },
        "ref_29": {
            "text": "A. Saxena, S. H. Chung, and A. Y . Ng. Learning\ndepth from single monocular images. In Advances\nin Neural Information Processing Systems , pages\n1161–1168, 2005. 2",
            "type": "numeric",
            "number": "29",
            "arxiv_id": null
        },
        "ref_30": {
            "text": "A. Saxena, M. Sun, and A. Ng. Make3d: Learning\n3d scene structure from a single still image. IEEETrans. Pattern Analysis and Machine Intelligence\n(PAMI) , 12(5):824–840, 2009. 2, 6, 10",
            "type": "numeric",
            "number": "30",
            "arxiv_id": null
        },
        "ref_31": {
            "text": "K. Simonyan and A. Zisserman. Very deep convo-\nlutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556 , 2014. 3, 4",
            "type": "numeric",
            "number": "31",
            "arxiv_id": "1409.1556"
        },
        "ref_32": {
            "text": "F. H. Sinz, J. Q. Candela, G. H. Bakır, C. E. Ras-\nmussen, and M. O. Franz. Learning depth from\nstereo. In Pattern Recognition , pages 245–252.\nSpringer, 2004. 2",
            "type": "numeric",
            "number": "32",
            "arxiv_id": null
        },
        "ref_33": {
            "text": "S. Suwajanakorn and C. Hernandez. Depth from fo-\ncus with your mobile phone. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition , 2015. 1, 2",
            "type": "numeric",
            "number": "33",
            "arxiv_id": null
        },
        "ref_34": {
            "text": "R. Szeliski. Structure from motion. In Computer\nVision , Texts in Computer Science, pages 303–334.\nSpringer London, 2011. 1, 2",
            "type": "numeric",
            "number": "34",
            "arxiv_id": null
        },
        "ref_35": {
            "text": "J. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon.\nThe vitruvian manifold: Inferring dense correspon-\ndences for one-shot human pose estimation. In\nComputer Vision and Pattern Recognition (CVPR),\n2012 IEEE Conference on , pages 103–110. IEEE,\n2012. 1",
            "type": "numeric",
            "number": "35",
            "arxiv_id": null
        },
        "ref_36": {
            "text": "A. Vedaldi and K. Lenc. Matconvnet – convolu-\ntional neural networks for matlab. 2015. 6",
            "type": "numeric",
            "number": "36",
            "arxiv_id": null
        },
        "ref_37": {
            "text": "P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and\nA. L. Yuille. Towards uniﬁed depth and semantic\nprediction from a single image. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition , pages 2800–2809, 2015. 2, 3, 7,\n9",
            "type": "numeric",
            "number": "37",
            "arxiv_id": null
        },
        "ref_38": {
            "text": "M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive\ndeconvolutional networks for mid and high level\nfeature learning. In Computer Vision (ICCV), 2011\nIEEE International Conference on , pages 2018–\n2025. IEEE, 2011. 4",
            "type": "numeric",
            "number": "38",
            "arxiv_id": null
        },
        "ref_39": {
            "text": "R. Zhang, P.-S. Tsai, J. E. Cryer, and M. Shah.\nShape-from-shading: a survey. Pattern Analysis\nand Machine Intelligence, IEEE Transactions on ,\n21(8):690–706, 1999. 1, 2",
            "type": "numeric",
            "number": "39",
            "arxiv_id": null
        },
        "ref_40": {
            "text": "L. Zwald and S. Lambert-Lacroix. The berhu\npenalty and the grouped effect. arXiv preprint\narXiv:1207.6868 , 2012. 2, 6",
            "type": "numeric",
            "number": "40",
            "arxiv_id": "1207.6868"
        },
        "ref_Springer_2004": {
            "text": "stereo. In Pattern Recognition , pages 245–252. Springer, 2004. 2 [33] S. Suwajanakorn and C. Hernandez. Depth from fo-",
            "type": "author_year",
            "key": "Springer, 2004",
            "arxiv_id": null
        },
        "ref_London_2011": {
            "text": "Springer London, 2011. 1, 2 [35] J. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon.",
            "type": "author_year",
            "key": "London, 2011",
            "arxiv_id": null
        }
    },
    "raw_chunks": [
        {
            "text": "3. Methodology\n\nIn this section, we describe our model for depth prediction from a single RGB image. We ﬁrst\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\npresent the employed architecture, then analyze the new components proposed in this work. Subse- quently, we propose a loss function suitable for the optimization of the given task.\n\n3.1. CNN Architecture\n\nAlmost all current CNN architectures contain a contractive part that progressively decreases the input image resolution through a series of con- volutions and pooling operations, giving higher- level neurons large receptive ﬁelds, thus capturing more global information. In regression problems in which the desired output is a high resolution im- age, some form of up-sampling is required in order to obtain a larger output map. Eigen et al. [5, 6], use fully-connected layers as in a typical classiﬁ- cation network, yielding a full receptive ﬁeld. The outcome is then reshaped to the output resolution.\n\ntremely deep architectures is their large receptive ﬁeld; ResNet-50 captures input sizes of 483 × 483, large enough to fully capture the input image even in higher resolutions. Given our input size and this architecture, the last convolutional layers result in 2048 feature maps of spatial resolution 10 × 8 pixels, when removing the last pooling layer. As we show later, the proposed model, which uses residual up-convolutions, produces an output of 160 × 128 pixels. If we instead added a fully- connected layer of the same size, it would intro- duce 3.3 billion parameters, worth 12.6GB in mem- ory, rendering this approach impossible on current hardware. This further motivates our proposal of a fully convolutional architecture with up-sampling blocks that contain fewer weights while improving the accuracy of the predicted depth maps.\n\nWe introduce a fully convolutional network for depth prediction. Here, the receptive ﬁeld is an im- portant aspect of the architectural design, as there are no explicit full connections. Speciﬁcally, as- sume we set an input of 304 × 228 pixels (as in [6]) and predict an output map that will be at approximately half the input resolution. We inves- tigate popular architectures (AlexNet [14], VGG- 16 [31]) as the contractive part, since their pre- trained weights facilitate convergence. The recep- tive ﬁeld at the last convolutional layer of AlexNet is 151 × 151 pixels, allowing only very low reso- lution input images when true global information (e.g. monocular cues) should be captured by the network without fully-connected layers. A larger receptive ﬁeld of 276 × 276 is achieved by VGG- 16 but still sets a limit to the input resolution. Eigen and Fergus [5] show a substantial improvement when switching from AlexNet to VGG, but since both their models use fully-connected layers, this is due to the higher discriminative power of VGG.\n\nRecently, ResNet [7] introduced skip layers that by-pass two or more convolutions and are summed to their output, including batch normalization [9] after every convolution (see Fig. 1). Following this design, it is possible to create much deeper net- works without facing degradation or vanishing gra- dients. Another beneﬁcial property of these ex-\n\nOur proposed architecture can be seen in Fig. 1. The feature map sizes correspond to the network trained for input size 304×228, in the case of NYU Depth v2 data set [23]. The ﬁrst part of the net- work is based on ResNet-50 and initialized with pre-trained weights. The second part of our ar- chitecture guides the network into learning its up- scaling through a sequence of unpooling and con- volutional layers. Following the set of these up- sampling blocks, dropout is applied and succeeded by a ﬁnal convolutional layer yielding the predic- tion.\n\nUp-Projection Blocks. Unpooling layers [4, 21, 38], perform the reverse operation of pooling, in- creasing the spatial resolution of feature maps. We adapt the approach described in [4] for the imple- mentation of unpooling layers, in order to double the size by mapping each entry into the top-left cor- ner of a 2 × 2 (zero) kernel. Each such layer is fol- lowed by a 5×5 convolution – so that it is applied to more than one non-zero elements at each location – and successively by ReLU activation. We refer to this block as up-convolution. Empirically, we stack four such up-convolutional blocks, i.e. 16x upscal- ing of the smallest feature map, resulting in the best trade-off between memory consumption and reso- lution. We found that performance did not increase when adding a ﬁfth block.\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\n*Knxm | + ® ‘nxmcomvolution 22 un-pooling —ReLU interleaving | _ Ks a #s * Ts Ts + I. (a) up-convolution (c) up-projection af rl _ a oo feel al tof fo 2 a Re ee Ly CH D2 OF a & Sa Ay tbe he T (b) fast up-convolution (d) fast up-projection\n\nFigure 2. From up-convolutions to up-projections. (a) Standard up-convolution. (b) The equivalent but faster up-convolution. (c) Our novel up-projection block, fol- lowing residual logic. (d) The faster equivalent version of (c)\n\nWe further extend simple up-convolutions using a similar but inverse concept to [7] to create up- sampling res-blocks. The idea is to introduce a simple 3 × 3 convolution after the up-convolution and to add a projection connection from the lower resolution feature map to the result, as shown in Fig. 2(c). Because of the different sizes, the small- sized map needs to be up-sampled using another up-convolution in the projection branch, but since the unpooling only needs to be applied once for both branches, we just apply the 5 × 5 convolu- tions separately on the two branches. We call this new up-sampling block up-projection since it ex- tends the idea of the projection connection [7] to up-convolutions. Chaining up-projection blocks al- lows high-level information to be more efﬁciently passed forward in the network while progressively increasing feature map sizes. This enables the con- struction of our coherent, fully convolutional net- work for depth prediction. Fig. 2 shows the dif- ferences between an up-convolutional block to up- projection block. It also shows the corresponding fast versions that will be described in the following section.\n\nFast Up-Convolutions. One further contribution of this work is to reformulate the up-convolution operation so to make it more efﬁcient, leading to a decrease of training time of the whole network of",
            "section": "methodology",
            "section_idx": 0,
            "citations": [
                "5, 6",
                "6",
                "14",
                "31",
                "5",
                "7",
                "9",
                "23",
                "4, 21, 38",
                "4",
                "7",
                "7"
            ]
        },
        {
            "text": "4.2. NYU Depth Dataset\n\nFirst, we evaluate on one of the largest RGB- D data sets for indoor scene reconstruction, NYU Depth v2 [23]. The raw dataset consists of 464 scenes, captured with a Microsoft Kinect, with the ofﬁcial split consisting in 249 training and 215 test scenes. For training, however, our method only requires a small subset of the raw distribu- tion. We sample equally-spaced frames out of each training sequence, resulting in approximately 12k unique images. After ofﬂine augmentations of the extracted frames, our dataset comprises approxi- mately 95k pairs of RGB-D images. We point out that our dataset is radically smaller than that re- quired to train the model in [5, 6], consisting of 120k unique images, as well as the 800k samples extracted in the patch-wise approach of [16]. Fol- lowing [6], the original frames of size 640 × 480 pixels are down-sampled to 1/2 resolution and center-cropped to 304 × 228 pixels, as input to the network. At last, we train our model with a batch size of 16 for approximately 20 epochs. The start- ing learning rate is 10−2 for all layers, which we gradually reduce every 6-8 epochs, when we ob- serve plateaus; momentum is 0.9.\n\nFor the quantitative evaluation of our methods and comparison to the state of the art on this data set, we compute various error measures on the com-\n\nmonly used test subset of 654 images. The predic- tions’ size depends on the speciﬁc model; in our conﬁguration, which consists of four up-sampling stages, the corresponding output resolutions are 128 × 96 for AlexNet, 144 × 112 for VGG and 160 × 128 for ResNet-based models. The predic- tions are then up-sampled back to the original size (640 × 480) using bilinear interpolation and com- pared against the provided ground truth with ﬁlled- in depth values for invalid pixels.\n\nArchitecture Evaluation.\n\nIn Table 1 we com-\n\npare different CNN variants of the proposed archi- tecture, in order to study the effect of each compo- nent. First, we evaluate the inﬂuence of the depth of the architecture using the convolutional blocks of AlexNet, VGG-16 and ResNet-50. It becomes ap- parent that a fully convolutional architecture (Up- Conv) on AlexNet is outperformed by the typical network with full connections (FC). As detailed in Sec. 3.1, a reason for this is the relatively small ﬁeld of view in AlexNet, which is not enough to cap- ture global information that is needed when remov- ing the fully-connected layers. Instead, using VGG as the core architecture, improves the accuracy on depth estimation. As a fully-connected VGG vari- ant for high-dimensional regression would incorpo- rate a high number of parameters, we only perform tests on the fully convolutional (UpConv) model here. However, a VGG-based model with fully- connected layers was indeed employed by [5] (for their results see Table 2) performing better than our fully convolutional VGG-variant mainly due to their multi-scale architecture, including the reﬁne-",
            "section": "methodology",
            "section_idx": 1,
            "citations": [
                "23",
                "5, 6",
                "16",
                "6",
                "5"
            ]
        },
        {
            "text": "4. Experimental Results\n\nThe Berhu loss is equal to the L1(x) = |x| norm when x ∈ [−c,c] and equal to L2 outside this range. The version used here is continuous and ﬁrst order differentiable at the point c where the switch from L1 to L2 occurs. In every gradient descent step, when we compute B(˜y − y) we set c = 1 5 maxi(|˜yi − yi|), where i indexes all pixels over each image in the current batch, that is 20% of the maximal per-batch error. Empirically, BerHu shows a good balance between the two norms in the given problem; it puts high weight towards sam- ples/pixels with a high residual because of the L2 term, contrary for example to a robust loss, such as Tukey’s biweight function that ignores samples with high residuals [2]. At the same time, L1 ac- counts for a greater impact of smaller residuals’ gradients than L2 would.\n\nWe provide two further intuitions with respect to the difference between L2 and berHu loss. In both datasets that we experimented with, we observe a heavy-tailed distribution of depth values, also reported in [27], for which Zwald and Lambert- Lacroix [40] show that the berHu loss function is more appropriate. This could also explain why [5, 6] experience better convergence when predict- ing the log of the depth values, effectively mov- ing a log-normal distribution back to Gaussian. Secondly we see the greater beneﬁt of berHu in the small residuals during training as there the L1 derivative is greater than L2’s. This manifests in the error measures rel. and δ1 (Sec. 4), which are more sensitive to small errors.\n\nIn this section, we provide a thorough analy- sis of our methods, evaluating the different com- ponents that comprise the down-sampling and up- sampling part of the CNN architecture. We also re- port the quantitative and qualitative results obtained by our model and compare to the state of the art in two standard benchmark datasets for depth predic- tion, i.e. NYU Depth v2 [23] (indoor scenes) and Make3D [30] (outdoor scenes).\n\n4.1. Experimental Setup\n\nFor the implementation of our network we use MatConvNet [36], and train on a single NVIDIA GeForce GTX TITAN with 12GB of GPU mem- ory. Weight layers of the down-sampling part of the architecture are initialized by the corresponding models (AlexNet, VGG, ResNet) pre-trained on the ILSVRC [28] data for image classiﬁcation. Newly added layers of the up-sampling part are initialized as random ﬁlters sampled from a normal distribu- tion with zero mean and 0.01 variance.\n\nThe network is trained on RGB inputs to predict the corresponding depth maps. We use data aug- mentation to increase the number of training sam- ples. The input images and corresponding ground truth are transformed using small rotations, scaling, color transformations and ﬂips with a 0.5 chance, with values following Eigen et al. [6]. Finally, we model small translations by random crops of the augmented images down to the chosen input size of the network.\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\nNYU Depth v2 rel rms rms(log) log10 δ1 δ2 δ3 Karsch et al. [10] Ladicky et al. [15] 0.374 - 1.12 - - - 0.134 - - 0.542 - 0.829 - 0.941 Liu et al. [20] 0.335 1.06 - 0.127 - - - Li et al. [16] Liu et al. [19] Wang et al. [37] 0.232 0.230 0.220 0.821 0.824 0.745 - - 0.262 0.094 0.095 0.094 0.621 0.614 0.605 0.886 0.883 0.890 0.968 0.971 0.970 Eigen et al. [6] 0.215 0.907 0.285 - 0.611 0.887 0.971 Roy and Todorovic [27] Eigen and Fergus [5] 0.187 0.158 0.744 0.641 - 0.214 0.078 - - 0.769 - 0.950 - 0.988\n\nours (ResNet-UpProj) 0.127 0.573 0.195 0.055 0.811 0.953 0.988 Table 2. Comparison of the proposed approach against the state of the art on the NYU Depth v2 dataset. The values are those originally reported by the authors in their respective paper\n\nFor the quantitative evaluation that follows, the same error metrics which have been used in prior works [5, 6, 15, 16, 19] are computed on our exper- imental results.",
            "section": "results",
            "section_idx": 0,
            "citations": [
                "2",
                "27",
                "40",
                "5, 6",
                "23",
                "30",
                "36",
                "28",
                "6",
                "10",
                "15",
                "20",
                "16",
                "19",
                "37",
                "6",
                "27",
                "5",
                "5, 6, 15, 16, 19"
            ]
        },
        {
            "text": "ment scales.\n\nFinally, switching to ResNet with a fully- connected layer (ResNet-FC) – without removing the ﬁnal pooling layer – achieves similar perfor- mance to [5] for a low resolution output (64 × 48), using 10 times fewer data; however increasing the output resolution (160 × 128) results in such a vast number of parameters that convergence be- comes harder. This further motivates the reason- ing for the replacement of fully-connected layers and the need for more efﬁcient upsampling tech- niques, when dealing with high-dimensional prob- lems. Our fully convolutional variant using simple\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\nRGB Image AlexNet proposed ground truth Eigen et al. [5]\n\nFigure 4. Depth Prediction on NYU Depth Qualitative results showing predictions using AlexNet, VGG, and the fully-connected ResNet compared to our model and the predictions of [5]. All colormaps are scaled equally for better comparison\n\nup-convolutions (ResNet-UpConv) improves accu- racy, and at last, the proposed architecture (ResNet- UpProj), enhanced with the up-projection blocks, gives by far the best results. As far as the num- ber of parameters is concerned, we see a drastic de- crease when switching from fully-connected layers to fully convolutional networks. Another common up-sampling technique that we investigated is de- convolution with successive 2 × 2 kernels, but the up-projections notably outperformed it. Qualita- tively, since our method consists in four successive up-sampling steps (2x resolution per block), it can preserve more structure in the output when compar- ing to the FC-variant (see Fig. 4).\n\nIn all shown experiments the berHu loss outper- forms L2. The difference is higher in relative error which can be explained by the larger gradients of L1 (berHu) over L2 for small residuals; the inﬂu- ence on the relative error is higher, as there pixels in smaller distances are more sensitive to smaller errors. This effect is also well visible as a stronger gain in the challenging δ1 measure.\n\nFinally, we measure the timing of a single up- convolutional block for a single image (1.5 ms) and compare to our up-projection (0.14 ms). This exceeds the theoretical speed up of 4 and is due to the fact that smaller ﬁlter sizes beneﬁt more\n\nfrom the linearization inside cuDNN. Furthermore, one of the advantages of our model is the overall computation time. Predicting the depth map of a single image takes only 55ms with the proposed up-sampling (78ms with up-convolutions) on our setup. This enables real-time processing images, for example from a web-cam. Further speed up can be achieved when several images are processed in a batch. A batch size of 16 results in 14ms per image with up-projection and 28ms for up-convolutions.\n\nComparison with related methods. In Table 2 we compare the results obtained by the proposed architecture to those reported by related work. Ad- ditionally, in Fig. 4 we qualitatively compare the accuracy of the estimated depth maps using the proposed approach (ResNet-UpProj) with that of the different variants (AlexNet, VGG, ResNet-FC- 64x48) as well as with the publicly available pre- dictions of Eigen and Fergus [5]. One can clearly see the improvement in quality from AlexNet to ResNet, however the fully-connected variant of ResNet, despite its increased accuracy, is still lim- ited to coarse predictions. The proposed fully con- volutional model greatly improves edge quality and structure deﬁnition in the predicted depth maps.\n\nInterestingly, our depth predictions exhibit note- worthy visual quality, even though they are derived\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\nAlexNet VGG ground truth proposed\n\nFigure 6. 3D SLAM Comparison of the 3D reconstructions obtained on NYU Depth dataset between the ground-truth depth (left-most) and the depth predicted, respectively (left to right), by AlexNet, VGG and our architecture.\n\nRGB Image proposed ground truth\n\nFigure 5. Depth Prediction on Make3D. Displayed are RGB images (ﬁrst row), ground truth depth maps (mid- dle row) and our predictions (last row). Pixels that corre- spond to distances > 70m in the ground truth are masked out\n\nby a single model, trained end-to-end, without any additional post-processing steps, as for example the CRF inference of [16, 37]. On the other hand, [5]\n\nMake3D rel rms log10 Karsch et al. [10] 0.355 9.20 0.127 Liu et al. [20] 0.335 9.49 0.137 Liu et al. [19] 0.314 8.60 0.119 Li et al. [16] 0.278 7.19 0.092 ours (L2) 0.223 4.89 0.089 ours (berHu) 0.176 4.46 0.072\n\nTable 3. Comparison with the state of the art. We re- port our results with l2 and berHu loss. The shown val- ues of the evaluated methods are those reported by the authors in their paper\n\nreﬁne their predictions through a multi-scale ar- chitecture that combines the RGB image and the original prediction to create visually appealing re- sults. However, they sometimes mis-estimate the global scale (second and third row) or introduce noise in case of highly-textured regions in the orig- inal image, even though there is no actual depth border in the ground truth (last row). Furthermore, we compare to the number of parameters in [5], which we calculated as 218 million for the three scales, that is approximately 3.5 times more than our model. Instead, the CNN architecture proposed here is designed with feasibility in mind; the num- ber of parameters should not increase uncontrol- lably in high-dimensional problems. This further means a reduction in the number of gradient steps required as well as the data samples needed for training. Our single network generalizes better and successfully tackles the problem of coarseness that has been encountered by previous CNN approaches on depth estimation.\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016",
            "section": "results",
            "section_idx": 1,
            "citations": [
                "5",
                "5",
                "5",
                "5",
                "16, 37",
                "5",
                "10",
                "20",
                "19",
                "16",
                "5"
            ]
        },
        {
            "text": "[39] R. Zhang, P.-S. Tsai, J. E. Cryer, and M. Shah. Shape-from-shading: a survey. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 21(8):690–706, 1999. 1, 2\n\n[40] L. Zwald and S. Lambert-Lacroix. The berhu penalty and the grouped effect. arXiv preprint arXiv:1207.6868, 2012. 2, 6\n\n[29] A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from single monocular images. In Advances in Neural Information Processing Systems, pages 1161–1168, 2005. 2\n\n[30] A. Saxena, M. Sun, and A. Ng. Make3d: Learning 3d scene structure from a single still image. IEEE",
            "section": "discussion",
            "section_idx": 0,
            "citations": [
                "39",
                "40",
                "29",
                "30"
            ]
        },
        {
            "text": "merit future analysis.\n\n5. Conclusion\n\nIn this work we present a novel approach to the problem of depth estimation from a single im- age. Unlike typical CNN approaches that require a multi-step process in order to reﬁne their origi- nally coarse depth predictions, our method consists in a powerful, single-scale CNN architecture that follows residual learning. The proposed network is fully convolutional, comprising up-projection lay-\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\ners that allow for training much deeper conﬁgura- tions, while greatly reducing the number of param- eters to be learned and the number of training sam- ples required. Moreover, we illustrate a faster and more efﬁcient approach to up-convolutional layers. A thorough evaluation of the different architectural components has been carried out not only by op- timizing with the typical l2 loss, but also with the berHu loss function, showing that it is better suited for the underlying value distributions of the ground truth depth maps. All in all, the model emerging from our contributions is not only simpler than ex- isting methods, can be trained with less data in less time, but also achieves higher quality results that lead our method to state-of-the-art in two bench- mark datasets for depth estimation.\n\n[8] D. Hoiem, A. Efros, M. Hebert, et al. Geometric context from a single image. In Computer Vision, 2005. ICCV 2005. Tenth IEEE International Con- ference on, volume 1, pages 654–661. IEEE, 2005. 2\n\n[9] S. Ioffe and C. Szegedy. Batch normalization: Ac- celerating deep network training by reducing inter- nal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning, pages 448–456, 2015. 4\n\n[10] K. Karsch, C. Liu, and S. B. Kang. Depth extrac- tion from video using non-parametric sampling. In Proc. Europ. Conf. Computer Vision (ECCV), pages 775–788, 2012. 2, 3, 7, 9\n\n[11] M. Keller, D. Leﬂoch, M. Lambers, S. Izadi, T. Weyrich, and A. Kolb. Real-time 3d reconstruc- tion in dynamic scenes using point-based fusion. In Proc. Int. Conf. 3D Vision(3DV), pages 1–8, 2013. 10",
            "section": "conclusion",
            "section_idx": 0,
            "citations": [
                "8",
                "9",
                "10",
                "11"
            ]
        },
        {
            "text": "6\n\n2016\n\n1\n\n0\n\n2 p e S 9 1 ] V C . s c [ 2 v 3 7 3 0 0 . 6 0 6 1 :\n\nv\n\narXiv\n\ni\n\nX\n\nr\n\na\n\nDeeper Depth Prediction with Fully Convolutional Residual Networks\n\nIro Laina∗1\n\nChristian Rupprecht∗1,2\n\nVasileios Belagiannis3\n\niro.laina@tum.de\n\nchristian.rupprecht@in.tum.de\n\nvb@robots.ox.ac.uk\n\nFederico Tombari1,4\n\nNassir Navab1,2\n\ntombari@in.tum.de\n\nnavab@in.tum.de\n\n1. Introduction\n\nAbstract\n\nThis paper addresses the problem of estimating the depth map of a scene given a single RGB im- age. We propose a fully convolutional architecture, encompassing residual learning, to model the am- biguous mapping between monocular images and depth maps. In order to improve the output reso- lution, we present a novel way to efﬁciently learn feature map up-sampling within the network. For optimization, we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional reﬁnement steps. As a result, it runs in real-time on images or videos. In the eval- uation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperform- ing all approaches on depth estimation. Code and models are publicly available5.\n\n∗ equal contribution\n\n1 Technische Universit¨at M¨unchen, Munich, Germany\n\nDepth estimation from a single view is a dis- cipline as old as computer vision and encom- passes several techniques that have been developed throughout the years. One of the most successful among these techniques is Structure-from-Motion (SfM) [34]; it leverages camera motion to estimate camera poses through different temporal intervals and, in turn, estimate depth via triangulation from pairs of consecutive views. Alternatively to motion, other working assumptions can be used to estimate depth, such as variations in illumination [39] or fo- cus [33].\n\nIn absence of such environmental assumptions, depth estimation from a single image of a generic scene is an ill-posed problem, due to the inherent ambiguity of mapping an intensity or color mea- surement into a depth value. While this also is a human brain limitation, depth perception can nev- ertheless emerge from monocular vision. Hence, it is not only a challenging task to develop a computer vision system capable of estimating depth maps by exploiting monocular cues, but also a necessary one in scenarios where direct depth sensing is not avail- able or not possible. Moreover, the availability of reasonably accurate depth information is well- known to improve many computer vision tasks with respect to the RGB-only counterpart, for example in reconstruction [23], recognition [26], semantic segmentation [5] or human pose estimation [35].",
            "section": "other",
            "section_idx": 0,
            "citations": [
                "34",
                "39",
                "33",
                "23",
                "26",
                "5",
                "35"
            ]
        },
        {
            "text": "2 Johns Hopkins University, Baltimore MD, USA\n\n3 University of Oxford, Oxford, United Kingdom\n\n4 University of Bologna, Bologna, Italy\n\n5 https://github.com/iro-cp/FCRN-DepthPrediction\n\nFor this reason, several works tackle the prob- lem of monocular depth estimation. One of the ﬁrst\n\n1\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\napproaches assumed superpixels as planar and in- ferred depth through plane coefﬁcients via Markov Random Fields (MRFs) [30]. Superpixels have also been considered in [16, 20, 37], where Conditional Random Fields (CRFs) are deployed for the regu- larization of depth maps. Data-driven approaches, such as [10, 13], have proposed to carry out image matching based on hand-crafted features to retrieve the most similar candidates of the training set to a given query image. The corresponding depth can- didates are then warped and merged in order to pro- duce the ﬁnal outcome.\n\nRecently, Convolutional Neural Networks (CNNs) have been employed to learn an implicit relation between color pixels depth [5, 6, 16, 19, 37]. CNN approaches have often been combined with CRF-based regulariza- tion, either as a post-processing step [16, 37] or via structured deep learning [19], as well as with random forests [27]. These methods encompass a higher complexity due to either the high number of parameters involved in a deep network [5, 6, 19] or the joint use of a CNN and a CRF [16, 37]. Nevertheless, deep learning boosted the accuracy on standard benchmark datasets considerably, ranking these methods ﬁrst in the state of the art. and\n\nIn this work, we propose to learn the mapping between a single RGB image and its correspond- ing depth map using a CNN. The contribution of our work is as follows. First, we introduce a fully convolutional architecture to depth prediction, en- dowed with novel up-sampling blocks, that allows for dense output maps of higher resolution and at the same time requires fewer parameters and trains on one order of magnitude fewer data than the state of the art, while outperforming all existing meth- ods on standard benchmark datasets [23, 29]. We further propose a more efﬁcient scheme for up- convolutions and combine it with the concept of residual learning [7] to create up-projection blocks for the effective upsampling of feature maps. Last, we train the network by optimizing a loss based on the reverse Huber function (berHu) [40] and demonstrate, both theoretically and experimentally, why it is beneﬁcial and better suited for the task at hand. We thoroughly evaluate the inﬂuence\n\nof the network’s depth, the loss function and the speciﬁc layers employed for up-sampling in order to analyze their beneﬁts. Finally, to further as- sess the accuracy of our method, we employ the trained model within a 3D reconstruction scenario, in which we use a sequence of RGB frames and their predicted depth maps for Simultaneous Local- ization and Mapping (SLAM).",
            "section": "other",
            "section_idx": 1,
            "citations": [
                "30",
                "16, 20, 37",
                "10, 13",
                "5, 6, 16, 19, 37",
                "16, 37",
                "19",
                "27",
                "5, 6, 19",
                "16, 37",
                "23, 29",
                "7",
                "40"
            ]
        },
        {
            "text": "2. Related Work\n\nDepth estimation from image data has origi- nally relied on stereo vision [22, 32], using image pairs of the same scene to reconstruct 3D shapes. In the single-view case, most approaches relied on motion (Structure-from-Motion [34]) or differ- ent shooting conditions (Shape-from-Shading [39], Shape-from-Defocus [33]). Despite the ambigui- ties that arise in lack of such information, but in- spired by the analogy to human depth perception from monocular cues, depth map prediction from a single RGB image has also been investigated. Be- low, we focus on the related work for single RGB input, similar to our method.\n\nClassic methods on monocular depth estimation have mainly relied on hand-crafted features and used probabilistic graphical models to tackle the problem [8, 17, 29, 30], usually making strong as- sumptions about scene geometry. One of the ﬁrst works, by Saxena et al. [29], uses a MRF to infer depth from local and global features extracted from the image, while superpixels [1] are introduced in the MRF formulation in order to enforce neighbor- ing constraints. Their work has been later extended to 3D scene reconstruction [30]. Inspired by this work, Liu et al. [17] combine the task of seman- tic segmentation with depth estimation, where pre- dicted labels are used as additional constraints to facilitate the optimization task. Ladicky et al. [15] instead jointly predict labels and depths in a classi- ﬁcation approach.\n\nA second cluster of related work comprises non-parametric approaches for depth transfer [10, 13, 18, 20], which typically perform feature-based matching (e.g. GIST [24], HOG [3]) between a given RGB image and the images of a RGB-D repository in order to ﬁnd the nearest neighbors; the\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\nWell Input DY ees 26 25, 256. sizg}3ex29x| [siz] ex29x] | siz,| sex29x| | siz, |28x29% 0s ana [PP] 776 | 2515726] LB ro srase| FD) ressr ase] PAE sate] Laie) aazze) Base aon) ba ae) cotton 1 (strides) i foi ut ot lg tem 4 1024 2088. 2048, 2038, stride 2 102 1024, ss 2 as | sy x TD 7 7 ry) w; 7 7 vy sit |Je[|_ eatch nocmatzation 1915x1024 10x8x 2048 ® Up-Convolution * ra (Please refer to 1 a Fig. 2 forthe tl ® ® ® ® 7s | prediction implementation 1S >| <Hvorcenss] Fa) t60x128x66 |} Coase balsa 10x8%1028 20x16x512 40x32 256 di di s i *3 *, *:—*s —*, nf ale! Lala ees hee he a = Ey F * ul @ Skip a2 Projection a2\n\nFigure 1. Network architecture. The proposed architecture builds upon ResNet-50. We replace the fully-connected layer, which was part of the original architecture, with our novel up-sampling blocks, yielding an output of roughly half the input resolution\n\nretrieved depth counterparts are then warped and combined to produce the ﬁnal depth map. Karsch et al. [10] perform warping using SIFT Flow [18], followed by a global optimization scheme, whereas Konrad et al. [13] compute a median over the re- trieved depth maps followed by cross-bilateral ﬁl- tering for smoothing. Instead of warping the can- didates, Liu et al. [20], formulate the optimiza- tion problem as a Conditional Random Field (CRF) with continuous and discrete variable potentials. Notably, these approaches rely on the assumption that similarities between regions in the RGB im- ages imply also similar depth cues.\n\nMore recently, remarkable advances in the ﬁeld of deep learning drove research towards the use of CNNs for depth estimation. Since the task is closely related to semantic labeling, most works have built upon the most successful architectures of the ImageNet Large Scale Visual Recogni- tion Challenge (ILSVRC) [28], often initializing their networks with AlexNet [14] or the deeper VGG [31]. Eigen et al. [6] have been the ﬁrst to use CNNs for regressing dense depth maps from a single image in a two-scale architecture, where the ﬁrst stage – based on AlexNet – produces a coarse output and the second stage reﬁnes the original pre- diction. Their work is later extended to addition- ally predict normals and labels with a deeper and\n\nmore discriminative model – based on VGG – and a three-scale architecture for further reﬁnement [5]. Unlike the deep architectures of [5, 6], Roy and Todorovic [27] propose combining CNNs with re- gression forests, using very shallow architectures at each tree node, thus limiting the need for big data.\n\nAnother direction for improving the quality of the predicted depth maps has been the combined use of CNNs and graphical models [16, 19, 37]. Liu et al. [19] propose to learn the unary and pair- wise potentials during CNN training in the form of a CRF loss, while Li et al. [16] and Wang et al. [37] use hierarchical CRFs to reﬁne their patch-wise CNN predictions from superpixel down to pixel level.\n\nOur method uses a CNN for depth estimation and differs from previous work in that it improves over the typical fully-connected layers, which are expensive with respect to the number of parame- ters, with a fully convolutional model incorporating efﬁcient residual up-sampling blocks, that we refer to as up-projections and which prove to be more suitable when tackling high-dimensional regression problems.",
            "section": "other",
            "section_idx": 2,
            "citations": [
                "22, 32",
                "34",
                "39",
                "33",
                "8, 17, 29, 30",
                "29",
                "1",
                "30",
                "17",
                "15",
                "10, 13, 18, 20",
                "24",
                "3",
                "10",
                "18",
                "13",
                "20",
                "28",
                "14",
                "31",
                "6",
                "5",
                "5, 6",
                "27",
                "16, 19, 37",
                "19",
                "16",
                "37"
            ]
        },
        {
            "text": "5x6 filter feature map ‘unpooling. > i} interleaving feature maps ry = Ss He\n\nFigure 3. Faster up-convolutions. Top row: the com- mon up-convolutional steps: unpooling doubles a fea- ture map’s size, ﬁlling the holes with zeros, and a 5 × 5 convolution ﬁlters this map. Depending on the position of the ﬁlter, only certain parts of it (A,B,C,D) are mul- tiplied with non-zero values. This motivates convolv- ing the original feature map with the 4 differently com- posed ﬁlters (bottom part) and interleaving them to ob- tain the same output, while avoiding zero multiplications. A,B,C,D only mark locations and the actual weight val- ues will differ\n\naround 15%. This also applies to the newly intro-\n\nduced up-projection operation. The main intuition is as follows: after unpooling 75% of the result- ing feature maps contain zeros, thus the following 5 × 5 convolution mostly operates on zeros which can be avoided in our modiﬁed formulation. This can be observed in Fig. 3. In the top left the origi- nal feature map is unpooled (top middle) and then convolved by a 5 × 5 ﬁlter. We observe that in an unpooled feature map, depending on the location (red, blue, purple, orange bounding boxes) of the 5×5 ﬁlter, only certain weights are multiplied with potentially non-zero values. These weights fall into four non-overlapping groups, indicated by different colors and A,B,C,D in the ﬁgure. Based on the ﬁl- ter groups, we arrange the original 5 × 5 ﬁlter to four new ﬁlters of sizes (A) 3 × 3, (B) 3 × 2, (C) 2×3 and (D) 2×2. Exactly the same output as the original operation (unpooling and convolution) can now be achieved by interleaving the elements of the four resulting feature maps as in Fig. 3. The corre- sponding changes from a simple up-convolutional block to the proposed up-projection are shown in\n\nFig. 2 (d).\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\n3.2. Loss Function\n\nA standard loss function for optimization in re- gression problems is the L2 loss, minimizing the squared euclidean norm between predictions ˜y and ground truth y: L2(˜y − y) = ||˜y − y||2 2. Although this produces good results in our test cases, we found that using the reverse Huber (berHu) [25, 40] as loss function B yields a better ﬁnal error than L2.\n\nArchitecture AlexNet FC Loss L2 #params 104.4 × 106 rel 0.209 rms 0.845 log10 0.090 δ1 0.586 δ2 0.869 berHu 0.207 0.842 0.091 0.581 0.872 VGG UpConv UpConv L2 berHu L2 6.3 × 106 18.5 × 106 0.218 0.215 0.194 0.853 0.855 0.746 0.094 0.094 0.083 0.576 0.574 0.626 0.855 0.855 0.894 ResNet FC-160x128 FC-64x48 DeConv UpConv berHu berHu berHu L2 L2 359.1 × 106 73.9 × 106 28.5 × 106 43.1 × 106 0.194 0.181 0.154 0.152 0.139 0.790 0.784 0.679 0.621 0.606 0.083 0.080 0.066 0.065 0.061 0.629 0.649 0.754 0.749 0.778 0.889 0.894 0.938 0.934 0.944 UpProj berHu L2 63.6 × 106 0.132 0.138 0.604 0.592 0.058 0.060 0.789 0.785 0.946 0.952 berHu 0.127 0.573 0.055 0.811 0.953 δ3 0.967 0.969 0.957 0.958 0.974 0.971 0.971 0.984 0.985 0.985 0.986 0.987 0.988\n\nB(x) = |x| x2+c2 2c |x| ≤ c, |x| > c. (1)\n\nTable 1. Comparison of the proposed approach against different variants on the NYU Depth v2 dataset. For the reported errors rel, rms, log10 lower is better, whereas for the accuracies δi < 1.25i higher is better",
            "section": "other",
            "section_idx": 3,
            "citations": [
                "25, 40"
            ]
        },
        {
            "text": "4.3. Make3D Dataset\n\nIn addition, we evaluated our model on Make3D\n\ndata set [30] of outdoor scenes. It consists of 400 training and 134 testing images, gathered using a custom 3D scanner. As the dataset acquisition dates to several years ago, the ground truth depth map resolution is restricted to 305×55, unlike the orig- inal RGB images of 1704 × 2272 pixels. Follow- ing [20], we resize all images to 345 × 460 and further reduce the resolution of the RGB inputs to the network by half because of the large archi- tecture and hardware limitations. We train on an augmented data set of around 15k samples using the best performing model (ResNet-UpProj) with a batch size of 16 images for 30 epochs. Start- ing learning rate is 0.01 when using the berHu loss, but it needs more careful adjustment starting at 0.005 when optimizing with L2. Momentum is 0.9. Please note that due to the limitations that come with the dataset, considering the low resolu- tion ground truth and long range inaccuracies (e.g. sky pixels mapped at 80m), we train against ground truth depth maps by masking out pixels of distances over 70m.\n\nIn order to compare our results to state-of-the- art, we up-sample the predicted depth maps back to 345 × 460 using bilinear interpolation. Table 3 reports the errors compared to previous work based on (C1) criterion, computed in regions of depth less than 70m as suggested by [20] and as implied by our training. As an aside, [20] pre-process the im- ages with a per-pixel sky classiﬁcation to also ex- clude them from training. Our method signiﬁcantly outperforms all previous works when trained with either L2 or berHu loss functions. In this challeng- ing dataset, the advantage of berHu loss is more eminent. Also similarly to NYU, berHu improves the relative error more than the rms because of the weighting of close depth values. Qualitative results from this dataset are shown in Fig. 5.\n\n4.4. Application to SLAM\n\nTo complement the previous results, we demon- strate the usefulness of depth prediction within a SLAM application, with the goal of reconstruct- ing the geometry of a 3D environment. In partic-\n\nular, we deploy a SLAM framework where frame- to-frame tracking is obtained via Gauss-Newton optimization on the pixelwise intensity differences computed on consecutive frame pairs as proposed in [12], while fusion of depth measurements be- tween the current frame and the global model is carried out via point-based fusion [11]. We wish to point out that, to the best of our knowledge, this is the ﬁrst demonstration of a SLAM reconstruction based on depth predictions from single images.\n\nA qualitative comparison between the SLAM re- constructions obtained using the depth values esti- mated with the proposed ResNet-UpProj architec- ture against that obtained using the ground truth depth values on part of a sequence of the NYU Depth dataset is shown in Fig. 6. The ﬁgure also includes a comparison with the depth predictions obtained using AlexNet and VGG architectures. As it can be seen, the improved accuracy of the depth predictions, together with the good edge- preserving qualities of our up-sampling method, is not only noticeable in the qualitative results of Fig. 4, but also yields a much more accurate SLAM reconstruction compared to the other architectures. We wish to point out that, although we do not be- lieve its accuracy could be yet compared to that achieved by methods exploiting temporal consis- tency for depth estimation such as SfM and monoc- ular SLAM, our method does not explicitly rely on visual features to estimate depths, and thus holds the potential to be applied also on scenes character- ized by low-textured surfaces such as walls, ﬂoors and other structures typically present in indoor en- vironments. Although clearly outside the scope of this paper, we ﬁnd these aspects relevant enough to",
            "section": "other",
            "section_idx": 4,
            "citations": [
                "30",
                "20",
                "20",
                "20",
                "12",
                "11"
            ]
        },
        {
            "text": "References\n\n[1] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk. SLIC superpixels compared to state-of-the-art superpixel methods. Pattern Anal- ysis and Machine Intelligence, IEEE Transactions on, 34(11):2274–2282, 2012. 2\n\n[2] V. Belagiannis, C. Rupprecht, G. Carneiro, and N. Navab. Robust optimization for deep regres- sion. Proceedings of the International Conference on Computer Vision, 2015. 6\n\n[3] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Proc. Conf. Computer Vision and Pattern Recognition (CVPR), pages 886–893, 2005. 2\n\n[4] A. Dosovitskiy, J. Tobias Springenberg, and T. Brox. Learning to generate chairs with convo- lutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1538–1546, 2015. 4\n\n[5] D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi- scale convolutional architecture. In Proc. Int. Conf. Computer Vision (ICCV), 2015. 1, 2, 3, 4, 6, 7, 8, 9\n\n[6] D. Eigen, C. Puhrsch, and R. Fergus. Prediction from a single image using a multi-scale deep net- work. In Proc. Conf. Neural Information Process- ing Systems (NIPS), 2014. 2, 3, 4, 6, 7\n\n[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid- ual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. 2, 4, 5\n\n[12] C. Kerl, J. Sturm, and D. Cremers. Robust odom- etry estimation for RGB-D cameras. In Proc. Int. Conf. on Robotics and Automation (ICRA), 2013. 10\n\n[13] J. Konrad, M. Wang, and P. Ishwar. 2d-to-3d image conversion by learning depth from examples. In Proc. Conf. Computer Vision and Pattern Recogni- tion Workshops, pages 16–22, 2012. 2, 3\n\n[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Ima- genet classiﬁcation with deep convolutional neural networks. In Advances in neural information pro- cessing systems, pages 1097–1105, 2012. 3, 4\n\n[15] L. Ladicky, J. Shi, and M. Pollefeys. Pulling things out of perspective. In Proc. Conf. Computer Vi- sion and Pattern Recognition (CVPR), pages 89– 96, 2014. 2, 7\n\n[16] B. Li, C. Shen, Y. Dai, A. V. den Hengel, and M. He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs. In Proc. Conf. Computer Vision and Pattern Recognition (CVPR), pages 1119–1127, 2015. 2, 3, 7, 9\n\n[17] B. Liu, S. Gould, and D. Koller. Single im- age depth estimation from predicted semantic la- bels. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 1253– 1260. IEEE, 2010. 2\n\n[18] C. Liu, J. Yuen, and A. Torralba. Sift ﬂow: Dense correspondence across scenes and its applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(5):978–994, 2011. 2, 3\n\nPublished at IEEE International Conference on 3D Vision (3DV) 2016\n\n[19] F. Liu, C. Shen, and G. Lin. Deep convolutional neural ﬁelds for depth estimation from a single im- age. In Proc. Conf. Computer Vision and Pattern Recognition (CVPR), pages 5162–5170, 2015. 2, 3, 7, 9\n\nTrans. Pattern Analysis and Machine Intelligence (PAMI), 12(5):824–840, 2009. 2, 6, 10\n\n[31] K. Simonyan and A. Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 3, 4\n\n[20] M. Liu, M. Salzmann, and X. He. Discrete- continuous depth estimation from a single image. In Proc. Conf. Computer Vision and Pattern Recog- nition (CVPR), pages 716–723, 2014. 2, 3, 7, 9, 10\n\n[32] F. H. Sinz, J. Q. Candela, G. H. Bakır, C. E. Ras- mussen, and M. O. Franz. Learning depth from stereo. In Pattern Recognition, pages 245–252. Springer, 2004. 2\n\n[21] J. Long, E. Shelhamer, and T. Darrell. Fully con- volutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3431–3440, 2015. 4\n\n[22] R. Memisevic and C. Conrad. Stereopsis via deep learning. In NIPS Workshop on Deep Learning, volume 1, 2011. 2\n\n[23] P. K. Nathan Silberman, Derek Hoiem and R. Fer- gus. Indoor segmentation and support inference from RGBD images. In ECCV, 2012. 1, 2, 4, 6, 7\n\n[33] S. Suwajanakorn and C. Hernandez. Depth from fo- cus with your mobile phone. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. 1, 2\n\n[34] R. Szeliski. Structure from motion. In Computer Vision, Texts in Computer Science, pages 303–334. Springer London, 2011. 1, 2\n\n[35] J. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon. The vitruvian manifold: Inferring dense correspon- dences for one-shot human pose estimation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 103–110. IEEE, 2012. 1\n\n[24] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. Int. Journal of Computer Vision (IJCV), pages 145–175, 2014. 2\n\n[25] A. B. Owen. A robust hybrid of lasso and ridge regression. Contemporary Mathematics, 443:59– 72, 2007. 6\n\n[26] X. Ren, L. Bo, and D. Fox. Rgb-(d) scene label- ing: Features and algorithms. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Con- ference on, pages 2759–2766. IEEE, 2012. 1\n\n[27] A. Roy and S. Todorovic. Monocular depth estima- tion using neural regression forest. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition CVPR (CVPR), 2016. 2, 3, 6,\n\n7\n\n[28] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei- Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vi- sion (IJCV), 115(3):211–252, 2015. 3, 6\n\n[36] A. Vedaldi and K. Lenc. Matconvnet – convolu- tional neural networks for matlab. 2015. 6\n\n[37] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L. Yuille. Towards uniﬁed depth and semantic prediction from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition, pages 2800–2809, 2015. 2, 3, 7, 9\n\n[38] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive deconvolutional networks for mid and high level feature learning. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2018– 2025. IEEE, 2011. 4",
            "section": "other",
            "section_idx": 5,
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "31",
                "20",
                "32",
                "21",
                "22",
                "23",
                "33",
                "34",
                "35",
                "24",
                "25",
                "26",
                "27",
                "28",
                "36",
                "37",
                "38"
            ]
        }
    ],
    "figures": [
        {
            "path": "output\\images\\a56c4d0e-0212-40ca-a7ad-f2920ea356a4.jpg",
            "description": "This figure presents a comparative analysis of depth estimation from RGB images using different neural network architectures. The columns show results from AlexNet, VGG-16, ResNet-50, the proposed method, ground truth, and results from a previous method by Eigen et al. [5]. Each row corresponds to a different scene. The figure highlights how each model reconstructs depth information, with color gradients representing varying depths. This visualization is crucial for understanding the performance and accuracy of the proposed method compared to existing architectures and the ground truth, making it an important part of the research findings.",
            "importance": 9
        },
        {
            "path": "output\\images\\1f839ef0-81d6-4eff-af98-1004524aebe5.jpg",
            "description": "This figure presents a comparison between the results of a proposed method and ground truth data for depth estimation or similar vision tasks. The left column contains RGB images used as input. The middle column shows the output of the proposed method, likely a depth map or a similar representation, depicted using a color gradient to represent different depths or features. The right column displays the ground truth data for the same scenes, also using a color gradient to indicate depth or other corresponding features.\n\nThe figure is crucial for assessing the effectiveness and accuracy of the proposed method compared to established ground truth data. It visually demonstrates how closely the proposed method's output aligns with the expected results, thereby highlighting the performance and potential improvements or discrepancies in the algorithm.",
            "importance": 9
        },
        {
            "path": "output\\images\\ad15c071-0d1b-46a3-9117-a69ffe09dcc1.jpg",
            "description": "This figure illustrates a neural network architecture, likely a deep convolutional neural network (CNN) or a variant designed for image processing tasks. The architecture is composed of several layers:\n\n- **Input Layer:** The input is a tensor of size 304 x 228 x 3, representing an image with three color channels.\n- **Convolutional Layers:** The network uses a series of convolutional operations with different filter sizes (e.g., 76 x 57 x 64) and depths, indicated by blocks with dimensions.\n- **Activation Functions:** ReLU activations are applied throughout the network, as indicated by red arrows.\n- **Pooling and Normalization:** Max-pooling layers with a 3x3 kernel and stride of 2 are used, along with batch normalization layers.\n- **Downsampling and Bottleneck:** The architecture has a downsampling path with increasing feature dimensions, reaching a bottleneck.\n- **Upsampling Path:** It also includes an up-convolution (or transposed convolution) path for upsampling feature maps, leading to a prediction layer.\n- **Skip Connections:** There are skip connections (d1 and d2) which likely facilitate information flow between layers.\n\nThe lower section of the figure details specific components like skip and projection modules, indicating modular or residual connections that enhance learning efficiency.\n\nThis figure is critical for understanding the methodology and design of the neural network, showcasing key components and processes involved in the architecture.",
            "importance": 9
        },
        {
            "path": "output\\images\\4a78d055-da9e-4419-abd7-9a4bd98d73d9.jpg",
            "description": "This figure illustrates different architectural components used in neural network upsampling techniques, specifically focusing on up-convolution and up-projection methods.\n\n- **(a) Up-convolution:** Demonstrates a basic up-convolution process involving a series of operations, including convolution, un-pooling, and ReLU activation, followed by interleaving to increase spatial resolution.\n  \n- **(b) Fast up-convolution:** Enhances the basic up-convolution by adding parallel convolutional paths and interleaving, allowing for more efficient computation and possibly improved performance.\n\n- **(c) Up-projection:** Displays a method for upsampling that involves projecting the lower-resolution feature map into a higher-resolution space. This includes convolution, interleaving, and summing components to form an upsampled output.\n\n- **(d) Fast up-projection:** An optimized version of up-projection, utilizing multiple convolutional operations and interleaving to achieve faster computation and potentially better results.\n\nThe figure is crucial for understanding the methodology and architecture being proposed or evaluated in the research, making it an important component for grasping the key innovations and techniques.",
            "importance": 9
        },
        {
            "path": "output\\images\\434d92f2-23d4-467e-bfd2-a6c8b173bda8.jpg",
            "description": "This figure likely compares the performance of different neural network models (AlexNet, VGG, and a proposed model) against a ground truth in reconstructing a 3D scene. Each sub-image represents the output of one model's attempt at scene reconstruction, highlighting differences in accuracy and detail. The \"proposed\" model's output is likely closer to the \"ground truth,\" suggesting an improvement or innovation in the research. This figure is important as it visually demonstrates the effectiveness of the proposed method compared to established models, making it a key part of understanding the research findings.",
            "importance": 8
        },
        {
            "path": "output\\images\\78371024-4597-46ed-b225-5c33ae5c4b13.jpg",
            "description": "This figure illustrates a convolutional neural network (CNN) architecture process, focusing on feature map transformations. It starts with a feature map undergoing an \"unpooling\" operation, which expands the map. A 5x5 convolutional filter is applied, as indicated by the convolution operation (conv). The filter consists of labeled segments (A, B, C, D), suggesting different learned patterns.\n\nThe process involves multiple convolutions on different input feature maps (represented in red, blue, purple, and orange), each convolved with distinct kernels. The resulting feature maps are combined or interleaved to form a complex, integrated feature map.\n\nThis diagram is important for understanding the methodology and mechanism of feature extraction and transformation in the CNN, highlighting the use of specific operations like unpooling and interleaving, which are crucial for the network's functioning and performance in tasks like image reconstruction or segmentation.",
            "importance": 8
        }
    ],
    "metadata": {
        "key_themes": [
            "Spatial Meshing",
            "Neural Network Optimization",
            "Super-Resolution",
            "Nonlinear Loss Function",
            "Bayesian Inference"
        ],
        "methodology": [
            "Neural Network Training",
            "3D Modeling",
            "Image Processing",
            "Estimation",
            "Loss functions"
        ],
        "domain": [
            "Autonomous Robotics"
        ],
        "strengths": [
            "3D Reconstruction",
            "DeepVision Framework",
            "Enhanced Detection Techniques",
            "Data Enhancement",
            "Superiority"
        ],
        "limitations": [
            "Training challenges",
            "3D Challenges",
            "Complexity",
            "Ground-truth ambiguity",
            "Sacrifice diversity for fidelity"
        ]
    },
    "content_chunks": [
        {
            "chunk_id": "1606.00373v2_chunk_0",
            "section": "methodology",
            "citations": [
                "5, 6",
                "6",
                "14",
                "31",
                "5",
                "7",
                "9",
                "23",
                "4, 21, 38",
                "4",
                "7",
                "7"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_1",
            "section": "methodology",
            "citations": [
                "23",
                "5, 6",
                "16",
                "6",
                "5"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_2",
            "section": "results",
            "citations": [
                "2",
                "27",
                "40",
                "5, 6",
                "23",
                "30",
                "36",
                "28",
                "6",
                "10",
                "15",
                "20",
                "16",
                "19",
                "37",
                "6",
                "27",
                "5",
                "5, 6, 15, 16, 19"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_3",
            "section": "results",
            "citations": [
                "5",
                "5",
                "5",
                "5",
                "16, 37",
                "5",
                "10",
                "20",
                "19",
                "16",
                "5"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_4",
            "section": "discussion",
            "citations": [
                "39",
                "40",
                "29",
                "30"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_5",
            "section": "conclusion",
            "citations": [
                "8",
                "9",
                "10",
                "11"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_6",
            "section": "other",
            "citations": [
                "34",
                "39",
                "33",
                "23",
                "26",
                "5",
                "35"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_7",
            "section": "other",
            "citations": [
                "30",
                "16, 20, 37",
                "10, 13",
                "5, 6, 16, 19, 37",
                "16, 37",
                "19",
                "27",
                "5, 6, 19",
                "16, 37",
                "23, 29",
                "7",
                "40"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_8",
            "section": "other",
            "citations": [
                "22, 32",
                "34",
                "39",
                "33",
                "8, 17, 29, 30",
                "29",
                "1",
                "30",
                "17",
                "15",
                "10, 13, 18, 20",
                "24",
                "3",
                "10",
                "18",
                "13",
                "20",
                "28",
                "14",
                "31",
                "6",
                "5",
                "5, 6",
                "27",
                "16, 19, 37",
                "19",
                "16",
                "37"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_9",
            "section": "other",
            "citations": [
                "25, 40"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_10",
            "section": "other",
            "citations": [
                "30",
                "20",
                "20",
                "20",
                "12",
                "11"
            ]
        },
        {
            "chunk_id": "1606.00373v2_chunk_11",
            "section": "other",
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "31",
                "20",
                "32",
                "21",
                "22",
                "23",
                "33",
                "34",
                "35",
                "24",
                "25",
                "26",
                "27",
                "28",
                "36",
                "37",
                "38"
            ]
        }
    ]
}