{
    "basic_info": {
        "title": "Unsupervised Learning of 3D Object Categories from Videos in the Wild",
        "authors": [
            "Philipp Henzler",
            "Jeremy Reizenstein",
            "Patrick Labatut",
            "Roman Shapovalov",
            "Tobias Ritschel",
            "Andrea Vedaldi",
            "David Novotny"
        ],
        "paper_id": "2103.16552v1",
        "published_year": 2021,
        "references": []
    },
    "raw_chunks": [
        {
            "text": "1. Introduction\n\nUnderstanding and reconstructing categories of 3D ob- jects from 2D images remains an important open challenge in computer vision. Recently, there has been progress in using deep learning methods to do so but, due to the difﬁculty of the task, these methods still have signiﬁcant limitations. In particular, early efforts focused on clean synthetic data such as ShapeNet [5], further simplifying the problem by assum- ing the availability of several images of each object instance,\n\n1\n\nknowledge of the object masks, object-centric viewpoints, etc. Methods such as [8, 7, 45, 54, 24] have demonstrated that, under these restrictive assumptions, it is possible to obtain high-quality reconstructions, motivating researchers to look beyond synthetic data.\n\nOther methods have attempted to learn the 3D shape of object categories given a number of independent views of real-world objects, such as a collection of images of different birds. However, in order to simplify the task, most of them use some form of manual or automatic annotations of the 2D images. We seek to relax these assumptions, avoiding the use of manual 2D annotations or a priori constraints on the reconstructed shapes.\n\nWhen it comes to high-quality general-purpose recon- structions, methods such as [36, 31, 33, 38, 63] have demon- strated that these can be obtained by training a deep neural network given only multiple views of a scene or object with- out manual annotations or particular assumptions on the 3D shape of the scene. Yet, these techniques can only learn a single object or scene at a time, whereas we are interested in modelling entire categories of 3D objects with related but different shapes, textures and reﬂectances. Nevertheless, the success of these methods motivates the use of multi-view supervision for learning collections of 3D objects.\n\nIn this paper, our ﬁrst goal is thus to learn 3D object categories given as input multiple views of a large collection of different object instances. To the best of our knowledge, this is the ﬁrst paper to conduct such a large-scale study of reconstruction approaches applied to learning 3D object categories from real-world 2D image data. Unfortunately, existing datasets for 3D category understanding are either small or synthetic. Thus, our ﬁrst contribution is to introduce a new dataset of videos collected ‘in the wild’ by Mechanical Turkers (ﬁg. 3). These videos capture a large number of object instances from the viewpoint of a moving camera, with an effect similar to a turntable. Viewpoint changes are estimated with high accuracy using off-the-shelf Structure from Motion (SfM) techniques. We collect hundreds of videos of several different categories.\n\nOur second contribution is to assess current reconstruc- tion technology on our new ‘in the wild’ data. For example, since each video provides several views of a single object with known camera parameters, it is suitable for an appli- cation of recent methods such as NeRF [36], and we ﬁnd that learning individual videos works very well, as expected. However, we show that a direct application of such models to several videos of different but related objects is much harder. In fact, we experiment with related representations such as voxels and meshes, and ﬁnd that they also do not work well if applied na¨ıvely to this task. This is true even though reconstructions are focused on a single object at a time — thus disregarding the background — suggesting that these architectures have a difﬁcult time at handling even\n\n2\n\nrelatively mild geometric variability.\n\nOur ﬁnal contribution is to propose a novel deep neural\n\nnetwork architecture to better learn 3D object categories in such difﬁcult conditions. We hypothesize that the main chal- lenge in extending high-quality reconstruction techniques, that work well for single objects, to object categories is the difﬁculty of absorbing the geometric variability that comes in tackling many different objects together. An obvious but important source of variability is viewpoint: given only real images of different objects, it is not obvious how these should align in 3D space, and a lack of alignment adds to the variability that the model must cope with. We address this issue with a novel idea of Warp-Conditioned Ray Embed- dings (WCR), a new neural rendering approach that is far less sensitive to inaccurate 3D alignment in the input data. Our method modiﬁes previous differentiable ray marchers to pool information at variable locations in input views, conditioned on the 3D location of reconstructed points.\n\nWith this, we are able to train deep neural networks that, given as input a small number of images of new object in- stances in a given target category, can reconstruct them in 3D, including generating high-quality new views of the objects. Compared to existing state-of-the-art reconstruction tech- niques, our method achieves better reconstruction quality in challenging datasets of real-world objects.",
            "section": "introduction",
            "section_idx": 0,
            "citations": [
                "5",
                "8, 7, 45, 54, 24",
                "36, 31, 33, 38, 63",
                "36"
            ]
        },
        {
            "text": "3. Method\n\nOverview. The goal of our method is to learn a model of a 3D object category from a dataset {Vp}Nvideo p=1 of video sequences. Each video Vp = (Ip t )0≤t<T p consists of T p ∈ N color frames Ip t ∈ R3×H×W. While we do not use any manual annotations for the videos, we do pre-process them using a Structure-from-Motion algorithm (COLMAP [48]). In this manner, for each video frame Ip t , we obtain sequence- speciﬁc camera poses gp t ∈ SE(3) and the camera instrinsics Kp t ∈ R3×3. We further obtain a segmentation mask mp t ∈ R1×H×W of the given category using Mask-RCNN [16].\n\nGenerative models trained in the wild were proposed in [9, 37]. While these methods can ‘hallucinate’ high-quality images, they, unlike us, are unable to also perform recon- struction of the objects given an image as input.\n\nImplicit representation of 3D scenes. NeRF [36] has raised the interest in neural scene representation due to its high-quality output, inspired by positional encoding pro- posed in [55] and differentiable volume rendering from [19, 54]. NSVF [31] combined NeRF and voxel grids to im- prove the scalability and expressivity of the model whereas Yariv et al [63] uses sphere tracing to render signed dis- tance ﬁelds. GRAF [50] extended NeRF to allow learning category-speciﬁc image generators, but do not perform re- construction, which is our goal. Our method is inspired by NeRF, however, we learn a model of a whole object category, rather than a single scene or object.\n\nThe model parametrizes the appearance and geometry of the object in each video with an implicit surface map Ψ:\n\nΨ : R3 × S2 × Z → R3 × R+ Ψ(x,r,z) = (c,σ),\n\nwhich labels each 3D scene point x ∈ R3 and viewing di- rection r ∈ S2 with an RGB triplet c(x,r,z) ∈ R3 and an occupancy value σ(x,z) ∈ (0,1] representing the opaque- ness of the 3D space. Furthermore, the implicit function Ψ is conditioned on a latent code z ∈ Z that captures the factors of variation of the object. By changing z we can adjust the occupancy ﬁeld to represent shapes of different objects of a visual category. As described in section 3.3, the design of the latent space Z is crucial for the success of the method.\n\nWhile we use video sequences to train the model, at test time we would like to reconstruct any new object instance from a small number of images. To this end, we learn an encoder function\n\nRecent works, [20, 46, 47, 64, 59] utilize sampled per- pixel encodings similar to us. [64] averages features over multiple views and [59] learns to interpolate between views in an IBR fashion [18, 2] which prevents inpainting unseen\n\nΦ : R3×H×W×Nsrc → Z,\n\nthat takes a number of input source images {Isrc 1 ,...,Isrc Nsrc of the new instance and produces the latent code z ∈ Z.\n\n3\n\n}\n\nGiven a known target view (different view than the source images) we render the implicit surface to form a color image ˆItgt ∈ R3×H×W and minimize the discrepancy between the rendered ˆItgt and the masked ground truth image Itgt.\n\nIn the following, we describe the main building blocks of our method. The rendering step follows Emission- Absorption raymarching [34, 19, 36, 53] as detailed in sec- tion 3.1. Section 3.2 describes the speciﬁcs of the surface function Ψ, and section 3.3 introduces the main technical contribution — a novel Warp-Conditioned Ray Embedding that deﬁnes the image encoder Φ.",
            "section": "methodology",
            "section_idx": 0,
            "citations": [
                "48",
                "16",
                "9, 37",
                "36",
                "55",
                "19, 54",
                "31",
                "63",
                "50",
                "20, 46, 47, 64, 59",
                "64",
                "59",
                "18, 2",
                "34, 19, 36, 53"
            ]
        },
        {
            "text": "4. Experiments\n\nWe discuss implementation details, data and evaluation\n\nprotocols (section 4.1) and assess our method and baselines on the tasks of novel-view synthesis and depth prediction. Implementation details. As noted in section 3.3, al- though WCR is in principle capable of dealing with the scene misalignments by itself, we found it beneﬁcial to ap- proximately “synchronize” the viewpoints of different videos in pre-processing, using a modiﬁed version of the method from [40]. First, we use the scene point clouds from SfM to register translation and scale by centering (subtracting the mean) and dividing by average per-dimension variance, re- sulting in adjusted viewpoints ¯gt. We then proceed with train- ing the rotation part of the viewpoint factorization branch of the VpDR network from [40], in order to align the rotational components of the viewpoints.\n\nAMT\n\nFreiburg Cars\n\nTrain-test Test Train-test Test Method (RGB YEG TOU perth (RGB NGG To perth (RGB VGC TOU erent (RGB YEG TOU gperth Mesh 0.10 1.17 0.60 5.13 010 1.16 0.60 5.09 0.14 2.03 060 1.19 O17 2.17 0.56 1.06 Voxel 0.06 1.05 0.78 2.14 0.09 1.13 0.66 3.07 0.05 1.58 089 0.59 0.16 2.05 0.51 2.18 Voxel+MLP 0.06 1.04 0.78 1.95 0.09 1.13 0.65 2.87 0.05 147 0.88 048 0.16 2.06 0.54 1.97 MLP 0.04 0.90 0.87 1.38 0.09 1.13 0.65 3.59 0.04 1.39 0.87 0.59 O15 2.03 0.47 2.52 Ours 0.03 0.86 0.88 1.31 0.05 0.93 0.83 1.90 0.04 1.39 0.90 0.48 0.12 1.89 0.62 1.60\n\nTable 1: Novel-view synthesis on AMT Objects and Freiburg Cars. Each row evaluates either a baseline or our method. Results are reported for two perceptual metrics (8°, (YS, depth error ¢?°?\"\", and intersection-over-union (IoU). For training we randomly selected between | and 7 source images. For testing we separately calculated the error metrics for 1, 3, 5 and 7 source images respectively and provide the average among those. For a more detailed evaluation we refer to the supplemental. Lower is better for (RB, (Y66, and ¢?°?\", whereas higher is better for IoU. The best result is bolded.\n\n1\n\n1\n\n1",
            "section": "methodology",
            "section_idx": 1,
            "citations": [
                "40",
                "40"
            ]
        },
        {
            "text": "Unsupervised Learning of 3D Object Categories from Videos in the Wild Supplementary material\n\nA. Additional implementation details\n\nIn this section, we provide more detailed information about the dense image descriptors Φ as well as the neural radiance ﬁeld Ψ. Furthermore, we give more insights into the training process.\n\nA.1. Dense image descriptors\n\nThis section describes in more detail the dense pixel-wise embeddings Φ(It) introduced in Section 3.3 in the main paper.\n\nFor a given source image It, the embedding ﬁeld Φ(It) is composed of 3 different types of features: 1) learned 5 · 32-dimensional dense pixel-wise features output by a deep convolutional encoder network ΦU-Net, 2) raw image rgb colors It ∈ R3×H×W, and 3) the segmentation mask mt ∈ R1×H×W.\n\nDense feature extractor ΦU-Net. The architecture of the U-Net inside ΦU-Net is deﬁned as follows (a detailed visuali- sation is present in Fig. 7). A source image Isrc ∈ R3×H×W, masked by msrc (retrieved from Mask-RCNN), is fed into a ResNet-50 which returns spatial features from intermediate convolutional layers (layer1, layer2, layer3, layer4, layer5), and the ﬁnal linear ResNet layer which outputs global fea- tures zCNN, i.e. non-spatial. Each feature layer including the global one is then passed through a 1x1 convolution to equal- ize the size of all feature channels to 32. The spatial features are further bilinearly upsampled to the spatial size of the source image and concatenated along the channel dimension to create a dense embedding ﬁeld ΦU-Net(It) ∈ R5·32×H×W.\n\nx ~ > —> 0o(x) 2(x, I\") M 197 a?\n\nFigure 6: The neural radiance ﬁeld Ψ is represented by an MLP. It takes as input the warp-conditioned embedding z(x), the harmonic positional embedding γ(x) and to account for view point variations the harmonic directional embedding γ(r). It returns the rgb and opacity values.\n\nference that we additionally condition the ﬁeld with our warp-conditioned ray embedding, see Fig. 6.\n\nA.2. Training details\n\nWe trained both the U-Net encoder ΦU-Net and the neural radiance ﬁeld Ψ with Adam optimizer. We set the batch size to 8 and the learning rate to 1e-4. Our method as well as all baselines were trained on an NVIDIA Tesla V100 for 7 days. For all raymarching baselines and our method, we shoot 1024 rays per iteration through random image pixels in Monte-Carlo fashion. For each ray we ﬁrst uniformly sample 128 times along the ray in order to retrieve a coarse rendering (voxel or mlp based depending on the method used). In the second pass we sample each ray 128 times based on probabilistic importance sampling following [36].\n\nFor the mesh baseline we shoot rays for each pixel per iteration and use soft rasterization to predict the surface inter- section. In addition to the losses used for the other baselines as well as our method, we additionally use a negative IoU loss Liou, a Laplacian loss Llap and smoothness loss Lsm according to [44] and weighted them with 1.0, 19.0, 1.0 respectively.",
            "section": "methodology",
            "section_idx": 2,
            "citations": [
                "36",
                "44"
            ]
        },
        {
            "text": "B. Additional qualitative results\n\nAdditional qualitative results are available presented in Fig. 9 and Fig. 8. Also, we provide more qualitative results on our project webpage: https://henzler.github. io/publication/unsupervised_videos/. The page contains comparison of our method to baselines by showing the scenes from the train-test or test subsets ren- dered from a viewpoint that rotates around the object of interest.\n\nC. Test-time view ablation\n\nFurthermore, we also provide a view ablation of our method at test time. Recall that we randomly sample be- tween 1 and 7 source images during training. During test time we evaluated our method separately on 1, 3, 5 and 7 views as input. In the main paper we provide an average of those numbers. In Table 3 we give insight into how changing the number of source views affects performance. Not surpris- ingly, increasing the numbers of source views consistently improves all metrics.\n\nNeural radiance ﬁeld Ψ. Our scene is represented by a neural radiance ﬁeld Ψ similar to [36] with the only dif-\n\n11\n\nAMT\n\nFreiburg Cars\n\nTrain-test Test Train-test Test Method 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 Mesh 1.163 1.167 1.168 1.169 1.160 1.161 1.163 1.163 2.030 2.029 2.028 2.023 2.170 2.168 2.166 2.167 ~» Voxe 1.052 1.051 1.051 1.051 1.127 1.127 1.127 1.127 1.581 1.581 1.580 1.580 2.050 2.050 2.046 2.046 ey Voxel+MLP 1.041 1.040 1.040 1.040 1.131 1.130 1.130 1.130 1.469 1.468 1.468 1.468 2.067 2.063 2.063 2.064 MLP 0.900 0.899 0.899 0.899 1.130 1.130 1.130 1.131 1.391 1.389 1.389 1.389 2.027 2.025 2.024 2.025 Ours 0.905 0.846 0.837 0.832 1.007 0.921 0.896 0.883 1.450 1.381 1.372 1.359 1.945 1.897 1.874 1.863 Mesh 0.599 0.599 0.599 0.598 0.598 0.598 0.598 0.598 0.601 0.604 0.605 0.606 0.556 0.556 0.556 0.556 > Voxe 0.776 0.777 0.777 0.777 0.660 0.660 0.660 0.661 0.891 0.892 0.892 0.893 0.517 0.511 0.509 0.510 S Voxelt+MLP 0.775 0.776 0.777 0.776 0.652 0.654 0.654 0.654 0.878 0.878 0.878 0.878 0.540 0.541 0.542 0.541 MLP 0.871 0.871 0.872 0.872 0.654 0.653 0.653 0.653 0.872 0.872 0.872 0.872 0.472 0.470 0.472 0.471 Ours 0.866 0.884 0.886 0.889 0.774 0.788 0.787 0.787 0.889 0.897 0.898 0.897 0.600 0.624 0.629 0.632 Mesh 5.138 5.119 5.128 5.130 5.100 5.101 5.090 5.086 1.202 1.185 1.178 1.177 1.062 1.061 1.063 1.063 = Voxel 2.150 2.141 2.140 2.141 3.069 3.064 3.067 3.065 0.591 0.590 0.585 0.583 2.133 2.181 2.207 2.200 g. Voxel+MLP 1.958 1.942 1.942 1.941 2.881 2.868 2.861 2.864 0.478 0.479 0.479 0.479 1.972 1.979 1.968 1.968 ~~ MLP 1.389 1.378 1.377 1.377 3.583 3.587 3.590 3.593 0.595 0.593 0.594 0.593 2.521 2.530 2.519 2.520 Ours 1.593 1.291 1.201 1.172 2.186 1.847 1.802 1.776 0.535 0.467 0.457 0.453 1.606 1.595 1.589 1.603\n\nTable 3: We complement the evaluation of the impact of the number of source views during test time for the metrics: YSS, eperth IoU. We report results for 1, 3, 5 and 7 source images. The best result is bolded where lower is better for (YC, eo epth and higher is better for IoU.\n\na(x), 1, 09) = We Ld —_d > ———_! me mi —* +8 an [se De net MI\") v warp-conditioned ray embedding z(x, {Is\"*})\n\nFigure 7: The input to the dense feature extractor Φ is a source image from a given view. It ﬁrst makes use of a ResNet-50 (ΦU-Net) to retrieve the layer-wise features. Then, each layer is independently fed to a 1x1 convolution followed by bilinear upsmapling to the original input resolution. The resulting feature blocks are concatenated with the input image Isrc and its corresponding object mask msrc. In case there are multiple source images available, this process is repeated for each of them. Once all per-view features are obtained the warp-conditioned ray embedding is retrieved after applying the view-aggregation.\n\n12\n\n,",
            "section": "methodology",
            "section_idx": 3,
            "citations": [
                "36"
            ]
        },
        {
            "text": "3.1. Implicit surface rendering\n\nIn order to render a target image et, we emmit a ray from the camera center through each pixel, assigning the color of ray’s first ‘intersection’ with the surface to the respective pixel. Formally, let Q = {0,...,W —1} x {0,..., H-1} be an image grid, wu € ( the index of a pixel, and Z € Ry a depth value. Following the ray from the camera center through wu to depth Z > 0 results in the 3D point: x(u, Z) = Z-K- [u! 1)\", where K € R°*? are the camera intrinsics. The camera’s pose is given by an Euclidean transformation g'* € SE(3), where we use the convention that x = g'®'(x) maps points x expressed in the world reference frame to points X in camera coordinates.\n\nIn order to determine the color of a pixel u ∈ Ω, we then ‘shoot’ a ray seeking the surface intersection. To do so, we sample points Xu = (x(u,Zi))NZ+1 for depth values i=0 Z0 ≤ ··· ≤ ZNZ obtaining their colors and occupancies:\n\n(ci,σi) = Ψ(x(u,Zi),r,z), i = 0,...,NZ. (1)\n\nThe probability of the ray not intersecting the surface in the interval (Zi+1,Zi] is set to Ti = e−(Zi+1−Zi)σi(x(u,Zi),z) (transmission probability). Summing over all possible inter- sections Z0,...,Zi, the probability p(Z = Zi|u) of a ray terminating at depth Zi is thus deﬁned as:\n\nNz-1 i-l1 p(Z = Z\\u)=|( [7 )a-1), m=1- TI 4, j=0 i=0\n\nwith the overall probability of intersection ˆmu. Given the distributions of ray-termination probabilities p(Z|u), the ren- dered color ˆcu(Xu,r,z) ∈ R3 and opacity ˆσu(Xu,z) ∈ R are deﬁned as an expectation over the outputs of the implicit function within the range [0,...,NZ − 1]:\n\nNz-1 Nz-1 éy = > WZ = Ziluje;, Gu = > WZ = Z;\\u)oi. i=0 i=0\n\nSince we are only interested in rendering the interior of the object, the colors cu are softly-masked with ˆmu leading to the ﬁnal target image render ˆItgt ∈ R3×H×W:\n\nI® = I(g\",2) =moe. (2)\n\n4\n\nNote that the reconstruction depends on the target viewpoint gtgt and the object code z, which is viewpoint independent.\n\n3.2. Neural implicit surface\n\nNext, we detail the implicit surface function Ψ. Sim- ilar to previous methods [36, 39, 35], we exploit the representational power of deep neural networks and de- ﬁne Ψ as a deep multi-layer perceptron (MLP): (c,σ) = Ψnr(x,r,z). The network Ψnr follows a design simi- lar to [36]. In particular, the world-coordinates x are preprocessed with the harmonic encoding γNx (x) = f [sin(x),cos(x),...,sin(2Nx f x),cos(2Nx f x)] ∈ R2Nx f be- fore being input to the ﬁrst layer of the MLP. In order to enable modelling of viewpoint dependent color variations, we further use the harmonic encoding of the target ray direc- (rtgt(x)) ∈ R2Nr tion γNr f as input (see Figure 2).",
            "section": "results",
            "section_idx": 0,
            "citations": [
                "36, 39, 35",
                "36"
            ]
        },
        {
            "text": "1\n\n2021\n\n2\n\n0\n\n2 r a M 0 3 ] V C . s c [ 1 v 2 5 5 6 1 . 3 0 1 2 : v\n\ni\n\nX\n\nr\n\na\n\nUnsupervised Learning of 3D Object Categories from Videos in the Wild\n\nPhilipp Henzler1* Jeremy Reizenstein2 Patrick Labatut2 Roman Shapovalov2 Tobias Ritschel1 Andrea Vedaldi2 David Novotny2\n\n{reizenstein,plabatut,romansh,vedaldi,dnovotny}@fb.com\n\n{p.henzler,t.ritschel}@cs.ucl.ac.uk\n\n1University College London\n\n2Facebook AI Research\n\nTest input\n\nOutput: Monocular 3D reconstruction\n\nSSSe 6800 42 42\n\nFigure 1: We present a novel deep architecture that contributes Warp-conditioned Ray Embedding (WCR) to reconstruct and render new views (right) of object categories from one or few input images (middle). Our model is learned automatically from videos of the objects (left) and works on difﬁcult real data where competitor architectures fail to produce good results.\n\nAbstract\n\nOur goal is to learn a deep network that, given a small number of images of an object of a given category, recon- structs it in 3D. While several recent works have obtained analogous results using synthetic data or assuming the avail- ability of 2D primitives such as keypoints, we are interested in working with challenging real data and with no manual an- notations. We thus focus on learning a model from multiple views of a large collection of object instances. We contribute with a new large dataset of object centric videos suitable for training and benchmarking this class of models. We show that existing techniques leveraging meshes, voxels, or im- plicit surfaces, which work well for reconstructing isolated objects, fail on this challenging data. Finally, we propose a new neural network design, called warp-conditioned ray embedding (WCR), which signiﬁcantly improves reconstruc- tion while obtaining a detailed implicit representation of\n\n1Work completed during an internship at Facebook AI Research.\n\nthe object surface and texture, also compensating for the noise in the initial SfM reconstruction that bootstrapped the learning process. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on existing benchmarks and on our novel dataset. For additional material please visit: https://henzler. github.io/publication/unsupervised_videos/.",
            "section": "other",
            "section_idx": 0,
            "citations": []
        },
        {
            "text": "2. Related Work\n\nOur work is related to many prior papers that leveraged deep learning for 3D reconstruction.\n\nLearning synthetic 3D object categories. Early deep learning methods for 3D reconstruction focused on clean syn- thetic datasets such as ShapeNet [5]. Fully supervised meth- ods [7, 12] mapped 2D images to 3D voxel grids. Follow-up methods proposed several alternatives: [8, 62] predict a point clouds, Park et al. [43, 1] label each 3D point with its signed distance to the nearest surface point, [35, 6] predict binary per-point occupancies, [11, 10] proposed more structured occupancy functions, and [13, 58] reconstruct meshes from single views. All aforementioned methods require full su- pervision in form of images and corresponding 3D CAD models. In contrast, our method requires only a set of videos of an object category captured from a moving camera.\n\nMethods that avoid 3D supervision project 3D shapes to 2D images using differentiable rendering, allowing for image space optimization instead of 3D [45, 61, 54, 24, 22, 53].\n\nLearning 3D object categories in the wild. Early recon- struction methods for 3D object categories used Non-Rigid SfM (NR-SfM) applied to 2D keypoint annotations [4, 56, 3]. CMR [23] used NR-SfM and 2D keypoints to initialize the camera poses on the CUB [57] dataset based on differen- tiable mesh rendering [26, 32, 6]. The texturing model of CMR was improved in DIB-R [6].\n\nre ae . a 1 . i ¢ sre Le\n\n_ 450) ewan v m(@)] — warp-conditioned sampled features: [z#°(x), re ae ray directions: [rj°(x), ..., a target ray: r(x) —————————_ ray empecking 2X, {I\"5}) * re) + — t t = learne CH . Color loss — fixed yx) -y(r*(x)) i Mask loss variables ¢ sre Le\n\nFigure 2: Our method takes as input an image and produces per pixel features using a U-Net Φ. We then shoot rays from a target view and retrieve per-pixel features from one or multiple source images. Once all spatial feature vectors are aggregated into a single feature vector (see Section 3.3 for more details), we combine them with their harmonic embeddings and pass them to an MLP yielding per location colors and opacities. Finally, we use differentiable raymarching to produce a rendered image.\n\nInstead of assuming knowledge of pose, [28, 27, 14] as- sume a deformable 3D template. PlatonicGAN [19] enables template-free 3D reconstruction via differentiable emission- absorption raymarching, but requires knowledge of the camera-pose distribution.\n\nareas. Our method aggregates latent encodings, which allows for representing unseen areas. Furthermore, we observed that simply averaging features from signiﬁcantly different viewpoints, as done in [64], hurts performance. We thus propose to aggregate depending on view angles.\n\nSimilarly, [60] does not require pose supervision, but it has been demonstrated only for limited viewpoint variations. Li et al. [29] do not assume camera poses as input, but use the self-supervised semantic features of [21] as a proxy for 2D keypoints as well as further constraints such as symmetry to help the reconstruction. We avoid such constraints for the sake of generality. Exploiting the StyleGAN [25] latent space, Zhang et al. [65] only require very few manual pose annotations. Our method, furthermore, does not require keypoint or pose supervision; instead, it recovers scene- speciﬁc camera poses automatically by analyzing camera motion. [41, 42] canonically align point clouds by only supervising with relative pose, but only learn a shape model.",
            "section": "other",
            "section_idx": 1,
            "citations": [
                "5",
                "7, 12",
                "8, 62",
                "43, 1",
                "35, 6",
                "11, 10",
                "13, 58",
                "45, 61, 54, 24, 22, 53",
                "4, 56, 3",
                "23",
                "57",
                "26, 32, 6",
                "6",
                "28, 27, 14",
                "19",
                "64",
                "60",
                "29",
                "21",
                "25",
                "65",
                "41, 42"
            ]
        },
        {
            "text": "f\n\n3.3. Warp-conditioned ray embedding\n\nAn important component of our method is the design of the latent code z. A na¨ıve solution is to ﬁrst map a source image Isrc to a D-dimensional vector zCNN = ΦCNN(Isrc) ∈ RD with a deep convolutional neural network ΦCNN, fol- lowed by appending a copy of zCNN to each positional em- bedding γ(x) to form an input to the neural occupancy func- tion Ψnr. This approach, successfully utilized in [53, 32] for synthetic datasets where the training shapes are approxi- mately rigidly aligned, is however insufﬁcient when facing more challenging in-the-wild scenarios.\n\nTo show why there is an issue here, recall that our inputs are videos V” of different object instances, each consisting of a sequence (I?)o<:<r» of video frames, together with viewpoint transformations g? € SE(3) recovered by SfM. Crucially, due to the global coordinate frame and scaling ambiguity of the SfM reconstructions [15], there is no re- lationship between the camera positions g? and g? recon- structed for two different videos p # g. Even two iden- tical videos V? = V4, reconstructed using Sf{M from two different random initializations, will result in two differ- ent sets of cameras (9? Jo<tere. (gf = 9* 9) )o<terr, Te- lated by an unknown similarity transformation g* € S(3). Since the frames J? = J? are identical, the reconstruc- tion network ®cnn must assign to them identical codes: ZcNN,t = Zennx = Penn (I?) = Benn (I?) = Zénne- Plug- ging this in eq. (2), means that two identical frames are recon- structed from the same code zcnn,t but two different view- points g? # gf: IP = I(g?, zcnn.) = I(gf, zene) = Lf While of course we do not work with identical copies of the same videos, this extreme case demonstrates a fundamental issue with the naive model, where different object instances must be reconstructed with respect to unrelated viewpoints.\n\nWe can partially tackle this issue by using a variant of [40] to approximately align the viewpoint of different video se- quences before training (see supplemental).\n\nNext, we introduce a more fundamental change to the model that also helps addressing this issue. The idea is to change the implicit surface (1)\n\nΨWCR(x,z(x)), (3)\n\nsuch that the code z is a function of the queried ray point x in world coordinates. Given a source image Isrc t with viewpoint gt, the projection of this point in the image is: ut(x) = πt(x) = π(Kgtx) where π denotes the perspective projection operator R3 → Ω. In particular, if x is also a point on the surface of the object, then ut(x) is the image of the corresponding point in the source view Isrc .",
            "section": "other",
            "section_idx": 2,
            "citations": [
                "53, 32",
                "15",
                "40"
            ]
        },
        {
            "text": "t\n\nMore speciﬁcally, we task a convolutional neural net- work Φ to map the image Isrc to a feature ﬁeld Φ(Isrc t ) ∈ t RD×H×W (see supplementary for details). In this way, for each pixel ut in the source view, we obtain a corresponding embedding vector Φ(It)[ut(x)] (using differentiable bilinear interpolation [·]):\n\nzt(x) = Φ(It)[πt(x)] ∈ RD, (4)\n\nand call it Warp-Conditioned Ray Embedding (WCR).\n\nIntuitively, as shown in fig. 2, by using eqs. (3) and (4) during ray marching, the implicit surface network Vwcr can pool information from relevant 2D locations u, in the source view J/\"°. Importantly, this occurs in a manner which is invariant to the global viewpoint ambiguity. In fact, if the geometry is now changed by the application of an arbitrary similarity transformation g*, then the 3D point changes as x’! = g*x, but the viewpoint also changes as g} = gi(g*)~!, so that g/x’ = gi(g*)~1g*x = g:x and the encoding of the points x and x’ is the same: ®(J;)[7:(x)] = ®(1t)[7/(x’)] Finally, note that the network eq. (3) combines two sources of information: (1) codes z(x) that capture the appearance of each point in a manner which is invariant from the global coordinate transforms; and (2) the absolute location of the 3D point x (internally encoded by using position-sensitive coding 7(x)). The combination of 1) and 2) above allows to resolve misalignments by localizing the implicit surface equivariantly with changes of the global coordinates.\n\nMulti-view aggregation. Having described WCR for a single source image we now extend to the more common case with multiple source images. For a set of source views t }Nsrc {Isrc t=1 with their warp-conditioned embeddings zsrc t (x), source rays rsrc t (x), and the target ray rtgt(x) (see Figure 2), we calculate the aggregate WCR z(x,{Isrc t }):\n\n{If*}) = cat( al(x, {Zi\"°}), 27 (2, {Zr\"°}), zon ({Z7\"\"}))\n\nz(x,{Isrc\n\nas a concatenation (cat) of the angle-weighted mean and variance embedding zµ ∈ RD and zσ ∈ R+ respectively,\n\n5\n\nand a plain average zCNN = N−1 t zCNN,t over global src source embeddings zCNN,t.\n\na source The weighted the weight W(x) the source-view is closer R, is specific embedding 3.4. For where \\ between appearance the masked\n\nis a with ensuring to the which z7 € dimension- source + Lieb cross-entropy For the between\n\nThe mean z/(x, {I7\"}) = ve \" wr(x)Z7\"°(x) is a weighted average of the source embeddings z/\"°(x) with the weight w;,(x) defined as\n\naverage w;,(x) w(x) = Ms weights to the defined set Overall training, = 0.05. the loss target aigp\n\nr2\"(x)). constant weight viewpoint embedding of the ALmask cross-entropy mask. error\n\ngives from variance average deviations 3i8(x)- loss L as the ground mean-squared rendering.\n\nwt(x) = W(x)−1(1 + rsrc t (x) · rtgt(x)).\n\nW(x) the source-view is closer R, is specific embedding 3.4. For where \\ between appearance the masked sab Roitto . ae Figure the",
            "section": "other",
            "section_idx": 3,
            "citations": []
        },
        {
            "text": "W(x) = Ms\n\nThis imaged The as an standard objective the defined and the our learning\n\nt=1 wt(x) is a normalization constant ensuring the weights integrate to 1. This gives more weight to the source-view features that are imaged from a viewpoint which is closer to the target view. The variance embedding zσ ∈ R+ is deﬁned analogously as an average over dimension- speciﬁc wt(x)-weighted standard deviations of the source t (x)}Nsrc embedding set {zsrc t=1.\n\nmore a over deviations L = binary truth 3D object collection Turk. videos, the videos\n\nfeatures target analogously {z‘°(x)} learning we Lmask rendered £,. we view eS order to from from\n\nt=1.\n\nis defined embedding Overall For \\ = between the appearance masked sab Roitto aigp ae 9 3: the wild, videos frames show\n\nthe loss as the and ground rendering. learning a large Mechanical example of the\n\noptimize is opacity we use view and to study Amazon three\n\n3.4. Overall learning objective\n\ntraining, we 0.05. rendered loss £,. target eS In order we from from SfM cameras.\n\nFor training, we optimize the loss L = λLmask + Lrgb where λ = 0.05. Lmask is deﬁned as the binary cross-entropy between the rendered opacity and ground truth mask. For the appearance loss Lrgb we use the mean-squared error between the masked target view and our rendering.\n\nbinary truth 3D object collection Turk. videos, the videos data and method depth in\n\ndefined and the our learning a example of assess\n\nappearance masked sab Roitto aigp ae 9 3: the wild, videos frames show We discuss protocols the tasks WCR\n\nuse the and our study Amazon example and assess As\n\nto study Amazon three and novel-view details. principle by\n\nTE\n\naigp 9 3: In wild, we videos frames show SfM cameras. discuss (section tasks of WCR\n\nof The bottom together and and prediction. section dealing with beneficial different of the from (subtracting\n\nRoitto aigp . ae 9\n\nFigure 3: In order to study learning 3D object categories in the wild, we crowd-sourced a large collection of object- centric videos from Amazon Mechanical Turk. The top row shows frames from three example videos, the bottom two rows show SfM reconstructions of the videos together with tracked cameras.",
            "section": "other",
            "section_idx": 4,
            "citations": []
        },
        {
            "text": "4.1. AMT Objects and other benchmarks\n\nOne of our main contributions is to introduce the AMT Objects dataset, a large collection of object-centric videos that we collected (ﬁg. 3) using Amazon Mechanical Turk. The dataset contains 7 object categories from the MS COCO classes [30]: apple, sandwich, orange, donut, banana, carrot and hydrant. For each class, we ask Turkers to collect a video by looking ‘around’ a class instance, resulting in a turntable video. For reconstruction, we uniformly sampled 100 frames from each video, discarding any video where COLMAP pre- processing was unsuccessful. The dataset contains 169-457 videos per class. For each class, we randomly split videos into training and testing videos in an 8:1 ratio.\n\nIn order to assess the quality of view synthesis, we calcu- late the 2RC® error, between the target and predicted image. We also use the /Y© perceptual metric, which computes the £; distance between the two images encoded by means of the VGG-19 network [52] pretrained on ImageNet. For depth reconstruction, we compute the eperth distance be- tween ground truth depth map (obtained from COLMAP SfM) and the predicted one in the target view. Finally we report Intersection-over-Union (IoU) between the predicted object mask and the object mask obtained by Mask-RCNN in the target view.\n\n4.2. Baselines\n\nWe also consider the Freiburg Cars [51], consisting of 45 training and 5 testing videos of various parked cars.\n\nFor every video, we deﬁne three disjoint sets of frames on which we either train or evaluate: (1) train-train, (2) train-test and (3) test. For each training video, we form the train-test set by randomly selecting 16 frames and a disjoint train-train set containing the complement of train- test. While the train-train frames are utilized for training, the train-test frames are never seen during training and only serve for evaluation. The evaluation on the test set is the most challenging since it is conducted with views of previously unseen object instances.\n\nEvaluation protocol. Recall that, at test time, our network takes as input a certain number of source images Isrc and reconstructs a target image ˆItgt seen from a different view- point. We assess the view synthesis and depth reconstruction quality of this prediction. To this end, for each object cat- egory, we randomly extract a batch of 8 different images from the train-test and test respectively. To increase view variability we repeat this process 5 times for every object. For each batch one of the images is picked as a target image Itgt and from the remaining images we individually select 1,3,5,7 images and perform the forward pass to generate ˆItgt for each selection.\n\nIn this section we detail the baselines we compare with. The first is MLP, corresponding to a naive version of the latent global encoding Zcnn already discussed in section 3.3. Here, the N* source images {I?\"°})\") are first indepen- dently mapped to embedding vectors {z, € R7°°}N\" by a ResNet50 [17] encoder and subsequently averaged to form an encoding of the object zcnn = ie ye z,. A copy of Zcnn is then concatenated to each positional embedding (x) of each target ray point x. MLP renders with the EA ray marcher (section 3.1).\n\nThe second baseline is Voxel, which closely resem- bles [54]. This uses the same encoding scheme as MLP, but differs by the fact that the object is represented by a voxel grid. Speciﬁcally, zCNN is decoded with a series of 3D convolution-transpose layers to a 1283 voxel grid containing RGB and opacity values. Voxel also renders with EA.\n\nNext, Voxel+MLP is inspired by Neural Sparse Voxel ﬁelds [31] and marries NeRF [36] with voxel grids. As in Voxel, zCNN is ﬁrst 3D-deconvolved into a 1283 volume of 32-dimensional features. Each target view ray point x is then described with a positional embedding γ(x), and a latent feature zg(x) ∈ R32 trilinearly sampled at the voxel grid location x. The rest is the same as in MLP.\n\nFinally, the Mesh baseline uses the soft-rasterization of [6] as implemented in PyTorch3D [44] with the top-k face\n\n6",
            "section": "other",
            "section_idx": 5,
            "citations": [
                "30",
                "52",
                "51",
                "17",
                "54",
                "31",
                "36",
                "6",
                "44"
            ]
        },
        {
            "text": "Source image\n\nMesh\n\nVoxelt+MLP\n\ne<\n\nwe\n\nS\n\n~\n\nx54\n\n=\n\nFigure 4: Monocular reconstruction on Freiburg Cars and AMT Objects. In reach row, a single source image (1st column) is processed by one of the evaluated methods (Mesh, Voxel, MLP+Voxel, MLP, Ours - columns 2 to 6) to generate a prescribed target view (last column). We show results on the test split.\n\nAMT\n\nFreiburg Cars\n\nAMT Freiburg Cars Train-test Test Train-test Test Method 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 Mesh 096 .096 .096 .096 .102 .102 .102 102 .141 .141 .140 .140 166 .166 .166 .166 = Voxel 062.061 .061 .061 .091 091 091 .091 .055 .055 055 .054 159 159 158 .158 By Voxelt+MLP .059 .059 .058 = .059 =.090 .090 .090 .90 .045 .045 .045 045 158 .157 .158 157 MLP 037.036.036.036 =©.088 =.088 088 088 .041 .041 .041 041 152 152 .152 .152 Ours 038 .032 .031 .030 .058 .046 .043 .042 046 .041 .041 .040 130 120 .115 .114\n\nTable 2: We evaluate the impact of increasing the number of source views during test time for the (86 metric. Target renders and the corresponding metrics are produced for 1, 3, 5 and 7 source images. The best result is bolded where lower is better.\n\naccumulation. The scene encoding zCNN is converted with a pair of linear layers to: (1) a set {vi(z) ∈ R3}Nvertex i=1 of 3D vertex locations of the object mesh, and (2) a 128 × 128 UV map of the texture mapped to the surface of the mesh, which is rendered in order to evaluate the reconstruction losses from section 3.4. The mesh is initialized with an icosahedral sphere with 642 vertices.\n\n4.3. Quantitative Results\n\nTable | presents quantitative results on Freiburg Cars and the AMT Objects, respectively. In terms of all percep- tual metrics (RGB, YG) as well as depth and IoU, our method is on par with the MLP on the train-test split. On the test split, we outperform all other baselines in (RGB\n\n7\n\n1\n\n#2 Sre. image #1 #5 #6\n\nTarget image\n\nTgt. render w/ #1 src. image\n\nTgt. render w/ #3 src. images\n\nTgt. render w/ #5 src. images\n\no\n\nSre. image #1\n\n#2\n\ng ey 4 Mic ¢",
            "section": "other",
            "section_idx": 6,
            "citations": []
        },
        {
            "text": "Tgt. render w/ #1 src. image\n\nTet.\n\nw/ #3 src. images\n\nTgt. render w/ #5 src. images\n\nTgt. render w/ #7 src. images\n\n| 3 ; ?\n\nFigure 5: Reconstruction with multiple source views. For each object, the top row shows all available source images (columns 1-7) for a given target image (top right). The bottom row contains results conditioned on 1, 3, 5 or 7 source images. In addition to the rendered new RGB views we also provide shaded surface renderings.\n\nLYSG and IoU on all 7 classes of AMT Objects and Freiburg Cars. This indicates significantly better ability of our warp- conditioned embedding to generalize to previously unseen object instances.\n\nment when multiple source views Nsrc > 1 are available.\n\n5. Discussion and conclusions\n\nWe further find that our method is better at leveraging multiple source views Nee > 1, outperforming all baselines for the 288 error, see Table 2. When increasing the number of source images our method performance for all metrics improves whereas for all baselines it stays more or less constant. This further shows the effectiveness of the warp- conditioned embedding (WCR).\n\nRegarding depth reconstruction (¢?°\"\"), our method out- performs all alternatives on all datasets except the test split of Freiburg Ca Cars, where we are 2nd after Mesh. Here, we note that £; his only an approximate measure because: 1) the predicted depth is compared to the COLMAP-MVS esti- mate of depth [49], which tends to be noisy and; 2) the scale ambiguity in SfM reconstructions that supervise learning leads to a significantly unconstrained problem of estimating the scale of a testing scene given a small number of source views, which is challenging to resolve for any method.\n\n4.4. Qualitative Results\n\nFig. 4 provide qualitative comparisons for monocular novel-view synthesis. It shows that our method produces signiﬁcantly more detailed novel views, probably due to its ability to retrieve spatial encodings from the given source view. Fig. 5 further demonstrates the reconstruction improve-\n\nLimitations. Even though our method outperforms base- lines on the vast majority of metrics and datasets, there are still several limitations. First, the execution of the deep MLP at every 3D ray-location in a rendered frame is relatively slow (depending on the number of source views rendering takes between 3 and 8 sec for a 128×256 image on average), which makes a real-time deployment challenging. Secondly, due to our template-free approach, the object silhouettes can be blurry. Lastly, despite no manual labeling is neces- sary, our method still relies on segmentation masks that were automatically generated with Mask-RCNN.\n\nConclusions. In this paper, we have presented a method that is able to reconstruct category-speciﬁc 3D shape and ap- pearance from videos of object categories in the wild alone, without requiring manual annotations. We demonstrated that our main contribution, Warp-Conditioned Ray Embedding, can successfully deal with the inherent ambiguities present in the video SfM reconstructions that provide our supervi- sory signal, outperforming alternatives on a novel dataset of crowd-sourced object videos. Future work could include decomposition of shape, appearance and lighting allowing for more control over the rendered images.\n\n8",
            "section": "other",
            "section_idx": 7,
            "citations": [
                "49"
            ]
        },
        {
            "text": "References\n\n[1] Matan Atzmon and Yaron Lipman. Sal: Sign agnostic learn- ing of shapes from raw data. In Proc. CVPR, 2020. 2\n\n[2] Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured lumigraph ren- dering. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, 2001. 3\n\n[3] Joao Carreira, Abhishek Kar, Shubham Tulsiani, and Jitendra Malik. Virtual view networks for object reconstruction. In Proc. CVPR, 2015. 2\n\n[4] Thomas J Cashman and Andrew W Fitzgibbon. What shape are dolphins? building 3d morphable models from 2d images. PAMI, 35(1):232–244, 2013. 2\n\n[5] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information- rich 3d model repository. arXiv:1512.03012, 2015. 1, 2\n\n[6] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learning to pre- dict 3d objects with an interpolation-based differentiable ren- derer. In Proc. NeurIPS, pages 9609–9619, 2019. 2, 6\n\n[7] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A uniﬁed approach for single and multi-view 3d object reconstruction. In Proc. ECCV, pages 628–644. Springer, 2016. 2\n\n[8] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object reconstruction from a single image. In Proc. ICCV, pages 605–613, 2017. 2\n\n[9] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape induction from 2d views of multiple objects. In Proc. 3DV, pages 402–411. IEEE, 2017. 3\n\n[10] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for 3d shape. In Proc. CVPR, pages 4857–4866, 2020. 2\n\n[11] Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In Proc. ICCV, pages 7154–7164, 2019. 2\n\n[12] Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab- hinav Gupta. Learning a predictable and generative vector representation for objects. In Proc. ECCV. Springer, 2016. 2\n\n[13] Georgia Gkioxari, Justin Johnson, and Jitendra Malik. Mesh R-CNN. In Proc. ICCV, 2019. 2\n\n[14] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik. Shape and viewpoint without keypoints. Proc. ECCV, 2020. 3\n\n[15] Richard Hartley and Andrew Zisserman. Multiple view geom- etry in computer vision. Cambridge university press, 2003. 4\n\n[16] K. He, G. Gkioxari, and P. Doll´ar and. R. Girshick. Mask R-CNN. In Proc. ICCV, 2017. 3\n\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016. 6\n\n[18] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Trans Graph (Proc. SIGGRAPH Asia), 2018. 3\n\n9\n\n[19] Philipp Henzler, Niloy Mitra, and Tobias Ritschel. Escap- ing plato’s cave using adversarial training: 3d shape from unstructured 2d image collections. In Proc. ICCV, 2019. 3, 4\n\n[20] Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao, Jun Xing, Chloe LeGendre, Linjie Luo, Chongyang Ma, and Hao Li. Deep volumetric video from very sparse multi-view perfor- mance capture. In Proc. ECCV, pages 336–354, 2018. 3\n\n[21] Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, and Jan Kautz. Scops: Self-supervised co-part segmentation. In Proc. CVPR, 2019. 3\n\n[22] Eldar Insafutdinov and Alexey Dosovitskiy. Unsupervised learning of shape and pose with differentiable point clouds. In Proc. NeurIPS, pages 2802–2812, 2018. 2\n\n[23] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and Ji- tendra Malik. Learning category-speciﬁc mesh reconstruction from image collections. In Proc. ECCV, 2018. 2\n\n[24] Abhishek Kar, Christian H¨ane, and Jitendra Malik. Learning a multi-view stereo machine. In Proc. NeurIPS, 2017. 2\n\n[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proc. CVPR, 2019. 3\n\n[26] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu- ral 3d mesh renderer. In Proc. CVPR, 2018. 2\n\n[27] Nilesh Kulkarni, Abhinav Gupta, David F. Fouhey, and Shub- ham Tulsiani. Articulation-aware canonical surface mapping. In Proc. CVPR, 2020. 3\n\n[28] Nilesh Kulkarni, Abhinav Gupta, and Shubham Tulsiani. Canonical surface mapping via geometric cycle consistency. In Proc. ICCV, 2019. 3\n\n[29] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised single-view 3d reconstruction via semantic consistency. In Proc. ECCV, 2020. 3\n\n[30] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In Proc. ECCV, 2014. 6\n\n[31] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel ﬁelds. In Proc. NeurIPS, 2020. 2, 3, 6\n\n[32] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft raster- izer: A differentiable renderer for image-based 3d reasoning. In Proc. ICCV, 2019. 2, 4\n\n[33] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Saj- jadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In Proc. CVPR, 2021. 2\n\n[34] Nelson L. Max. Optical models for direct volume rendering. IEEE Trans. Vis. Comput. Graph., 1995. 4\n\n[35] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- bastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proc. CVPR, pages 4460–4470, 2019. 2, 4\n\n[36] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance ﬁelds for view synthe- sis. Proc. ECCV, 2020. 2, 3, 4, 6, 11",
            "section": "other",
            "section_idx": 8,
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33",
                "34",
                "35",
                "36"
            ]
        },
        {
            "text": "[37] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. HoloGAN: Unsupervised learning of 3D representations from natural images. In Proc. ICCV, 2019. 3\n\n[38] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learn- ing implicit 3d representations without 3d supervision. In Proc. CVPR, pages 3504–3515, 2020. 2\n\n[39] Michael Niemeyer, Lars M. Mescheder, Michael Oechsle, and Andreas Geiger. Occupancy ﬂow: 4d reconstruction by learning particle dynamics. In Proc. ICCV, 2019. 4\n\n[40] David Novotn´y, Diane Larlus, and Andrea Vedaldi. Learning the semantic structure of objects from web supervision. In Proceedings of the ECCV workshop on Geometry Meets Deep Learning, 2016. 4, 5\n\n[41] David Novotny, Diane Larlus, and Andrea Vedaldi. Learning 3d object categories by looking around them. In Proc. ICCV, 2017. 3\n\n[42] David Novotn´y, Diane Larlus, and Andrea Vedaldi. Capturing the geometry of object categories from video supervision. PAMI, 2018. 3\n\n[43] Jeong Joon Park, Peter Florence, Julian Straub, Richard New- combe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proc. CVPR, pages 165–174, 2019. 2\n\n[44] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay- lor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501, 2020. 6, 11\n\n[45] Danilo Jimenez Rezende, SM Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, and Nicolas Heess. Unsuper- vised learning of 3d structure from images. In Proc. NeurIPS, pages 4996–5004, 2016. 2\n\n[46] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor- ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitiza- tion. In Proc. ICCV, October 2019. 3\n\n[47] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization. In Proc. CVPR, June 2020. 3\n\n[48] Johannes Lutz Sch¨onberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proc. CVPR, 2016. 3\n\n[49] Johannes Lutz Sch¨onberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstruc- tured multi-view stereo. In Proc. ECCV, 2016. 8\n\n[50] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance ﬁelds for 3d-aware image synthesis. In Proc. NeurIPS, 2020. 3\n\n[51] Nima Sedaghat and Tomas Brox. Unsupevised generation of a viewpoint annotated car dataset from videos. In Proc. ICCV, 2015. 6\n\n[52] Karen Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. In Proc. ICLR, 2015. 6\n\n[53] Shubham Tulsiani, Alexei A Efros, and Jitendra Malik. Multi- view consistency as supervisory signal for learning shape and pose prediction. In Proc. CVPR, 2018. 2, 4\n\n10\n\n[54] Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Jiten- dra Malik. Multi-view supervision for single-view reconstruc- tion via differentiable ray consistency. In Proc. CVPR, 2017. 2, 3, 6\n\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. NeurIPS, 2017. 3\n\n[56] Sara Vicente, Joao Carreira, Lourdes Agapito, and Jorge Batista. Reconstructing PASCAL VOC. In Proc. CVPR, 2014. 2\n\n[57] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Re- port CNS-TR-2011-001, California Institute of Technology, 2011. 2\n\n[58] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proc. ECCV, 2018. 2\n\n[59] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini- vasan, Howard Zhou, Jonathan T Barron, Ricardo Martin- Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. arXiv, 2021. 3\n\n[60] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Un- supervised learning of probably symmetric deformable 3d objects from images in the wild. In Proc. CVPR, 2020. 3\n\n[61] Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and Honglak Lee. Perspective transformer nets: Learning single- view 3D object reconstruction without 3D supervision. In Proc. NIPS, pages 1696–1704, 2016. 2\n\n[62] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointﬂow: 3d point cloud generation with continuous normalizing ﬂows. In Proc. ICCV, pages 4541–4550, 2019. 2\n\n[63] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appear- ance. Proc. NIPS, 2020. 2, 3\n\n[64] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance ﬁelds from one or few images. arXiv, 2020. 3\n\n[65] Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio Torralba, and Sanja Fidler. Image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering. In Proc. ICLR, 2021. 3",
            "section": "other",
            "section_idx": 9,
            "citations": [
                "37",
                "38",
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45",
                "46",
                "47",
                "48",
                "49",
                "50",
                "51",
                "52",
                "53",
                "54",
                "55",
                "56",
                "57",
                "58",
                "59",
                "60",
                "61",
                "62",
                "63",
                "64",
                "65"
            ]
        },
        {
            "text": "Source image Mesh Voxel le RE\n\nMLP Ours Target image @’Q\\90 \"86 ’@\\@\n\nVoxel+MLP\n\nBS\n\n7 Lon\n\n_ = a Lon pe ek\n\n}\n\nFigure 8: In reach row, a single source image (1st column) is processed by one of the evaluated methods (Mesh, Voxel, MLP+Voxel, MLP, Ours - columns 2 to 6) to generate a prescribed target view (last column). We show results on the test split.\n\n13\n\nSre.\n\n#1\n\nimage Target image ~~ c 2 > ¢ Cc’ yes Tat. render w/ #1 src. image Tat. render w/ #3 src. images Tgt. render w/ #5 src. images Tat. render w/ #7 src. images A PF» A vA\n\n(@@L7e@)\n\n~ © w Ge (ee) y wv bd\n\na by a4\n\n@ @e\\ x © O+-4-+\n\n€ ww al v S\n\nFigure 9: Reconstruction with multiple source views. The top row for each object shows all available source images (columns 1-7) for a given target image (top right). The bottom row contains results conditioned on 1, 3, 5 or 7 source images. In addition to the rendered new RGB views we also provide shaded surface renderings.\n\n14",
            "section": "other",
            "section_idx": 10,
            "citations": []
        }
    ],
    "figures": [
        {
            "path": "output\\images\\c031d04d-0dd4-426a-a6ee-da46a18784b6.jpg",
            "description": "The figure illustrates a deep learning architecture for rendering a 3D object using neural radiance fields (NeRF). Key components include:\n\n- **Architecture Diagrams and Components**: The diagram shows a series of processes starting from input images of a 3D object, which are processed through learned and fixed components. Source images are used to extract sampled features and ray directions, which are aggregated for view synthesis.\n\n- **Graphs, Charts, and Data Visualizations**: There are no traditional graphs or charts, but the figure includes visual representations of processes such as raymarching, which is essential for rendering.\n\n- **Mathematical Formulas or Concepts**: It involves mathematical concepts like ray embedding and neural radiance fields, indicated by symbols such as \\( \\Psi \\), \\( \\sigma(x) \\), and \\( c(x, r^{tgt}) \\).\n\n- **Algorithm Flowcharts or Processes**: The flowchart describes the process of generating a target view from source images. Key steps include view aggregation, warp-conditioned ray embedding, and raymarching to compute color and mask loss.\n\n- **Results or Findings**: The outcome is the generation of a target image \\( \\hat{I}^{tgt} \\) from a different viewpoint, demonstrating the model's ability to synthesize novel views.\n\nThis figure is critical to understanding the methodology and innovation of the research, showing how the components work together to achieve 3D view synthesis.",
            "importance": 9
        },
        {
            "path": "output\\images\\cd306a7e-27aa-4aad-8ead-c32ecf310d78.jpg",
            "description": "1. **Architecture Diagrams and Components**:\n   - The figure showcases a neural network architecture that includes a U-Net structure (Φ_U-Net) and a CNN block (Z_CNN).\n   - The U-Net portion involves down-sampling and up-sampling layers with feature maps of sizes 64, 256, 512, 1024, 2048, and 256, utilizing a ResNet-50 backbone.\n   - The CNN block processes outputs through several layers with 32 feature channels, finally reducing to 3 channels.\n\n2. **Graphs, Charts, and Data Visualizations**:\n   - The right portion visualizes a process of view aggregation, where different colored dots represent features or data points being aggregated or transformed.\n\n3. **Mathematical Formulas or Concepts Illustrated**:\n   - Mathematical notations such as \\(Z_t^{src}(x)\\), \\(r_t^{tgt}\\), and \\(r_t^{src}(x)\\) suggest transformations or embeddings in the feature space, possibly related to image warping or transformation.\n\n4. **Algorithm Flowcharts or Processes**:\n   - A process flow is implied, starting from the input image \\(I_{src}\\), passing through the U-Net and CNN, and resulting in a warp-conditioned ray embedding \\(Z(x, \\{I_{src}\\})\\).\n\n5. **Results or Findings Being Presented**:\n   - The figure illustrates how the network processes input data into a form suitable for view aggregation, indicating the methodology for transforming image data through the network's architecture.\n\nThis figure is important as it visually represents the core methodology used in the research, showing how data is processed and transformed through the neural network architecture.",
            "importance": 8
        },
        {
            "path": "output\\images\\8c70fe6d-5a92-4028-b96f-dc923fbd80a3.jpg",
            "description": "This figure illustrates a neural network architecture, likely part of a machine learning model used in the research. It shows a sequence of layers, each with 256 units, indicating a deep learning architecture. The inputs to the network are labeled as \\( \\gamma(x) \\) with 63 units and \\( Z(x, I^{src}) \\) with 197 units, suggesting feature vectors or input data representations. \n\nThe architecture includes a series of transformations or operations across multiple layers, culminating in two distinct paths: one leading to \\( \\sigma(x) \\) and the other to \\( c(x, r) \\). The presence of operations such as addition and possibly concatenation (indicated by the \"+\" symbols) suggests a merging of information from different layers. A smaller block labeled with \"1\" and another with \"3\" might represent additional processing layers or different types of operations.\n\nThis figure is important as it likely represents the model's core architecture, detailing the processing steps involved in transforming input data into desired outputs. Understanding this architecture is crucial for interpreting the methodology and results of the research.",
            "importance": 8
        },
        {
            "path": "output\\images\\de7be4ab-7aec-40d3-a8a2-8c62e3206c11.jpg",
            "description": "This figure appears to demonstrate a process related to image synthesis or rendering, likely in a computer vision or graphics context. The top row shows a series of source images labeled #1 to #7, followed by a target image. These source images seem to represent different views or conditions of a similar object. The second row shows the results of rendering the target image using different combinations of these source images. Each rendering is labeled according to the source image(s) used, suggesting a comparison of how well each source input contributes to recreating or approximating the target image. This figure is important as it likely illustrates the effectiveness of a method or algorithm in generating realistic target images from various source inputs, highlighting the process's strengths or limitations.",
            "importance": 7
        },
        {
            "path": "output\\images\\e69a5d72-4b3b-4726-a6c3-61bf0e30c87b.jpg",
            "description": "The figure appears to show a series of images or visualizations likely related to a process or transformation. It seems to be demonstrating a step-by-step progression or comparison, possibly involving image processing or object recognition techniques. The top row may represent original or input images, while the bottom row could showcase processed or analyzed results. The variations in color and texture suggest stages of a computational procedure, such as segmentation, filtering, or enhancement. This figure would be important in understanding the methodology and visual results of the research, providing insight into how data is transformed or analyzed, supporting the study's findings.",
            "importance": 7
        },
        {
            "path": "output\\images\\73aa8e59-39b9-4f5d-94ab-e6559f5fb99c.jpg",
            "description": "This figure appears to depict a sequence of images illustrating a transformation or progression. It likely shows a process involving image manipulation or generation, such as a generative model output in machine learning. The images transition from a clear, possibly original state (a donut with icing and sprinkles) to a more distorted or abstract version, and then back to clarity. This could represent steps in an algorithm for image restoration, style transfer, or another image processing technique. The figure is important as it visually demonstrates the effectiveness or stages of the algorithm or model being discussed in the research.",
            "importance": 7
        },
        {
            "path": "output\\images\\59699336-fd18-48d6-8031-37ea818a2378.jpg",
            "description": "The figure appears to represent a 3D reconstruction or rendering process. It includes:\n\n- **Architecture Diagrams and Components**: The image shows a process involving two source images, \\( I_{\\text{src}1} \\) and \\( I_{\\text{src}2} \\), captured by cameras. These images are processed through a series of blue-colored layers, possibly representing a neural network or a series of transformations labeled as \\( \\Phi \\).\n\n- **Graphs, Charts, and Data Visualizations**: There are no explicit graphs or charts, but the visualization of the donut in 3D space suggests a rendering or reconstruction outcome.\n\n- **Mathematical Formulas or Concepts**: The figure includes notations like \\( r_{\\text{src}1} \\), \\( z_{\\text{src}1} \\), \\( r_{\\text{src}2} \\), \\( z_{\\text{src}2} \\), \\( x \\), \\( r_{\\text{tgt}} \\), and \\( \\hat{I}_{\\text{tgt}} \\). These might represent coordinates, transformations, or other mathematical parameters involved in the reconstruction or rendering process.\n\n- **Algorithm Flowcharts or Processes**: The process seems to flow from capturing source images to transforming them and finally reconstructing a 3D model of a donut. The arrows indicate data flow and transformation steps.\n\n- **Results or Findings**: The final result is the rendered image \\( \\hat{I}_{\\text{tgt}} \\) of the donut, which is presumably compared to a target image or used as the final output of the model.\n\nThis figure likely plays a significant role in illustrating the methodology or results of a 3D reconstruction or rendering technique, making it important for understanding the research.",
            "importance": 7
        },
        {
            "path": "output\\images\\0cebcf5c-9482-4b59-98fb-e9bd623b2191.jpg",
            "description": "This figure appears to show a series of images of an apple with varying levels of pixelation or blurring. The sequence likely demonstrates different stages of image processing or analysis, such as resolution changes, filtering effects, or data compression techniques. Each image might represent a step in an algorithm or a comparison of different methods for visual quality assessment. This figure provides supplementary information, possibly illustrating how a particular image processing technique affects visual clarity or data representation. It helps in understanding the effect of techniques discussed in the paper but is not central to the main findings or innovations.",
            "importance": 6
        },
        {
            "path": "output\\images\\7c064d41-905f-4b7c-b685-b1c5ebe8c0aa.jpg",
            "description": "I'm unable to analyze or interpret the image content for you. To help, I can provide guidance on how to evaluate figures based on typical components like architecture diagrams, graphs, or results presentation. Let me know if you need further assistance!",
            "importance": 5
        },
        {
            "path": "output\\images\\c4b2607e-48a5-44b6-9b40-2bfb626d59bb.jpg",
            "description": "** The figure appears to be a visual sequence showing variations of an object, likely used to illustrate differences or transformations. This might be part of a supplementary section demonstrating image processing, transformations, or comparisons in a project related to computer vision or pattern detection. While it provides visual evidence or examples, it likely supports larger findings rather than presenting core results or methodologies. \n\nFor a more precise analysis, consider the context within the paper and the captions or descriptions provided.",
            "importance": 5
        }
    ],
    "metadata": {
        "key_themes": [
            "Spatial Meshing",
            "Neural Network Optimization",
            "Statistical modeling",
            "Super-Resolution",
            "Bayesian Inference"
        ],
        "methodology": [
            "Neural Network Training",
            "3D Modeling",
            "Image Processing",
            "Feature detection",
            "Estimation"
        ],
        "domain": [
            "Autonomous Robotics",
            "Object removal"
        ],
        "strengths": [
            "3D Reconstruction",
            "DeepVision Framework",
            "Enhanced Detection Techniques",
            "Data Enhancement",
            "Superiority"
        ],
        "limitations": [
            "Training challenges",
            "3D Challenges",
            "Complexity",
            "Sensory Integration",
            "Scaling Impact"
        ]
    },
    "content_chunks": [
        {
            "chunk_id": "2103.16552v1_chunk_0",
            "section": "introduction",
            "citations": [
                "5",
                "8, 7, 45, 54, 24",
                "36, 31, 33, 38, 63",
                "36"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_1",
            "section": "methodology",
            "citations": [
                "48",
                "16",
                "9, 37",
                "36",
                "55",
                "19, 54",
                "31",
                "63",
                "50",
                "20, 46, 47, 64, 59",
                "64",
                "59",
                "18, 2",
                "34, 19, 36, 53"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_2",
            "section": "methodology",
            "citations": [
                "40",
                "40"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_3",
            "section": "methodology",
            "citations": [
                "36",
                "44"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_4",
            "section": "methodology",
            "citations": [
                "36"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_5",
            "section": "results",
            "citations": [
                "36, 39, 35",
                "36"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_6",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "2103.16552v1_chunk_7",
            "section": "other",
            "citations": [
                "5",
                "7, 12",
                "8, 62",
                "43, 1",
                "35, 6",
                "11, 10",
                "13, 58",
                "45, 61, 54, 24, 22, 53",
                "4, 56, 3",
                "23",
                "57",
                "26, 32, 6",
                "6",
                "28, 27, 14",
                "19",
                "64",
                "60",
                "29",
                "21",
                "25",
                "65",
                "41, 42"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_8",
            "section": "other",
            "citations": [
                "53, 32",
                "15",
                "40"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_9",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "2103.16552v1_chunk_10",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "2103.16552v1_chunk_11",
            "section": "other",
            "citations": [
                "30",
                "52",
                "51",
                "17",
                "54",
                "31",
                "36",
                "6",
                "44"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_12",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "2103.16552v1_chunk_13",
            "section": "other",
            "citations": [
                "49"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_14",
            "section": "other",
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33",
                "34",
                "35",
                "36"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_15",
            "section": "other",
            "citations": [
                "37",
                "38",
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45",
                "46",
                "47",
                "48",
                "49",
                "50",
                "51",
                "52",
                "53",
                "54",
                "55",
                "56",
                "57",
                "58",
                "59",
                "60",
                "61",
                "62",
                "63",
                "64",
                "65"
            ]
        },
        {
            "chunk_id": "2103.16552v1_chunk_16",
            "section": "other",
            "citations": []
        }
    ]
}