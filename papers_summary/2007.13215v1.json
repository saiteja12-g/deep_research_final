{
    "basic_info": {
        "title": "OASIS: A Large-Scale Dataset for Single Image 3D in the Wild",
        "authors": [
            "Weifeng Chen",
            "Shengyi Qian",
            "David Fan",
            "Noriyuki Kojima",
            "Max Hamilton",
            "Jia Deng"
        ],
        "paper_id": "2007.13215v1",
        "published_year": 2020,
        "references": [
            "1606.00373v2",
            "1704.02956v1",
            "1812.04072v2"
        ]
    },
    "detailed_references": {
        "ref_1": {
            "text": "Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-\ntendra Malik. Contour detection and hierarchical image seg-\nmentation. IEEE transactions on pattern analysis and ma-\nchine intelligence , 33(5):898–916, 2011.",
            "type": "numeric",
            "number": "1",
            "arxiv_id": null
        },
        "ref_2": {
            "text": "Jonathan T Barron and Jitendra Malik. Color constancy, in-\ntrinsic images, and shape estimation. In European Confer-\nence on Computer Vision , pages 57–70. Springer, 2012.",
            "type": "numeric",
            "number": "2",
            "arxiv_id": null
        },
        "ref_3": {
            "text": "Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala.\nOpenSurfaces: A richly annotated catalog of surface appear-\nance. ACM Trans. on Graphics (SIGGRAPH) , 32(4), 2013.",
            "type": "numeric",
            "number": "3",
            "arxiv_id": null
        },
        "ref_4": {
            "text": "D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A nat-\nuralistic open source movie for optical ﬂow evaluation. In\nA. Fitzgibbon et al. (Eds.), editor, European Conf. on Com-\nputer Vision (ECCV) , Part IV , LNCS 7577, pages 611–625.\nSpringer-Verlag, Oct. 2012.",
            "type": "numeric",
            "number": "4",
            "arxiv_id": null
        },
        "ref_5": {
            "text": "Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-\nimage depth perception in the wild. In Advances in Neural\nInformation Processing Systems , pages 730–738, 2016.",
            "type": "numeric",
            "number": "5",
            "arxiv_id": null
        },
        "ref_6": {
            "text": "Weifeng Chen, Shengyi Qian, and Jia Deng. Learning single-\nimage depth from videos using quality assessment networks.\nInProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 5604–5613, 2019.",
            "type": "numeric",
            "number": "6",
            "arxiv_id": null
        },
        "ref_7": {
            "text": "Weifeng Chen, Donglai Xiang, and Jia Deng. Surface nor-\nmals in the wild. In Proceedings of the 2017 IEEE Interna-\ntional Conference on Computer Vision, Venice, Italy , pages\n22–29, 2017.",
            "type": "numeric",
            "number": "7",
            "arxiv_id": null
        },
        "ref_8": {
            "text": "Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition , pages 3213–3223, 2016.",
            "type": "numeric",
            "number": "8",
            "arxiv_id": null
        },
        "ref_9": {
            "text": "Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas A Funkhouser, and Matthias Nießner. Scan-\nnet: Richly-annotated 3d reconstructions of indoor scenes.\nInCVPR , volume 2, page 10, 2017.",
            "type": "numeric",
            "number": "9",
            "arxiv_id": null
        },
        "ref_10": {
            "text": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition , pages 248–255. Ieee, 2009.",
            "type": "numeric",
            "number": "10",
            "arxiv_id": null
        },
        "ref_11": {
            "text": "Piotr Doll ´ar and C Lawrence Zitnick. Structured forests for\nfast edge detection. In Proceedings of the IEEE international\nconference on computer vision , pages 1841–1848, 2013.",
            "type": "numeric",
            "number": "11",
            "arxiv_id": null
        },
        "ref_12": {
            "text": "David Eigen, Christian Puhrsch, and Rob Fergus. Depth map\nprediction from a single image using a multi-scale deep net-\nwork. In Advances in neural information processing systems ,\npages 2366–2374, 2014.",
            "type": "numeric",
            "number": "12",
            "arxiv_id": null
        },
        "ref_13": {
            "text": "Antonio Fern ´andez-Baldera, Jos ´e M Buenaposada, and Luis\nBaumela. Badacost: multi-class boosting with costs. Pattern\nRecognition , 79:467–479, 2018.",
            "type": "numeric",
            "number": "13",
            "arxiv_id": null
        },
        "ref_14": {
            "text": "Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. Interna-\ntional Journal of Robotics Research (IJRR) , 2013.",
            "type": "numeric",
            "number": "14",
            "arxiv_id": null
        },
        "ref_15": {
            "text": "Derek Hoiem, Alexei A Efros, and Martial Hebert. Recover-\ning occlusion boundaries from an image. International Jour-\nnal of Computer Vision , 91(3):328–346, 2011.",
            "type": "numeric",
            "number": "15",
            "arxiv_id": null
        },
        "ref_16": {
            "text": "Eddy Ilg, Tonmoy Saikia, Margret Keuper, and Thomas\nBrox. Occlusions, motion and depth boundaries with a\ngeneric network for disparity, optical ﬂow or scene ﬂow esti-\nmation. In Proceedings of the European Conference on Com-\nputer Vision (ECCV) , pages 614–630, 2018.",
            "type": "numeric",
            "number": "16",
            "arxiv_id": null
        },
        "ref_17": {
            "text": "Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Bar-\nron, and Derek Hoiem. Boundary cues for 3d object shape\nrecovery. In CVPR , 2013.",
            "type": "numeric",
            "number": "17",
            "arxiv_id": null
        },
        "ref_18": {
            "text": "Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and temples: Benchmarking large-scale scene\nreconstruction. ACM Transactions on Graphics , 36(4), 2017.",
            "type": "numeric",
            "number": "18",
            "arxiv_id": null
        },
        "ref_19": {
            "text": "Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-\nerico Tombari, and Nassir Navab. Deeper depth prediction\nwith fully convolutional residual networks. In 2016 Fourth\ninternational conference on 3D vision (3DV) , pages 239–\n248. IEEE, 2016.",
            "type": "numeric",
            "number": "19",
            "arxiv_id": null
        },
        "ref_20": {
            "text": "Katrin Lasinger, Ren ´e Ranftl, Konrad Schindler, and Vladlen\nKoltun. Towards robust monocular depth estimation: Mixing\ndatasets for zero-shot cross-dataset transfer. arXiv preprint\narXiv:1907.01341 , 2019.",
            "type": "numeric",
            "number": "20",
            "arxiv_id": "1907.01341"
        },
        "ref_21": {
            "text": "Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 2041–2050, 2018.",
            "type": "numeric",
            "number": "21",
            "arxiv_id": null
        },
        "ref_22": {
            "text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740–755.\nSpringer, 2014.",
            "type": "numeric",
            "number": "22",
            "arxiv_id": null
        },
        "ref_23": {
            "text": "Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, and\nJan Kautz. Planercnn: 3d plane detection and reconstruction\nfrom a single image. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 4450–\n4459, 2019.",
            "type": "numeric",
            "number": "23",
            "arxiv_id": null
        },
        "ref_24": {
            "text": "Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, and Ya-\nsutaka Furukawa. Planenet: Piece-wise planar reconstruc-\ntion from a single rgb image. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2579–2588, 2018.",
            "type": "numeric",
            "number": "24",
            "arxiv_id": null
        },
        "ref_25": {
            "text": "N. Mayer, E. Ilg, P. H ¨ausser, P. Fischer, D. Cremers, A.\nDosovitskiy, and T. Brox. A large dataset to train con-\nvolutional networks for disparity, optical ﬂow, and scene\nﬂow estimation. In IEEE International Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2016.\narXiv:1512.02134.",
            "type": "numeric",
            "number": "25",
            "arxiv_id": "1512.02134"
        },
        "ref_26": {
            "text": "John McCormac, Ankur Handa, Stefan Leutenegger, and\nAndrew J Davison. Scenenet rgb-d: 5m photorealistic im-\nages of synthetic indoor trajectories with ground truth. arXiv\npreprint arXiv:1612.05079 , 2016.",
            "type": "numeric",
            "number": "26",
            "arxiv_id": "1612.05079"
        },
        "ref_27": {
            "text": "Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 4460–4470, 2019.",
            "type": "numeric",
            "number": "27",
            "arxiv_id": null
        },
        "ref_28": {
            "text": "Yvain Qu ´eau, Jean-Denis Durou, and Jean-Franc ¸ois Aujol.\nNormal integration: a survey. Journal of Mathematical\nImaging and Vision , 60(4):576–593, 2018.",
            "type": "numeric",
            "number": "28",
            "arxiv_id": null
        },
        "ref_29": {
            "text": "Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen\nKoltun. Playing for data: Ground truth from computer\ngames. In European Conference on Computer Vision , pages\n102–118. Springer, 2016.",
            "type": "numeric",
            "number": "29",
            "arxiv_id": null
        },
        "ref_30": {
            "text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International journal of\ncomputer vision , 115(3):211–252, 2015.",
            "type": "numeric",
            "number": "30",
            "arxiv_id": null
        },
        "ref_31": {
            "text": "Bryan C Russell and Antonio Torralba. Building a database\nof 3d scenes from user annotations. In Computer Vision and\nPattern Recognition, 2009. CVPR 2009. IEEE Conference\non, pages 2711–2718. IEEE, 2009.",
            "type": "numeric",
            "number": "31",
            "arxiv_id": null
        },
        "ref_32": {
            "text": "Ashutosh Saxena, Sung H Chung, and Andrew Y Ng. 3-d\ndepth reconstruction from a single still image. International\njournal of computer vision , 76(1):53–69, 2008.",
            "type": "numeric",
            "number": "32",
            "arxiv_id": null
        },
        "ref_33": {
            "text": "Thomas Sch ¨ops, Johannes L Sch ¨onberger, Silvano Galliani,\nTorsten Sattler, Konrad Schindler, Marc Pollefeys, and An-\ndreas Geiger. A multi-view stereo benchmark with high-\nresolution images and multi-camera videos. In Conference\non Computer Vision and Pattern Recognition (CVPR) , vol-\nume 2017, 2017.",
            "type": "numeric",
            "number": "33",
            "arxiv_id": null
        },
        "ref_34": {
            "text": "Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\nFergus. Indoor segmentation and support inference from\nrgbd images. In European Conference on Computer Vision ,\npages 746–760. Springer, 2012.",
            "type": "numeric",
            "number": "34",
            "arxiv_id": null
        },
        "ref_35": {
            "text": "Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-\nlis Savva, and Thomas Funkhouser. Semantic scene comple-\ntion from a single depth image. IEEE Conference on Com-\nputer Vision and Pattern Recognition , 2017.",
            "type": "numeric",
            "number": "35",
            "arxiv_id": null
        },
        "ref_36": {
            "text": "Andrew N Stein and Martial Hebert. Occlusion boundaries\nfrom motion: Low-level detection and mid-level reasoning.\nInternational journal of computer vision , 82(3):325, 2009.",
            "type": "numeric",
            "number": "36",
            "arxiv_id": null
        },
        "ref_37": {
            "text": "Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong\nZhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum,\nand William T Freeman. Pix3d: Dataset and methods for\nsingle-image 3d shape modeling. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2974–2983, 2018.",
            "type": "numeric",
            "number": "37",
            "arxiv_id": null
        },
        "ref_38": {
            "text": "Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,\nHaochen Wang, Falcon Z Dai, Andrea F Daniele, Moham-\nmadreza Mostajabi, Steven Basart, Matthew R Walter, et al.\nDiode: A dense indoor and outdoor depth dataset. arXiv\npreprint arXiv:1908.00463 , 2019.",
            "type": "numeric",
            "number": "38",
            "arxiv_id": "1908.00463"
        },
        "ref_39": {
            "text": "Peng Wang, Xiaohui Shen, Bryan Russell, Scott Cohen,\nBrian Price, and Alan L Yuille. Surge: Surface regular-\nized geometry estimation from a single image. In Advances\nin Neural Information Processing Systems , pages 172–180,\n2016.",
            "type": "numeric",
            "number": "39",
            "arxiv_id": null
        },
        "ref_40": {
            "text": "Xiaolong Wang, David Fouhey, and Abhinav Gupta. Design-\ning deep networks for surface normal estimation. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 539–547, 2015.",
            "type": "numeric",
            "number": "40",
            "arxiv_id": null
        },
        "ref_41": {
            "text": "Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao,\nRuibo Li, and Zhenbo Luo. Monocular relative depth percep-\ntion with web stereo data supervision. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 311–320, 2018.",
            "type": "numeric",
            "number": "41",
            "arxiv_id": null
        },
        "ref_42": {
            "text": "Yu Xiang, Wonhui Kim, Wei Chen, Jingwei Ji, Christopher\nChoy, Hao Su, Roozbeh Mottaghi, Leonidas Guibas, and Sil-\nvio Savarese. Objectnet3d: A large scale database for 3d\nobject recognition. In European Conference on Computer\nVision , pages 160–176. Springer, 2016.",
            "type": "numeric",
            "number": "42",
            "arxiv_id": null
        },
        "ref_43": {
            "text": "Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In Proceedings of the IEEE international conference\non computer vision , pages 1395–1403, 2015.",
            "type": "numeric",
            "number": "43",
            "arxiv_id": null
        },
        "ref_44": {
            "text": "Fengting Yang and Zihan Zhou. Recovering 3d planes from\na single image via convolutional neural networks. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV) , pages 85–100, 2018.",
            "type": "numeric",
            "number": "44",
            "arxiv_id": null
        },
        "ref_45": {
            "text": "Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, and\nShenghua Gao. Single-image piece-wise planar 3d recon-\nstruction via associative embedding. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1029–1037, 2019.",
            "type": "numeric",
            "number": "45",
            "arxiv_id": null
        },
        "ref_46": {
            "text": "Bernhard Zeisl, Marc Pollefeys, et al. Discriminatively\ntrained dense surface normal estimation. In European con-\nference on computer vision , pages 468–484. Springer, 2014.",
            "type": "numeric",
            "number": "46",
            "arxiv_id": null
        },
        "ref_47": {
            "text": "Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva,\nJoon-Young Lee, Hailin Jin, and Thomas Funkhouser.\nPhysically-based rendering for indoor scene understanding\nusing convolutional neural networks. The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2017.",
            "type": "numeric",
            "number": "47",
            "arxiv_id": null
        },
        "ref_Springer_2012": {
            "text": "pages 746–760. Springer, 2012. [35] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-",
            "type": "author_year",
            "key": "Springer, 2012",
            "arxiv_id": null
        },
        "ref_In_2009": {
            "text": "database. In 2009 IEEE conference on computer vision and",
            "type": "author_year",
            "key": "In, 2009",
            "arxiv_id": null
        },
        "ref_Ieee_2009": {
            "text": "pattern recognition , pages 248–255. Ieee, 2009. [11] Piotr Doll ´ar and C Lawrence Zitnick. Structured forests for",
            "type": "author_year",
            "key": "Ieee, 2009",
            "arxiv_id": null
        },
        "ref_In_2016": {
            "text": "with fully convolutional residual networks. In 2016 Fourth",
            "type": "author_year",
            "key": "In, 2016",
            "arxiv_id": null
        },
        "ref_Springer_2014": {
            "text": "ference on computer vision , pages 468–484. Springer, 2014. [47] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva,",
            "type": "author_year",
            "key": "Springer, 2014",
            "arxiv_id": null
        },
        "ref_Springer_2016": {
            "text": "Vision , pages 160–176. Springer, 2016. [43] Saining Xie and Zhuowen Tu. Holistically-nested edge de-",
            "type": "author_year",
            "key": "Springer, 2016",
            "arxiv_id": null
        },
        "ref_Recognition_2009": {
            "text": "Pattern Recognition, 2009. CVPR 2009. IEEE Conference",
            "type": "author_year",
            "key": "Recognition, 2009",
            "arxiv_id": null
        }
    },
    "raw_chunks": [
        {
            "text": "1. Introduction\n\nSingle-view 3D is the task of recovering 3D properties such as depth and surface normals from a single RGB im- age. It is a core computer vision problem of critical im- portance. 3D scene interpretation is a foundation for under- standing events and planning actions. 3D shape representa- tion is crucial for making object recognition robust against changes in viewpoint, pose, and illumination. 3D from a single image is especially important due to the ubiquity of monocular images and videos. Even with a stereo camera with which 3D can be reconstructed by triangulating match- ing pixels from different views, monocular 3D cues are still\n\nWe hypothesize that a major obstacle of single-image 3D is data. Unlike object recognition, whose progress has been propelled by datasets like ImageNet [10] covering diverse object categories with high-quality labels, single- image 3D has lacked an ImageNet equivalent that covers diverse scenes with high-quality 3D ground truth. Ex- isting datasets are restricted to either a narrow range of scenes [34, 9] or simplistic annotations such as sparse rela- tive depth pairs or surface normals [5, 7].\n\nIn this paper we introduce Open Annotations of Single- Image Surfaces (OASIS), a large-scale dataset for single- image 3D in the wild. It consists of human annotations that\n\n1\n\nenable pixel-wise reconstruction of 3D surfaces for 140,000 randomly sampled Internet images. Fig. 1 shows the human annotations of example images along with the reconstructed surfaces.\n\nA key feature of OASIS is its rich annotations of human 3D perception. Six types of 3D properties are annotated for each image: occlusion boundary (depth discontinuity), fold boundary (normal discontinuity), surface normal, relative depth, relative normal (orthogonal, parallel, or neither), and planarity (planar or not). These annotations together enable a reconstruction of pixelwise depth.\n\nTo construct OASIS, we created a UI for interactive 3D annotation. The UI allows a crowd worker to annotate the aforementioned 3D properties. It also provides a live, rotat- able rendering of the resulting 3D surface reconstruction to help the crowd worker ﬁne-tune their annotations.\n\nIt is worth noting that 140,000 images may not seem very large compared to millions of images in datasets like Ima- geNet. But the number of images can be a misleading met- ric. For OASIS, annotating one image takes 305 seconds on average. In contrast, verifying a single image-level label takes no more than a few seconds. Thus in terms of the to- tal amount of human time, OASIS is already comparable to millions of image-level labels.\n\nOASIS opens up new research opportunities on a wide range of single-image 3D tasks—depth estimation, sur- face normal estimation, boundary detection, and instance segmentation of planes—by providing in-the-wild ground truths either for the ﬁrst time, or at a much larger scale than prior work. For depth estimation and surface normals, pix- elwise ground truth is available for images in the wild for the ﬁrst time—prior data in the wild provide only sparse an- notations [5, 6]. For the detection of occlusion boundaries and folds, OASIS provides annotations at a scale 700 times larger than prior work—existing datasets [36, 17] have an- notations for only about 200 images. For instance segmen- tation of planes, ground truth annotation is available for im- ages in the wild for the ﬁrst time.\n\nTo facilitate future research, we provide extensive statis- tics of the annotations in OASIS, and train and evaluate leading deep learning models on a variety of single-image tasks. Experiments show that there is a large room for per- formance improvement, pointing to ample research oppor- tunities for designing new learning algorithms for single- image 3D. We expect OASIS to serve as a useful resource for 3D vision research.",
            "section": "introduction",
            "section_idx": 0,
            "citations": [
                "10",
                "34, 9",
                "5, 7",
                "5, 6",
                "36, 17"
            ]
        },
        {
            "text": "6. Experiments\n\nTo facilitate future research, we use OASIS to train and evaluate leading deep learning models on a suite of single- image 3D tasks including depth estimation, normal estima- tion, boundary detection, plane segmentation. Qualitative results are shown in Fig. 5. A train-val-test split of 110K, 10K, 20K is used for all tasks.\n\nFor each task we estimate human performance to pro- vide an upperbound accounting for the variance of human annotations. We randomly sample 100 images from the test\n\nImage Depth ‘Normal Occlusion Fold Planar Inst Imag’ Depth Normal ‘Occlusion Fold Planar Inst\n\nFigure 5. Qualitative outputs of the four tasks from representative models. More details and examples are in the appendix.\n\nset, and have each image re-annotated by 8 crowd workers. That is, each image now has “predictions” from 8 different humans. We evaluate each prediction and report the mean as the performance expected of an average human.\n\n6.1. Depth Estimation\n\n(Xz, Ye i982) LSIV_RMSE(Z, Z*) = min x ~ Asin (Xn Yn Zo) (6) — (0,0, heal\n\nWe ﬁrst study single-view depth estimation. OASIS pro- vides pixelwise metric depth in the wild. But as discussed in Sec 4, due to inherent single-image ambiguity, depth in OA- SIS is independently recovered within each continuous sur- face, after which the depth undergoes a surface-wise scal- ing to correct the depth order. The recovered depth is only accurate up to scaling within each continuous surface and ordering between continuous surfaces.\n\nGiven this, in OASIS we provide metric depth ground truths that is surface-wise accurate up to a scaling factor. This new form of depth necessitates new evaluation metrics and training losses.\n\nDepth Metric The images in OASIS have varied focal lengths. This means that to evaluate depth estimation, we cannot simply use pixelwise difference between a predicted depth map and the ground truth map. This is because the predicted 3D shape depends greatly on the focal length— given the same depth values, decreasing the focal length will ﬂatten the shape along the depth dimension. In practice, the focal length is often unknown for a test image. Thus, we require a depth estimator to predict a focal length along with depth. Because the predicted focal length may differ from the ground truth focal length, pixelwise depth difference is a poor indicator of how close the predicted 3D shape is to the ground truth.\n\nwhere S(p) denotes the surface a pixel p is on. The ground truth point cloud P∗ is normalized to a canonical scale by the standard deviation of its X coordinates σ(X∗). Under this metric, as long as P is accurate up to scaling and trans- lation, it will align perfectly with P∗, and get 0 error.\n\nNote that LSIV RMSE ignore the ordering between two separate surfaces; it allows objects ﬂoating in the air to be arbitrarily scaled. This is typically not an issue because in most scenes there are not many objects ﬂoating in the air. But we nonetheless also measure the correctness of depth ordering. We report WKDR [5], which is the percentage of point pairs that have incorrect depth order in the predicted depth. We evaluate on depth pairs sampled in the same way as [5], i.e. half are random pairs, half are from the same random horizontal lines.\n\nModels We train and evaluate two leading depth esti- mation networks on OASIS: the Hourglass network [5], and ResNetD [41], a dense prediction network based on ResNet50. Each network predicts a metric depth map and a focal length, which are together used to backproject pixels to 3D points, which are compared against the ground truth to compute the LSIV RMSE metric, which we optimize as the loss function during training. Note that we do not super- vise on the predicted focal length.\n\nA more reasonable metric is the Euclidean distance between the predicted and ground-truth 3D point cloud. Concretely, we backproject the predicted depth Z to a 3D point cloud P = {(Xp,Yp,Zp)} using f (the pre- dicted focal length), and ground truth depth Z∗ to P∗ = {(X∗ p,Y ∗ p ,Z∗ p)} using f∗ (the ground truth focal length). We then calculate the distance between P and P∗.\n\nThe metric also needs to be invariant to surface-wise depth scaling and translation. Therefore we introduce a surface-wise scaling factor λSi ∈ Λ, and a surface-wise translation δSi ∈ ∆, to align each predicted surface Si ∈ S in P to the ground truth point cloud P∗ in a least square manner. The ﬁnal metric, which we call Locally Scale- Invariant RMSE (LSIV RMSE), is deﬁned as:\n\nWe also evaluate leading pre-trained models that esti- mate single-image depth on OASIS, including FCRN [19] trained on ILSVRC [30] and NYU Depth [34], Hour- glass [21] trained on MegaDepth [21], ResNetD [41] trained on a combination of datasets including ILSVRC [30], Depth in the Wild [5], ReDWeb [41] and YouTube3D [6]. For net- works that do not produce a focal length, we use the valida- tion set to ﬁnd the best focal length that leads to the smallest LSIV RMSE, and use this focal length for each test image. In addition, we also evaluate plane, a naive baseline that predicts a uniform depth map.\n\nTab. 2 reports the results. In terms of metric depth, we see that networks trained on OASIS perform the best. This is expected because they are trained to predict a focal length and to directly optimize the LSIV RMSE metric. It is\n\nMethod ‘Training Data LSIV_RMSE WEKDR FCRN [19 ImageNet [30] + NYU [34] 0.67067) 30.95% G994%) Hourglass MegaDepth 0.67 (0:67) 38.37% (38.37%) ImageNet [30] + YouTube3D ne DIwW ae 0.66 (0:66) 34.01% (34-03%) TmageNet [30] + OASIS 037@37) 32.62% G204%) OASIS 0.47 (GAT) 39.73% (38-79%) OASIS 0.45 (AT) 39.01% (39-64%) = 0.67 (0-67) 10.00% (00-00%) Human (Approx) - 0.24 (024) 19.04% (19.33%)\n\nTable 2. Depth estimation performance of different networks on OASIS (lower is better). For networks that do not produce a focal length, we use the best focal length leading to the smallest error. See Sec. A7 about the numbers crossed out.\n\nnoteworthy that ImageNet pretraining provides a signiﬁcant beneﬁt even for this purely geometrical task. Off-the-shelf models do not perform better than the naive baseline, proba- bly because they were not trained on diverse enough scenes or were not trained to optimize metric depth error. In terms of relative depth, it is interesting to see that ResNetD trained on ImageNet and OASIS performs the best, even though the training loss does not enforce depth ordering. We also see that there is still a signiﬁcant gap between human perfor- mance and machine performance. At the same time, the gap is not hopelessly large, indicating the effectiveness of a large training set.\n\nOAS Wain RETENENOTTAT 2. 20 has TIGR se sas) | 13980435 AUC TOS ESOT (05016 (04718) 5968) (05253 (0.5227) 35) sal] my OS ST Human appro) |___~ 2 (ASA) 76.16 G59 35.24 047) | 08825 0.8870) 0.6514 (0.6439)\n\nTable 3. Surface normal estimation on OASIS. See Sec. A7 about the numbers crossed out.",
            "section": "results",
            "section_idx": 0,
            "citations": [
                "5",
                "5",
                "5",
                "41",
                "19",
                "30",
                "34",
                "21",
                "21",
                "41",
                "30",
                "5",
                "41",
                "6",
                "30",
                "34",
                "30",
                "30"
            ]
        },
        {
            "text": "6.4. Instance Segmentation of Planes\n\nOur last task focuses on instance segmentation of planes in the wild. This task is important because planes often have special functional roles in a scene (e.g. supporting surfaces, walls). Prior work has explored instance segmentation of planes, but is limited to indoor or driving environments [24, 45, 23, 44]. Thanks to OASIS, we are able to present the ﬁrst-ever evaluation of this task in the wild.\n\nWe follow the way prior work [24, 23, 45] performs this task: a network takes in an image, and produces instance masks of planes, along with an estimate of planar parame- ters that deﬁne each 3D plane. To measure performance, we report metrics used in instance segmentation literature [22]: the average precision (AP) computed and averaged across a range of overlap thresholds (ranges from 50% to 95% as in [22, 8]). A ground truth plane is considered correctly detected if it overlaps with one of the detected planes by more than the overlap threshold, and we penalize multiple detection as in [8]. We also report the AP at 50% overlap (AP50%) and 75% overlap (AP75%).\n\nPlanarReconstruction by Yu et al. [45] is a state-of-the- art method for planar instance segmentation. We train Pla- narReconstruction on three combinations of data: (1) Scan- Net [9] only as done in [45], (2) OASIS only, and (3) Scan- Net + OASIS. Tab. 6 compares their performance.\n\nAs expected, training on ScanNet alone performs the worse, because ScanNet only has indoor images. Train- ing on OASIS leads to better performance. Leveraging both ScanNet and OASIS is the best overall. But even the best network signiﬁcantly underperforms humans, suggest- ing ample space for improvement.\n\nMethod Training Data AP Ap AP™ ScanNet [9] 0.076 (0.076) | 0.16T 461) | 0.064 0.065) PlanarReconstruction OASIS. 0.125 (0427) | 0.249 (0.250) | 0.110 0442) ScanNet [9] + OASIS | 0.137 (0.439) | 0.262 (0.264) | 0.126 430) = 0.461 0542 0476 Human (Approx)\n\nTable 6. Planar instance segmentation performance on OASIS. See Sec. A7 about the numbers crossed out.",
            "section": "results",
            "section_idx": 1,
            "citations": [
                "24, 45, 23, 44",
                "24, 23, 45",
                "22",
                "22, 8",
                "8",
                "45",
                "9",
                "45",
                "9",
                "9"
            ]
        },
        {
            "text": "7. Conclusion\n\nWe have presented OASIS, a dataset of rich human 3D annotations. We trained and evaluated leading models on a variety of single-image tasks. We expect OASIS to be a useful resource for 3D vision research.\n\nAcknowledgement This work was partially supported by\n\na National Science Foundation grant (No. 1617767), a Google gift, and a Princeton SEAS innovation grant.\n\nReferences\n\n[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji- tendra Malik. Contour detection and hierarchical image seg- mentation. IEEE transactions on pattern analysis and ma- chine intelligence, 33(5):898–916, 2011.\n\n[2] Jonathan T Barron and Jitendra Malik. Color constancy, in- trinsic images, and shape estimation. In European Confer- ence on Computer Vision, pages 57–70. Springer, 2012.\n\n[3] Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala. OpenSurfaces: A richly annotated catalog of surface appear- ance. ACM Trans. on Graphics (SIGGRAPH), 32(4), 2013.\n\n[4] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A nat- uralistic open source movie for optical ﬂow evaluation. In A. Fitzgibbon et al. (Eds.), editor, European Conf. on Com- puter Vision (ECCV), Part IV, LNCS 7577, pages 611–625. Springer-Verlag, Oct. 2012.\n\n[5] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single- image depth perception in the wild. In Advances in Neural Information Processing Systems, pages 730–738, 2016.\n\n[6] Weifeng Chen, Shengyi Qian, and Jia Deng. Learning single- image depth from videos using quality assessment networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5604–5613, 2019.\n\n[7] Weifeng Chen, Donglai Xiang, and Jia Deng. Surface nor- mals in the wild. In Proceedings of the 2017 IEEE Interna- tional Conference on Computer Vision, Venice, Italy, pages 22–29, 2017.\n\n[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 3213–3223, 2016.\n\n[9] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas A Funkhouser, and Matthias Nießner. Scan- net: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, volume 2, page 10, 2017.\n\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.\n\n[11] Piotr Doll´ar and C Lawrence Zitnick. Structured forests for fast edge detection. In Proceedings of the IEEE international conference on computer vision, pages 1841–1848, 2013.\n\n[12] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep net- work. In Advances in neural information processing systems, pages 2366–2374, 2014.\n\n[13] Antonio Fern´andez-Baldera, Jos´e M Buenaposada, and Luis Baumela. Badacost: multi-class boosting with costs. Pattern Recognition, 79:467–479, 2018.\n\n[14] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. Interna- tional Journal of Robotics Research (IJRR), 2013.\n\n[15] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recover- ing occlusion boundaries from an image. International Jour- nal of Computer Vision, 91(3):328–346, 2011.\n\n[16] Eddy Ilg, Tonmoy Saikia, Margret Keuper, and Thomas Brox. Occlusions, motion and depth boundaries with a generic network for disparity, optical ﬂow or scene ﬂow esti- mation. In Proceedings of the European Conference on Com- puter Vision (ECCV), pages 614–630, 2018.\n\n[17] Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Bar- ron, and Derek Hoiem. Boundary cues for 3d object shape recovery. In CVPR, 2013.\n\n[18] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics, 36(4), 2017.\n\n[19] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed- erico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 2016 Fourth international conference on 3D vision (3DV), pages 239– 248. IEEE, 2016.\n\n[20] Katrin Lasinger, Ren´e Ranftl, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. arXiv preprint arXiv:1907.01341, 2019.\n\n[21] Zhengqi Li and Noah Snavely. Megadepth: Learning single- view depth prediction from internet photos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2041–2050, 2018.\n\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014.\n\n[23] Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, and Jan Kautz. Planercnn: 3d plane detection and reconstruction from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4450– 4459, 2019.\n\n[24] Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, and Ya- sutaka Furukawa. Planenet: Piece-wise planar reconstruc- tion from a single rgb image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2579–2588, 2018.\n\n[25] N. Mayer, E. Ilg, P. H¨ausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train con- volutional networks for disparity, optical ﬂow, and scene ﬂow estimation. In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2016. arXiv:1512.02134.\n\n[26] John McCormac, Ankur Handa, Stefan Leutenegger, and Andrew J Davison. Scenenet rgb-d: 5m photorealistic im- ages of synthetic indoor trajectories with ground truth. arXiv preprint arXiv:1612.05079, 2016.\n\n[27] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- bastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4460–4470, 2019.\n\n[28] Yvain Qu´eau, Jean-Denis Durou, and Jean-Franc¸ois Aujol. Normal integration: a survey. Journal of Mathematical Imaging and Vision, 60(4):576–593, 2018.",
            "section": "conclusion",
            "section_idx": 0,
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28"
            ]
        },
        {
            "text": "0\n\n2\n\n0 2 l u J 6 2 ] V C . s c [ 1 v 5 1 2 3 1 . 7 0 0 2 :\n\nv\n\narXiv\n\ni\n\nX\n\nr\n\na\n\nOASIS: A Large-Scale Dataset for Single Image 3D in the Wild\n\nWeifeng Chen1,2 Shengyi Qian1 David Fan2 Noriyuki Kojima1 Max Hamilton1 Jia Deng2 1University of Michigan, Ann Arbor 2Princeton University\n\n{wfchen,syqian,kojimano,johnmaxh}@umich.edu dfan@alumni.princeton.edu,jiadeng@princeton.edu\n\nReconstructed Pixel-wise Depth Human Annotations Human Annotations pape Reconstructed Pixel-wise Depth\n\nFigure 1. We introduce Open Annotations of Single-Image Surfaces (OASIS), a large-scale dataset of human annotations of 3D surfaces for 140,000 images in the wild. More examples in the appendix.\n\nAbstract\n\nnecessary in textureless or specular regions where it is dif- ﬁcult to reliably match pixel values.\n\nSingle-view 3D is the task of recovering 3D properties such as depth and surface normals from a single image. We hypothesize that a major obstacle to single-image 3D is data. We address this issue by presenting Open An- notations of Single Image Surfaces (OASIS), a dataset for single-image 3D in the wild consisting of annotations of de- tailed 3D geometry for 140,000 images. We train and eval- uate leading models on a variety of single-image 3D tasks. We expect OASIS to be a useful resource for 3D vision re- search. Project site: https://pvl.cs.princeton. edu/OASIS.\n\nSingle-image 3D is challenging. Unlike multiview 3D, it is ill-posed and resists tractable analytical formulation except in the most simplistic settings. As a result, data- driven approaches have shown greater promise, as evi- denced by a plethora of works that train deep networks to map an RGB image to depth, surface normals, or 3D mod- els [12, 19, 39, 16, 46, 27]. However, despite substantial progress, the best systems today still struggle with handling scenes “in the wild”— arbitrary scenes that a camera may encounter in the real world. As prior work has shown [5], state-of-art systems often give erroneous results when pre- sented with unfamiliar scenes with novel shapes or layouts.",
            "section": "other",
            "section_idx": 0,
            "citations": [
                "12, 19, 39, 16, 46, 27",
                "5"
            ]
        },
        {
            "text": "2. Related Work\n\n3D Ground Truth from Depth-Sensors and Computer Graphics Major 3D datatsets are either collected by sen- sors [34, 14, 32, 33, 9] or synthesized with Computer Graphics [4, 26, 35, 25, 29]. But due to the limitations of depth sensors and the lack of varied 3D assets for render-\n\ning, the diversity of scenes is quite limited. For example, sensor-based ground truth is mostly for indoor or driving scenes [34, 9, 26, 35, 14].\n\n3D Ground Truth from Multiview Reconstruction Single-image 3D training data can also be obtained by ap- plying classical Structure-from-Motion (SfM) algorithms on Internet images or videos [21, 41, 6]. However, classical SfM algorithms have many well known failure modes in- cluding scenes with moving objects and scenes with specu- lar or textureless surfaces. In contrast, humans can annotate all types of scenes.\n\n3D Ground Truth from Human Annotations Our work is connected to many previous works that crowdsource 3D annotations of Internet images. For example, prior work has crowdsourced annotations of relative depth [5] and surface normals [7] at sparse locations of an image (a single pair of relative depth and a single normal per image). Prior work has also aligned pre-existing 3D models to images [42, 37]. However, this approach has a drawback that not every shape can be perfectly aligned with available 3D models, whereas our approach can handle arbitrary geometry.\n\nOur work is related to that of Karsch et al. [17], who reconstruct pixelwise depth from human annotations of boundaries, with the aid of a shape-from-shading algo- rithm [2]. Our approach is different in that we annotate not only boundaries but also surface normals, planarity, and rel- ative normals, and our reconstruction method does not rely on automatic shape from shading, which is still unsolved and has many failure modes.\n\nOne of our inspirations is LabelMe3D [31], which anno- tated 3D planes attached to a common ground plane. An- other is OpenSurfaces [3], which also annotated 3D planes. We differ from LabelMe3D and OpenSurfaces in that our annotations recover not only planes but also curved sur- faces. Our dataset is also much larger, being 600× the size of LabelMe3D and 5× of OpenSurfaces in terms of the number of images annotated. It is also more diverse, because LabelMe3D and OpenSurface include only city or indoor scenes.",
            "section": "other",
            "section_idx": 1,
            "citations": [
                "34, 14, 32, 33, 9",
                "4, 26, 35, 25, 29",
                "34, 9, 26, 35, 14",
                "21, 41, 6",
                "5",
                "7",
                "42, 37",
                "17",
                "2",
                "31",
                "3"
            ]
        },
        {
            "text": "3. Crowdsourcing Human Annotations\n\nWe use random keywords to query and download Cre- ative Commons Flickr images with a known focal length (extracted from the EXIF data). Each image is presented to a crowd worker for annotation through a custom UI as shown in Fig. 2 (a). The worker is asked to mask out a region that she wishes to work on with a polygon of her choice, with the requirement that the polygon covers a pair of randomly pre-selected locations. She then works on the annotations and iteratively monitors the generated mesh (detailed in Sec 4) from an interactive preview win- dow (Fig. 2 (a)).\n\nSegment Normals Folds Sharp Occlusion Smooth Occlusion Draw | # Select % Delete Y (a) The Depth Scaling Procedure ; Surface-wise ik) depth scaling (1) 7 ——_ Zz Zz Before: incorrect depth order After: correct depth order (b)\n\nFigure 2. (a) Our UI allows a user to annotate rich 3D properties and includes a preview window for interactive 3D visualization. (b) An illustration of the depth scaling procedure in our backend.\n\nOcclusion Boundary and Fold An occlusion boundary de-\n\nnotes locations of depth discontinuity, where the surface on one side is physically disconnected from the surface on the other side. When it is drawn, the worker also speci- ﬁes which side of the occlusion is closer to the viewer, i.e. depth order of the surfaces on both sides of the occlusion. Workers need to distinguish between two kinds of occlu- sion boundaries. Smooth occlusion (green in Fig 2 (a)) is where the the closer surface smoothly curves away from the viewer, and the surface normals should be orthogonal to the occlusion line and parallel to the image plane, and pointing toward the further side. Sharp occlusion (red in Fig 2 (a)) has none of these constraints. On the other hand, fold de- notes locations of surface normal discontinuity, where the surface geometry changes abruptly, but the surfaces on the two sides of the fold are still physically attached to each other (orange in Fig 2 (a)).\n\nOcclusion boundaries segment a region into subregions, each of which is a continuous surface whose geometry can change abruptly but remains physically connected in 3D. Folds further segment a continuous surface into smooth sur- faces where the geometry vary smoothly without disconti- nuity of surface normals.\n\nSurface Normal The worker ﬁrst speciﬁes if a smooth sur- face is planar or curved. She annotates one normal at each planar surface which indicates the orientation of the plane. For each curved surface, she annotates normals at as many locations as she sees ﬁt. A normal is visualized as a blue arrow originating from a green grid (see the appendix), ren- dered in perspective projection according to the known fo- cal length. Such visualization helps workers perceive the normal in 3D [7]. To rotate and adjust the normal, the worker only needs to drag the mouse.\n\nRelative Normal Finally, to annotate normals with higher accuracy, the worker speciﬁes the relative normal between each pair of planar surfaces. She chooses between Neither, Parallel and Orthogonal. Surfaces pairs that are parallel or\n\northogonal to each other then have their normals adjusted automatically to reﬂect the relation.\n\nInteractive Previewing While annotating, the worker can click a button to see a visualization of the 3D shape con- structed from the current annotations (detailed later in Sec. 4). Workers can rotate or zoom to inspect the shape from different angles in a preview window (Fig 2 (a)). She keeps working on it until she is satisﬁed with the shape.\n\nQuality Control Completing our 3D annotation task re- quires knowledge of relevant concepts. To ensure good quality of the dataset, we require each worker to complete a training course to learn concepts such as occlusions, folds and normals, and usage of the UI. She then needs to pass a qualiﬁcation quiz before being allowed to work on our an- notation task. Besides explicitly selecting qualiﬁed work- ers, we also set up a separate quality veriﬁcation task on each collected mesh. In this task, a worker inspects the mesh to judge if it reﬂects the image well. Only meshes deemed high quality are accepted.\n\nTo improve our annotation throughput, we collected an- notations from three sources: Amazon Mechanical Turk, which accounts for 11% of all annotations, and two data an- notation companies that employ full-time annotators, who supplied the rest of the annotations.",
            "section": "other",
            "section_idx": 2,
            "citations": [
                "7"
            ]
        },
        {
            "text": "4. From Human Annotations to Dense Depth\n\nBecause humans do not directly annotate the depth value of each pixel, we need to convert the human annotations to pixelwise depth in order to visualize the 3D surface.\n\nGenerating Dense Surface Normals We ﬁrst describe how we generate dense surface normals from annotations. We assume the normals to be smoothly varying in the spatial domain, except across folds or occlusion boundaries where the normals change abruptly. Therefore, our system propa- gates the known normals to the unknown ones by requiring the ﬁnal normals to be smooth overall, but stops the propa-\n\nAeatve Densi Surface Normal Toe FY 1 107 10? 10? 104 10° we ak a7 ape Y Percentage of images ioe ° 11 10° 12345 10 x Zz Ratio of focal length to image width 100 ‘ontinuous surface mooth surface 2 Plane cs & 30} 2 ° S25 All-Occlusion —All-Fold Mixed o* * Boundary ey! 5 60 i 5S e 46.65% gs ‘5 40 @ ra 32.01% 2 10 So o| 21.30% § 20 5 “6 0 ‘AilPlanar—All-Curve Mixed ° 5 10 >15 Cuvature Number of Instances in a region (a) (b) (c) (d)\n\nFigure 3. Statistics of OASIS. (a) The distribution of focal length (unit: relative length to the image width). (b) The distribution of surface normals. (c) Boundary: the ratio of regions containing only occlusion, only fold, and both. Curvature: the distribution of regions containing only planes, only curved surfaces, and both. (d) The frequency distribution of each surface type in a region.\n\ngation at fold and occlusion lines.\n\nMore concretely, let Np denote the normal at pixel p on a normal map N, and F, O denotes the pixels belong to the folds and occlusion boundaries. We have a set of known normals ˜N at locations Pknown from (1) surface nor- mal annotations by workers, and (2) the pre-computed nor- mals along the smooth occlusion boundaries as mentioned in Sec 3. Each pixel p has four neighbors Φ(p). If p is on an occlusion boundary, its neighbors on the closer side of this boundary are ΓO(p). If p is on a fold line, only its neighbors ΓF(p) on one ﬁxed random side of this line are considered. We solve for the optimal normal N∗ using LU factorization and then normalize it into unit norm:\n\non such knowledge, we correct depth order by scaling the depth of each surface.\n\nWe now describe the details. Let S denotes the set of all continuous surface. From integration, we obtain the depth ZS of each S ∈ S. We then solve for a scaling factor XS for each S, which is used in scaling depth ZS. Let O denote the set of occlusion boundaries. Along O, we densely sample a set of point pairs B. Each pair (p,q) ∈ B has p lying on the closer side of one of the occlusion boundaries Oi ∈ O and q the further side. The continuous surface a pixel p lies on is S(p), and its depth is Zp. The set of optimal scaling factors X∗ is solved for as follows:\n\nN* = argmin y y N, — N,|?+ ae |Np al pEFUO qe®(p) q¢FUO > > [Np — Nal? + >> > Np — NqI? peO qeTo(p) peF qceTr(p)\n\ns.t. Np = ˜Np,∀p ∈ Pknown (2)\n\nGenerating Dense Depth Our depth generation pipeline consists of two stages: First, from surface normals and fo- cal length, we recover the depth of each continuous surface through integration [28] 1. Next, we adjust the depth or- der among these surfaces by performing surface-wise depth scaling (Fig. 2 (b)), i.e. each surface has its own scale factor.\n\n(1)\n\nX* = argmin Ss Xs (3) x ses\n\nst. XgipZpte<Xsq)ZqpVip.geB 4)\n\nXS ≥ η,∀S ∈ S (5)\n\nwhere € > 0 is aminimum separation between surfaces, and n > O is a minimum scale factor. Eq.(4) requires the sur- faces to meet the depth order constraints s pairs (p,q) € B after scaling. Meanwhile, Eq. the value of X so that they do not increase indefinitely. Af- ter correcting the depth order, the final depth for surface S' is X$Zs. We normalize and reproject the final depth to 3D as point clouds, and generate 3D meshes for visualization.",
            "section": "other",
            "section_idx": 3,
            "citations": [
                "28"
            ]
        },
        {
            "text": "5. Dataset Statistics\n\nOur design is motivated by this fact: in single-view depth recovery, depth within continuous surface can be recovered only up to an ambiguous scale; thus different surfaces may end up with different scales, leading to incorrect depth or- dering between surfaces. But workers already decide which side of an occlusion boundary is closer to the viewer. Based\n\n1We snap the z component of the surface normals to be no smaller than 0.3 so that the generated depth would not stretch into huge distance.\n\nStatistics of Surfaces Fig. 3 plots various statistics of the 3D surfaces. Fig. 3 (a) plots the distribution of focal length. We see that focal lengths in OASIS vary greatly: they range from wide angle to telezoom, and are mostly 1× to 10× of the width of the image. Fig. 3 (b) visualizes the distribu- tion of surface normals. We see that a substantial propor- tion of normals point directly towards the camera, suggest- ing that parallel-frontal surfaces frequently occur in natural\n\nNYU Depth [34] (depth mean: 2.471 m, depth std: 0.754 m) Tanks & Temples [18] (depth mean: 4.309m, depth std: 3.059m) Human-Human Human-Sensor CNN-Sensor Human-Human Human-Sensor CNN-Sensor Depth (EDist) 0.078m 0.095m 0.097m [19] 0.194m 0.213m 0.402m [19] Normals (MAE) 13.13◦ 17.82◦ 14.19◦ [47] 14.33◦ 20.29◦ 29.11◦ [47] Post-Rotation Depth (EDist) 0.037m 0.048m - 0.082m 0.080m - Depth Order (WKDR) 5.68% 8.67% 11.90% 9.28% 10.80% 32.13%\n\nTable 1. Depth and normal difference between different humans (Human-Human), between human and depth sensor (Human-Sensor), and between ConvNet and depth sensor (CNN-Sensor). The results are averaged over all human pairs.\n\nRGB Image (Giaitl Sensor Ground Truth MEME Human Annotated Shape IEE Shape after Rotating Normal “ Before Alignment (Top View) \\ Aligned View 1 Aligned View 2\n\nFigure 4. Humans estimate shape correctly but the absolute orientation can be slightly off, causing large depth error after perspective back-projection into 3D. Depth error drops signiﬁcantly (from 0.07m to 0.01m) after a global rotation of normals.\n\nscenes. Fig. 3 (c) presents region-wise statistics. We see that most regions (90%+) contain occlusion boundaries and close to half have both occlusion boundaries and folds (top). We also see that most regions (70%+) contain at least one curve surface (bottom). Fig. 3 (d) shows the histogram of the number of different kinds of surfaces in an annotated region. We see that most regions consist of multiple dis- connected pieces and have non-trivial geometry in terms of continuity and smoothness.\n\nAnnotation Quality We study how accurate and consistent the annotations are. To this end, we randomly sample 50 images from NYU Depth [34] and 70 images from Tanks and Temples [18], and have 20 workers annotate each im- age. Tab. I reports the depth and normal difference between human annotations, between human annotations and sen- sor ground truth, and between predictions from state-of-the- art ConvNets and sensor ground truth. Depth difference is measured by the mean Euclidean distance (EDist) be- tween corresponding points in two point clouds, after align- ing one to the other through a global translation and scaling (surface-wise scaling for human annotations and CNN pre- dictions). Normal difference is measured in Mean Angular Error (MAE). We see in Tab. I that human annotations are highly consistent with each other and with sensor ground truth, and are better than ConvNet predictions, especially when the ConvNet is not trained and tested on the same dataset.\n\nWe observe that humans often estimate the shape cor- rectly, but the overall orientation can be slightly off, caus- ing a large depth error against sensor ground truth (Fig. 4). This error can be particularly pronounced for planes close to orthogonal to the image plane. Thus we also compute the error after a rotational alignment with the sensor ground\n\ntruth—we globally rotate the human annotated normals (up to 30 degrees) before generating the shape. After account- ing for this global rotation of normals, human-sensor depth difference is further reduced by 47.96% (relative) for NYU and 62.44% (relative) for Tanks and Temples; a signiﬁcant drop of normal error is also observed in human-human dif- ference.\n\nWe also measure the qualitative aspect of human annota- tions by evaluating the WKDR metric [5], i.e. the percent- age of point pairs with inconsistent depth ordering between query and reference depth. Depth pairs are sampled in the same way as [5]. Tab. I again shows that human annota- tions are qualitatively accurate and highly consistent with each other.\n\nIt is worth noting that metric 3D accuracy is not re- quired for many tasks such as navigation, object manipu- lation, and semantic scene understanding—humans do well without perfect metric accuracy. Therefore human percep- tion of depth alone can be the gold standard for training and evaluating vision systems, regardless of its metric accuracy. As a result, our dataset would still be valuable even if it were less metrically accurate than it is currently.",
            "section": "other",
            "section_idx": 4,
            "citations": [
                "34",
                "18",
                "19",
                "19",
                "47",
                "47",
                "34",
                "18",
                "5",
                "5"
            ]
        },
        {
            "text": "Table 4. Cross-dataset generalization. See Sec. A7 about the num- bers crossed out.\n\n6.2. Surface Normal Estimation\n\nWe now turn to single-view surface normal estimation. We evaluate on absolute normal, i.e. the pixel-wise pre- dicted normal values, and relative normal, i.e. the parallel and orthogonal relation predicted between planar surfaces. Absolute Normal Evaluation We use standard metrics pro- posed in prior work [40]: the mean and median of angu- lar error measured in degrees, and the percentage of pixels whose angular error is within γ degrees.\n\nWe evaluate on OASIS four state-of-the-art networks that are trained to directly predict normals: (1) Hour- glass [7] trained on OASIS, (2) Hourglass trained on the Surface Normal in the Wild (SNOW) dataset [7], (3) Hour- glass trained on NYU Depth [34], and (4) PBRS, a normal estimation network by Zhang et al. [47] trained on NYU Depth [34]. We also include Front Facing, a naive baseline\n\nRGB Image Predicted Normals Ground-truth Normals by Humans\n\nFigure 6. Limitations of standard metrics: a deep network gets low mean angle error but important details are wrong.\n\npredicting all normals to be orthogonal to the image plane.\n\nTab. 3 reports the results. As expected, the Hourglass network trained on OASIS performs the best. Although SNOW is also an in-the-wild dataset, the same network trained on it does not perform as well, but is still better than training on NYU. Notably, the human-machine gap ap- pears fairly small numerically (17.27 versus 23.91 in mean angle error). However, we observe that the naive baseline can achieve 31.79; thus the dynamic range of this metric is small to start with, due to the natural distribution of normals in the wild. In addition, a close examination of the results suggests that these standard metrics of surface normals do not align well with perceptual quality. In natural images there can be large areas that dominate the metric but have uninteresting geometry, such as a blank wall in the back- ground. For example, in Fig. 6, a neural network gets the background correct, but largely misses the important details in the foreground. This opens up an interesting research question about developing new evaluation metrics.\n\nRelative Normal Evaluation We also evaluate the pre- dicted normals in terms of relative relations, speciﬁcally or- thogonality and parallelism. Getting these relations correct is important because it can help ﬁnd vanishing lines and perform self-calibration.\n\nWe ﬁrst deﬁne a metric to evaluate relative normal. From the human annotations, we ﬁrst sample an equal number of point pairs from surface pairs that are parallel, orthogo- nal, and neither. Given a predicted normal map, we look at the two normals at each point pair and measure the angle θ between them. We consider them orthogonal if |cos(θ − 90◦)| < cos(Θo), and parallel if |cos(θ)| > cos(Θp), where Θo, Θp are thresholds. We then plot the Precision-and-Recall curve for orthogonal by varying Θo, and measure its Area Under Curve AUCo, using neither and parallel pairs as negative examples. Varying Θp and using neither and orthogonal as negative examples, we ob- tain AUCp for parallel.\n\nTab. 3 reports results of relative normal evaluation. No- tably, all methods perform similarly, and all perform very poorly compared to humans. This suggests that existing ap- proaches to normal estimation have limitations in capturing orthogonality and parallelism, indicating the need for fur- ther research.\n\nCross-Dataset Generalization Next we study how net- works trained on OASIS generalize to other datasets. Sur-\n\nface normal estimation is ideal for such evaluation because unlike depth, which is tricky to evaluate on a new dataset due to scale ambiguity and varying focal length, a nor- mal estimation network can be directly evaluated on a new dataset without modiﬁcation.\n\nWe train the same Hourglass network on OASIS, and NYU, and report their performance on two benchmarks not seen in training: DIODE [38] and ETH3D [33]. From Tab. 4 we see that training on NYU underperforms on all benchmarks, showing that networks trained on scene- speciﬁc datasets have difﬁculties generalizing to diverse scenes. Training on OASIS outperforms on all benchmarks, demonstrating the effectiveness of diverse annotations.",
            "section": "other",
            "section_idx": 5,
            "citations": [
                "40",
                "7",
                "7",
                "34",
                "47",
                "34",
                "38",
                "33"
            ]
        },
        {
            "text": "6.3. Fold and Occlusion Boundary Detection\n\nOcclusion and fold are both important 3D cues, as they tell us about physical connectivity and curvature: Occlusion delineates the boundary at which surfaces are physically disconnected to each other, while Fold is where geometry changes abruptly but the surfaces remain connected.\n\nTask We investigate joint boundary detection and occlusion-versus-fold classiﬁcation: deciding whether a pixel is a boundary (fold or occlusion) and if so, which kind it is. Prior work has explored similar topics: Hoiem et al. [15] and Stein et al. [36] handcraft edge or motion fea- tures to perform occlusion detection, but our task involves folds, not just occlusion lines.\n\n<= Model Meio ——_ Edge: All Fold Edge: AllOcc = HED Hourglass Human (Approx) ODS 0.123 0.539 0.547 (0-533) 0.581 (0-585) 0.810 ols: 0.129 0.576 0.606 (0-584) 0.639 (0.639) O815 AP 0.02 0.44 0.488 (0.466) _ 0.530 (0.547) 0.642\n\nTable 5. Boundary detection performance on OASIS. See Sec. A7 about the numbers crossed out.\n\nEvaluation Metric We adopt metrics similar to standard ones used in edge detection [1, 43]: F-score by optimal threshold per image (OIS), by ﬁxed threshold (ODS) and average precision (AP). For a boundary to be considered correct, it has to be labeled correctly as either occlusion or fold. More details on the metrics can be found in the ap- pendix.\n\nTo perform joint detection of fold and occlusion, we adapt and train two networks on OASIS: Hourglass [5], and a state-of-the-art edge detection network HED [43]. The networks take in an image, and output two probabilities per pixel: pe is the probability of being an boundary pixel (oc- clusion or fold), and pf is the probability of being a fold pixel. Given a threshold τ, pixels whose pe < τ are nei- ther fold nor occlusion. Pixels whose pe > τ are fold if pF > 0.5 and otherwise occlusion.\n\nAs baselines, we also investigate how a generic edge de- tector would perform on this task. We use HED network trained on BSDS dataset [1] to detect image edges, and clas-\n\nsify the resulting edges to be either all occlusion (Edge: All Occ) or all fold (Edge: All Fold).\n\nAll results are reported on Tab 5. Hourglass outperforms HED when trained on OASIS, and signiﬁcantly outperforms both the All-Fold and All-Occlusion baselines, but still un- derperforms humans by a large margin, suggesting that fold and occlusion boundary detection remains challenging in the wild.",
            "section": "other",
            "section_idx": 6,
            "citations": [
                "15",
                "36",
                "1, 43",
                "5",
                "43",
                "1"
            ]
        },
        {
            "text": "[29] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision, pages 102–118. Springer, 2016.\n\n[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n\n[31] Bryan C Russell and Antonio Torralba. Building a database of 3d scenes from user annotations. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 2711–2718. IEEE, 2009.\n\n[32] Ashutosh Saxena, Sung H Chung, and Andrew Y Ng. 3-d depth reconstruction from a single still image. International journal of computer vision, 76(1):53–69, 2008.\n\n[33] Thomas Sch¨ops, Johannes L Sch¨onberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An- dreas Geiger. A multi-view stereo benchmark with high- resolution images and multi-camera videos. In Conference on Computer Vision and Pattern Recognition (CVPR), vol- ume 2017, 2017.\n\n[34] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European Conference on Computer Vision, pages 746–760. Springer, 2012.\n\n[35] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano- lis Savva, and Thomas Funkhouser. Semantic scene comple- tion from a single depth image. IEEE Conference on Com- puter Vision and Pattern Recognition, 2017.\n\n[36] Andrew N Stein and Martial Hebert. Occlusion boundaries from motion: Low-level detection and mid-level reasoning. International journal of computer vision, 82(3):325, 2009.\n\n[37] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum, and William T Freeman. Pix3d: Dataset and methods for single-image 3d shape modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2974–2983, 2018.\n\n[38] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z Dai, Andrea F Daniele, Moham- madreza Mostajabi, Steven Basart, Matthew R Walter, et al. Diode: A dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019.\n\n[39] Peng Wang, Xiaohui Shen, Bryan Russell, Scott Cohen, Brian Price, and Alan L Yuille. Surge: Surface regular- ized geometry estimation from a single image. In Advances in Neural Information Processing Systems, pages 172–180, 2016.\n\n[40] Xiaolong Wang, David Fouhey, and Abhinav Gupta. Design- ing deep networks for surface normal estimation. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 539–547, 2015.\n\n[41] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. Monocular relative depth percep- tion with web stereo data supervision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 311–320, 2018.\n\n[42] Yu Xiang, Wonhui Kim, Wei Chen, Jingwei Ji, Christopher Choy, Hao Su, Roozbeh Mottaghi, Leonidas Guibas, and Sil- vio Savarese. Objectnet3d: A large scale database for 3d object recognition. In European Conference on Computer Vision, pages 160–176. Springer, 2016.\n\n[43] Saining Xie and Zhuowen Tu. Holistically-nested edge de- tection. In Proceedings of the IEEE international conference on computer vision, pages 1395–1403, 2015.\n\n[44] Fengting Yang and Zihan Zhou. Recovering 3d planes from a single image via convolutional neural networks. In Pro- ceedings of the European Conference on Computer Vision (ECCV), pages 85–100, 2018.\n\n[45] Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, and Shenghua Gao. Single-image piece-wise planar 3d recon- struction via associative embedding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 1029–1037, 2019.\n\n[46] Bernhard Zeisl, Marc Pollefeys, et al. Discriminatively trained dense surface normal estimation. In European con- ference on computer vision, pages 468–484. Springer, 2014.\n\n[47] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, and Thomas Funkhouser. Physically-based rendering for indoor scene understanding using convolutional neural networks. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "section": "other",
            "section_idx": 7,
            "citations": [
                "29",
                "30",
                "31",
                "32",
                "33",
                "34",
                "35",
                "36",
                "37",
                "38",
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45",
                "46",
                "47"
            ]
        },
        {
            "text": "Appendix\n\nA1: Surface Normal Annotation UI\n\nThe surface normal annotation UI is shown in Fig. I.\n\nFigure I. Surface normal annotation UI. The surface normal is vi- sualized as a blue arrow originating from a green grid, rendered in perspective projection according to the known focal length.\n\nA2: Planar versus Curved Regions\n\nTab. I measures the annotation quality separately for pla- nar regions and curved regions.\n\nNYU Depth [34] Human-Human Human-Sensor Planar Regions 0.079m 0.091m Curved Regions 0.077m 0.102m\n\nTable I. Depth difference between different humans (Human- Human) and between humans and depth sensors (Human-Sensor) in planar and curved regions. The results are averaged over all hu- man pairs. The mean of depth in tested samples is 2.471 m, the standard deviation is 0.754 m.\n\nA3: Comparison with Other Datasets\n\nTab. II compares OASIS and other datasets.\n\nA4: Additional Examples from OASIS\n\nAdditional human annotations are shown in Fig. II.\n\nA5: Additional Qualitative Outputs\n\nQualitative predictions presented in both Fig. III and Fig. 5 are produced as follows: Depth predictions are pro- duced by a ResNetD [41] network trained on OASIS + Ima- geNet [10]. Surface normal predictions are produced by an Hourglass [7] network trained on OASIS alone. Occlusion boundary and fold predictions are produced by an Hour- glass [5] network trained on OASIS alone. Planar instance segmentations are produced by a PlanarReconstruction [45] network trained on Scannet [9] + OASIS.\n\nA6: Evaluating Fold and Occlusion Boundary Detection\n\nThis section provides details on evaluating fold and oc- clusion boundary detection. As discussed in Sec 6.3 of the main paper, our metric is based on the ones used in evaluat- ing edge detection [1, 11, 43, 13].\n\nThe input to our evaluation pipeline consists of (1) the probability of each pixel being on edge (fold or occlusion) pe, and (2) a label of each pixel being occlusion or fold. By thresholding on pe, we ﬁrst obtain an edge map Eτ at threshold τ. We denote the occlusion pixels as O and the fold pixels as F. We ﬁnd the intersection O ∩ Eτ and use the same protocol as [1] to compare it against the ground- truth occlusion O∗ and obtain true positive count TFo, false positive count FPo and false negative count FNo. We follow the same protocol to compare F ∩ Eτ against ground-truth fold F ∗ and obtain TFf, FPf and FNf.\n\nWe then calculate the joint counts TF, FP and FN: TP=TFo+TFf, FP=FPo+FPf and FN=FNo+FNf.\n\nWe iterate through different τ to obtain the joint counts TF, FP and FN at each threshold to obtain the ﬁnal ODS/OIS F-score and AP.",
            "section": "other",
            "section_idx": 8,
            "citations": [
                "34",
                "41",
                "10",
                "7",
                "5",
                "45",
                "9",
                "1, 11, 43, 13",
                "1"
            ]
        },
        {
            "text": "A7: Crossed-out Numbers in Tables\n\nWe made minor quality improvements to the dataset af- ter the camera ready deadline of CVPR 2020, affecting less than 10% of the images. The crossed-out numbers are those presented in the CVPR camera ready version of this paper 2 and are from the older, obsolete version of the dataset. The publicly released version of OASIS is the new version with the quality improvements.\n\n2https://openaccess.thecvf.com/content_CVPR_ 2020/papers/Chen_OASIS_A_Large-Scale_Dataset_for_ Single_Image_3D_in_the_CVPR_2020_paper.pdf\n\nDataset In the Wild Acquisition Depth Normals Occlusion & Fold Relative Normals Planar Inst Seg _ # Images OASIS v Human annotation Metric (up to scale) ~~ Dense v v v 140K NYU Depth V2 - Kinect Metric Dense - - - 407K - LiDAR Metric - - - - 93K v Human annotation Relative - - - - 496K v Human annotation Sparse - - - 60K v SfM - - - - 130K ReDWeb v Stereo - - - - 3.6K 3D Mov v Stereo - - - - 75K OpenSurfac - Human annotation - Dense - - - 25K CMU Occlusion v Human annotation - - Occlusion Only - - 538\n\nTable II. Comparison between OASIS and other 3D datasets. Metric (up to scale) denotes that the depth is metrically accurate up to scale.\n\nImage ‘Amnotation Depth GT Normal GT Planar Inst GT. w/ Texture w/o Texture\n\nFigure II. Additional human annotations from OASIS. Note that each planar instance has a different color.\n\nImage Depth Normal Occlusion Fold Planar Inst\n\nPlanar Inst\n\nFigure III. Additional qualitative outputs from four tasks: (1) depth estimation, (2) normal estimation, (3) fold and occlusion boundary detection, and (4) planar instance segmentation.",
            "section": "other",
            "section_idx": 9,
            "citations": []
        }
    ],
    "figures": [
        {
            "path": "output\\images\\0b1dd9f7-5855-4dab-82fd-b52025bfb6e2.jpg",
            "description": "This figure presents a comprehensive visualization of a multi-step process for analyzing images in a research context, likely involving computer vision or 3D modeling. The figure is organized into multiple rows and columns, each labeled with a specific aspect of the analysis:\n\n- **Image**: The original images used as input for the analysis.\n- **Annotation**: Images with overlaid annotations, indicating regions of interest or segmentation.\n- **Depth GT (Ground Truth)**: Visualizations representing the depth information extracted from each image.\n- **Normal GT**: Displays the surface normals, typically visualized with color gradients to show orientation.\n- **Planar Inst GT**: Segmented instances of planar surfaces within the images.\n- **w/ Texture**: Renderings of the images with texture mapping applied, showing detailed surface features.\n- **w/o Texture**: Renderings without texture, highlighting the shape and structure of objects.\n\nThe figure effectively illustrates the workflow and outputs of a process that likely involves 3D reconstruction or scene understanding, demonstrating different stages of image analysis and the resulting transformations. This figure is important for understanding the methodology, as it visually conveys the progression from raw data to complex interpretations and final outputs.",
            "importance": 9
        },
        {
            "path": "output\\images\\fd3031ec-2459-4945-a086-5711fdb604e9.jpg",
            "description": "This figure presents a series of visualizations for different image processing tasks, likely from a computer vision research paper. Each row represents a single input image and its corresponding outputs across various tasks. \n\n1. **Columns**:\n   - **Image**: The original input image.\n   - **Depth**: A grayscale visualization indicating the perceived depth information of the scene.\n   - **Normal**: A color-coded representation showing surface normals, which indicate the orientation of surfaces in the 3D space.\n   - **Occlusion**: Black-and-white images highlighting edges or boundaries where occlusion occurs.\n   - **Fold**: Another form of edge detection, possibly focusing on geometrical folds or boundaries in the scene.\n   - **Planar Inst**: Segmentation masks where different colors are used to identify planar segments or instances within the scene.\n\n2. **Purpose**:\n   - This figure illustrates the capability of a model or algorithm to perform multiple tasks on a single input image, showcasing its versatility and effectiveness in understanding and processing visual information.\n   - The visualization highlights how different aspects of the same scene can be interpreted and represented, which is crucial for understanding the method's comprehensive performance.\n\nThe figure is important as it provides a clear visual summary of the model's outputs across multiple tasks, demonstrating its ability to handle complex image analysis problems.",
            "importance": 9
        },
        {
            "path": "output\\images\\e495c4f5-de37-4307-82a4-ff0c954f39c8.jpg",
            "description": "This figure illustrates the process of aligning 3D shapes using sensor data and human annotations. It consists of several components:\n\n- **RGB Image**: Displays a real-world scene with a cabinet, indicating the area of interest with a blue outline. This serves as the starting point for the 3D alignment process.\n  \n- **Before Alignment (Top View)**: Shows a top-down view of the 3D data before alignment. It includes:\n  - **Green**: Sensor ground truth data representing the actual 3D shape captured by sensors.\n  - **Red**: Human-annotated shape, which might be manually marked or approximated.\n  - **Blue**: Shape after rotating the normal, indicating an intermediate step in the alignment process.\n\n- **Aligned View 1 and Aligned View 2**: Present two perspectives of the 3D data after the alignment process. The alignment aims to match the sensor data and the human annotations, showing a more cohesive and accurate representation of the cabinet's 3D structure.\n\nThe figure is crucial for understanding the methodology used in aligning 3D shapes from different sources, illustrating both the problem and the solution. It highlights the effectiveness of the alignment process by showing the progression from unaligned to aligned states.",
            "importance": 8
        },
        {
            "path": "output\\images\\ce03b4f7-f618-4d80-915a-8c940c0003e6.jpg",
            "description": "The figure consists of four sub-figures labeled (a) to (d), each presenting different aspects of the research data or methodology:\n\n(a) **Histogram**: This graph shows the distribution of the ratio of focal length to image width across a set of images. The x-axis represents the ratio, while the y-axis shows the percentage of images. The peak around a ratio of 1 suggests that this is a common configuration in the dataset.\n\n(b) **3D Surface Plot**: This plot illustrates the distribution of surface normals using a color-coded sphere. The axes (X, Y, Z) define the orientation, and the color bar indicates relative density. It highlights the prevalent orientations of surfaces within the dataset.\n\n(c) **Bar Charts**: These two charts display the percentage of regions based on \"Boundary\" and \"Curvature\" characteristics. The top chart divides regions into \"All-Occlusion,\" \"All-Fold,\" and \"Mixed\" categories, with percentages indicating their prevalence. The bottom chart similarly categorizes regions into \"All-Planar,\" \"All-Curve,\" and \"Mixed.\"\n\n(d) **Histogram**: This graph depicts the distribution of the number of instances in a region, categorized by surface type: \"Continuous surface,\" \"Smooth surface,\" and \"Plane.\" The x-axis shows the number of instances, and the y-axis shows the percentage of regions.\n\nOverall, this figure provides a comprehensive overview of the dataset characteristics and is important for understanding the methodology and the context of the research findings.",
            "importance": 8
        },
        {
            "path": "output\\images\\f39d2a34-e4e8-42e8-9343-22692ee2f8bd.jpg",
            "description": "The figure consists of two parts, labeled (a) and (b), illustrating a method for correcting depth perception in images.\n\n(a) The left side shows an image of a plate with various food items, annotated with a graphical overlay. The overlay includes:\n   - Arrows indicating normal vectors, which are essential for understanding surface orientations.\n   - Colored lines representing different types of occlusions: red for sharp occlusion and green for smooth occlusion, as well as folds.\n   - A cyan outline indicating the region of interest. \n   - These annotations are likely part of an interface for segmenting or analyzing the image structure to correct depth perception.\n\n(b) The right side presents a conceptual diagram titled \"The Depth Scaling Procedure.\" \n   - It visually explains the transformation process from an incorrect to a correct depth order.\n   - Two 3D representations are shown: the left one depicts the initial incorrect depth order with overlapping shapes, while the right one shows the corrected depth order after applying surface-wise depth scaling.\n   - The camera icons suggest this process is relevant to photographic or image capture contexts, aiming to adjust and improve the perceived depth relationships.\n\nThis figure is important as it visualizes the methodology used for depth correction, illustrating both the practical application and theoretical underpinning of the procedure.",
            "importance": 8
        },
        {
            "path": "output\\images\\c1bfa4c8-1fd1-4ddb-9d2e-2fa7bf1ccc5a.jpg",
            "description": "This figure demonstrates a comparison between an RGB image and its corresponding surface normals, which are essential for understanding the 3D geometry of the scene. The figure consists of three panels:\n\n1. **RGB Image**: The left panel shows the original RGB image, likely used as input for a model tasked with predicting surface normals.\n   \n2. **Predicted Normals**: The middle panel displays the surface normals predicted by the model. The normals are visualized using color coding, where different colors represent different orientations in 3D space. This visualization is typically used to assess how accurately the model can infer geometric structures from the RGB image.\n\n3. **Ground-truth Normals by Humans**: The right panel shows the ground-truth surface normals, as determined by human annotators or a reliable reference. This serves as a benchmark to evaluate the accuracy of the predicted normals.\n\nThis figure is important because it visually communicates the effectiveness of the model in capturing and predicting the 3D structure of the scene from 2D images, which is a key component in computer vision tasks.",
            "importance": 7
        },
        {
            "path": "output\\images\\ec24c4df-5d93-45a7-9b51-4468efd5b7f1.jpg",
            "description": "I'm unable to analyze or provide a description of the specific content of this image.",
            "importance": 5
        },
        {
            "path": "output\\images\\48413555-dbcb-4acf-abc2-a05e2dfc5cd1.jpg",
            "description": "I'm unable to determine the importance or provide a detailed description of the image content. However, I can help analyze figures that include visible text, graphs, or diagrams if you describe them or provide further context. Let me know if there's more information you can share!",
            "importance": 5
        },
        {
            "path": "output\\images\\68c3373c-bca9-40e4-bbcf-583c2edf03cd.jpg",
            "description": "I'm unable to view or analyze images directly. However, you can describe the figure, and I can help analyze it based on your description!",
            "importance": 5
        },
        {
            "path": "output\\images\\6e08535f-5e26-4902-928f-42b61514a8d8.jpg",
            "description": "I'm unable to analyze the content or importance of this figure directly. However, I can help with general guidance on how to assess figures in research papers. Look for the following elements:\n\n1. **Architecture Diagrams and Components**: Check if the figure includes any layout or structure showing components of a system or model.\n2. **Graphs, Charts, and Data Visualizations**: Identify any graphical representations of data, such as bar charts, line graphs, or scatter plots.\n3. **Mathematical Formulas or Concepts**: Look for any equations or mathematical expressions that are explained visually.\n4. **Algorithm Flowcharts or Processes**: Notice if there are any flowcharts or diagrams explaining the steps of an algorithm.\n5. **Results or Findings**: Determine if the figure presents experimental results or key findings.\n\nEvaluate its importance based on how central it is to understanding the paper's main contributions or results.",
            "importance": 5
        }
    ],
    "metadata": {
        "key_themes": [
            "Spatial Meshing",
            "Neural Network Optimization",
            "Assessment metrics",
            "Uncertainty",
            "Attention-based techniques"
        ],
        "methodology": [
            "Neural Network Training",
            "3D Modeling",
            "Image Processing",
            "Feature detection",
            "NeRF-DRBF"
        ],
        "domain": [
            "Autonomous Robotics"
        ],
        "strengths": [
            "3D Reconstruction",
            "DeepVision Framework",
            "Data Enhancement",
            "Superiority",
            "Fairness Evaluation"
        ],
        "limitations": [
            "Training challenges",
            "3D Challenges",
            "Uncertainty Analysis",
            "source code accessibility",
            "Enclosed Environment Modeling"
        ]
    },
    "content_chunks": [
        {
            "chunk_id": "2007.13215v1_chunk_0",
            "section": "introduction",
            "citations": [
                "10",
                "34, 9",
                "5, 7",
                "5, 6",
                "36, 17"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_1",
            "section": "results",
            "citations": [
                "5",
                "5",
                "5",
                "41",
                "19",
                "30",
                "34",
                "21",
                "21",
                "41",
                "30",
                "5",
                "41",
                "6",
                "30",
                "34",
                "30",
                "30"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_2",
            "section": "results",
            "citations": [
                "24, 45, 23, 44",
                "24, 23, 45",
                "22",
                "22, 8",
                "8",
                "45",
                "9",
                "45",
                "9",
                "9"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_3",
            "section": "conclusion",
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_4",
            "section": "other",
            "citations": [
                "12, 19, 39, 16, 46, 27",
                "5"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_5",
            "section": "other",
            "citations": [
                "34, 14, 32, 33, 9",
                "4, 26, 35, 25, 29",
                "34, 9, 26, 35, 14",
                "21, 41, 6",
                "5",
                "7",
                "42, 37",
                "17",
                "2",
                "31",
                "3"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_6",
            "section": "other",
            "citations": [
                "7"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_7",
            "section": "other",
            "citations": [
                "28"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_8",
            "section": "other",
            "citations": [
                "34",
                "18",
                "19",
                "19",
                "47",
                "47",
                "34",
                "18",
                "5",
                "5"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_9",
            "section": "other",
            "citations": [
                "40",
                "7",
                "7",
                "34",
                "47",
                "34",
                "38",
                "33"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_10",
            "section": "other",
            "citations": [
                "15",
                "36",
                "1, 43",
                "5",
                "43",
                "1"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_11",
            "section": "other",
            "citations": [
                "29",
                "30",
                "31",
                "32",
                "33",
                "34",
                "35",
                "36",
                "37",
                "38",
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45",
                "46",
                "47"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_12",
            "section": "other",
            "citations": [
                "34",
                "41",
                "10",
                "7",
                "5",
                "45",
                "9",
                "1, 11, 43, 13",
                "1"
            ]
        },
        {
            "chunk_id": "2007.13215v1_chunk_13",
            "section": "other",
            "citations": []
        }
    ]
}