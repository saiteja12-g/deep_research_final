{
    "basic_info": {
        "title": "Shape, Illumination, and Reflectance from Shading",
        "authors": [
            "Jonathan T. Barron",
            "Jitendra Malik"
        ],
        "paper_id": "2010.03592v1",
        "published_year": 2020,
        "references": []
    },
    "detailed_references": {
        "ref_1": {
            "text": "E. Adelson and A. Pentland, “The perception of shading and\nreﬂectance,” Perception as Bayesian inference , 1996.",
            "type": "numeric",
            "number": "1",
            "arxiv_id": null
        },
        "ref_2": {
            "text": "D. Field, “Relations between the statistics of natural images\nand the response properties of cortical cells,” JOSA A , 1987.",
            "type": "numeric",
            "number": "2",
            "arxiv_id": null
        },
        "ref_3": {
            "text": "D. Ruderman and W. Bialek, “Statistics of natural images:\nScaling in the woods,” Physical Review Letters , 1994.",
            "type": "numeric",
            "number": "3",
            "arxiv_id": null
        },
        "ref_4": {
            "text": "R. Grosse, M. K. Johnson, E. H. Adelson, and W. T. Freeman,\n“Ground-truth dataset and baseline evaluations for intrinsic\nimage algorithms,” ICCV , 2009.",
            "type": "numeric",
            "number": "4",
            "arxiv_id": null
        },
        "ref_5": {
            "text": "J. T. Barron and J. Malik, “High-frequency shape and albedo\nfrom shading using natural image statistics,” CVPR , 2011.",
            "type": "numeric",
            "number": "5",
            "arxiv_id": null
        },
        "ref_6": {
            "text": "——, “Shape, albedo, and illumination from a single image of\nan unknown object,” CVPR , 2012.",
            "type": "numeric",
            "number": "6",
            "arxiv_id": null
        },
        "ref_7": {
            "text": "——, “Color constancy, intrinsic images, and shape estima-\ntion,” ECCV , 2012.",
            "type": "numeric",
            "number": "7",
            "arxiv_id": null
        },
        "ref_8": {
            "text": "A. Gilchrist, Seeing in Black and White . Oxford University\nPress, 2006.",
            "type": "numeric",
            "number": "8",
            "arxiv_id": null
        },
        "ref_9": {
            "text": "E. H. Land and J. J. McCann, “Lightness and retinex theory,”\nJOSA , 1971.",
            "type": "numeric",
            "number": "9",
            "arxiv_id": null
        },
        "ref_10": {
            "text": "B. K. P . Horn, “Determining lightness from an image,” Com-\nputer Graphics and Image Processing , 1974.",
            "type": "numeric",
            "number": "10",
            "arxiv_id": null
        },
        "ref_11": {
            "text": "H. Barrow and J. Tenenbaum, “Recovering intrinsic scene\ncharacteristics from images,” Computer Vision Systems , 1978.",
            "type": "numeric",
            "number": "11",
            "arxiv_id": null
        },
        "ref_12": {
            "text": "M. Bell and W. T. Freeman, “Learning local evidence for\nshading and reﬂectance,” ICCV , 2001.",
            "type": "numeric",
            "number": "12",
            "arxiv_id": null
        },
        "ref_13": {
            "text": "J. Shen, X. Yang, Y. Jia, and X. Li, “Intrinsic images using\noptimization,” CVPR , 2011.",
            "type": "numeric",
            "number": "13",
            "arxiv_id": null
        },
        "ref_14": {
            "text": "M. F. Tappen, W. T. Freeman, and E. H. Adelson, “Recovering\nintrinsic images from a single image,” TP AMI , 2005.",
            "type": "numeric",
            "number": "14",
            "arxiv_id": null
        },
        "ref_15": {
            "text": "P . Gehler, C. Rother, M. Kiefel, L. Zhang, and B. Schoelkopf,\n“Recovering intrinsic images with a global sparsity prior on\nreﬂectance,” NIPS , 2011.",
            "type": "numeric",
            "number": "15",
            "arxiv_id": null
        },
        "ref_16": {
            "text": "D. A. Forsyth, “A novel algorithm for color constancy,” IJCV ,\n1990.",
            "type": "numeric",
            "number": "16",
            "arxiv_id": null
        },
        "ref_17": {
            "text": "L. T. Maloney and B. A. Wandell, “Color constancy: a method\nfor recovering surface spectral reﬂectance,” JOSA A , 1986.",
            "type": "numeric",
            "number": "17",
            "arxiv_id": null
        },
        "ref_18": {
            "text": "G. Klinker, S. Shafer, and T. Kanade, “A physical approach to\ncolor image understanding,” IJCV , 1990.",
            "type": "numeric",
            "number": "18",
            "arxiv_id": null
        },
        "ref_19": {
            "text": "G. Finlayson, S. Hordley, and P . Hubel, “Color by correlation:\na simple, unifying framework for color constancy,” TP AMI ,\n2001.",
            "type": "numeric",
            "number": "19",
            "arxiv_id": null
        },
        "ref_20": {
            "text": "D. H. Brainard and W. T. Freeman, “Bayesian color constancy,”\nJOSA A , 1997.",
            "type": "numeric",
            "number": "20",
            "arxiv_id": null
        },
        "ref_21": {
            "text": "B. K. P . Horn, “Obtaining shape from shading information,”\ninThe Psychology of Computer Vision , 1975.",
            "type": "numeric",
            "number": "21",
            "arxiv_id": null
        },
        "ref_22": {
            "text": "T. Rindﬂeisch, “Photometric method for lunar topography,”\nPhotogrammetric Engineering , 1966.",
            "type": "numeric",
            "number": "22",
            "arxiv_id": null
        },
        "ref_23": {
            "text": "M. J. Brooks and B. K. P . Horn, Shape from shading . MIT Press,\n1989.",
            "type": "numeric",
            "number": "23",
            "arxiv_id": null
        },
        "ref_24": {
            "text": "R. Zhang, P .-S. Tsai, J. E. Cryer, and M. Shah, “Shape-from-\nshading: a survey,” TP AMI , 2002.",
            "type": "numeric",
            "number": "24",
            "arxiv_id": null
        },
        "ref_25": {
            "text": "P . Belhumeur, D. Kriegman, and A. Yuille, “The Bas-Relief\nAmbiguity,” IJCV , 1999.",
            "type": "numeric",
            "number": "25",
            "arxiv_id": null
        },
        "ref_26": {
            "text": "J. Koenderink, A. van Doorn, C. Christou, and J. Lappin,\n“Shape constancy in pictorial relief,” Perception , 1996.",
            "type": "numeric",
            "number": "26",
            "arxiv_id": null
        },
        "ref_27": {
            "text": "K. Ikeuchi and B. Horn, “Numerical shape from shading and\noccluding boundaries,” Artiﬁcial Intelligence , 1981.",
            "type": "numeric",
            "number": "27",
            "arxiv_id": null
        },
        "ref_28": {
            "text": "H. Boyaci, K. Doerschner, J. L. Snyder, and L. T. Maloney,\n“Surface color perception in three-dimensional scenes,” Visual\nNeuroscience , 2006.",
            "type": "numeric",
            "number": "28",
            "arxiv_id": null
        },
        "ref_29": {
            "text": "R. Woodham, “Photometric method for determining surface\norientation from multiple images,” Optical engineering , 1980.",
            "type": "numeric",
            "number": "29",
            "arxiv_id": null
        },
        "ref_30": {
            "text": "R. Basri, D. Jacobs, and I. Kemelmacher, “Photometric stereo\nwith general, unknown lighting,” IJCV , 2007.",
            "type": "numeric",
            "number": "30",
            "arxiv_id": null
        },
        "ref_31": {
            "text": "R. Hartley and A. Zisserman, Multiple view geometry in com-\nputer vision . Cambridge University Press, 2003.",
            "type": "numeric",
            "number": "31",
            "arxiv_id": null
        },
        "ref_32": {
            "text": "B. Triggs, P . F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon,\n“Bundle adjustment - a modern synthesis,” ICCV , 1999.",
            "type": "numeric",
            "number": "32",
            "arxiv_id": null
        },
        "ref_33": {
            "text": "Y. Yu, P . Debevec, J. Malik, and T. Hawkins, “Inverse global\nillumination: recovering reﬂectance models of real scenes from\nphotographs,” SIGGRAPH , 1999.",
            "type": "numeric",
            "number": "33",
            "arxiv_id": null
        },
        "ref_34": {
            "text": "D. Hoiem, A. A. Efros, and M. Hebert, “Recovering surface\nlayout from an image,” IJCV , 2007.",
            "type": "numeric",
            "number": "34",
            "arxiv_id": null
        },
        "ref_35": {
            "text": "A. Saxena, M. Sun, and A. Ng, “Make3d: learning 3d scene\nstructure from a single still image,” TP AMI , 2008.",
            "type": "numeric",
            "number": "35",
            "arxiv_id": null
        },
        "ref_36": {
            "text": "V . Blanz and T. Vetter, “A morphable model for the synthesis\nof 3D faces,” SIGGRAPH , 1999.",
            "type": "numeric",
            "number": "36",
            "arxiv_id": null
        },
        "ref_37": {
            "text": "J. Huang and D. Mumford, “Statistics of natural images and\nmodels,” CVPR , 1999.",
            "type": "numeric",
            "number": "37",
            "arxiv_id": null
        },
        "ref_38": {
            "text": "J. Portilla, V . Strela, M. J. Wainwright, and E. P . Simoncelli,\n“Image denoising using scale mixtures of gaussians in the\nwavelet domain,” IEEE Trans. Image Process , 2003.",
            "type": "numeric",
            "number": "38",
            "arxiv_id": null
        },
        "ref_39": {
            "text": "S. Roth and M. J. Black, “Fields of experts: A framework for\nlearning image priors,” CVPR , 2005.",
            "type": "numeric",
            "number": "39",
            "arxiv_id": null
        },
        "ref_40": {
            "text": "R. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. Free-\nman, “Removing camera shake from a single photograph,”\nSIGGRAPH , 2006.",
            "type": "numeric",
            "number": "40",
            "arxiv_id": null
        },
        "ref_41": {
            "text": "J. Huang, A. B. Lee, and D. Mumford, “Statistics of range\nimages,” CVPR , 2000.",
            "type": "numeric",
            "number": "41",
            "arxiv_id": null
        },
        "ref_42": {
            "text": "F. Romeiro and T. Zickler, “Blind reﬂectometry,” ECCV , 2010.",
            "type": "numeric",
            "number": "42",
            "arxiv_id": null
        },
        "ref_43": {
            "text": "R. O. Dror, A. S. Willsky, and E. H. Adelson, “Statistical\ncharacterization of real-world illumination,” JOV, 2004.",
            "type": "numeric",
            "number": "43",
            "arxiv_id": null
        },
        "ref_44": {
            "text": "B. K. P . Horn, “Shape from shading: A method for obtaining\nthe shape of a smooth opaque object from one view,” MIT,\nTech. Rep., 1970.",
            "type": "numeric",
            "number": "44",
            "arxiv_id": null
        },
        "ref_45": {
            "text": "R. Ramamoorthi and P . Hanrahan, “An Efﬁcient Representa-\ntion for Irradiance Environment Maps,” CGIT , 2001.",
            "type": "numeric",
            "number": "45",
            "arxiv_id": null
        },
        "ref_46": {
            "text": "N. Alldrin, S. Mallick, and D. Kriegman, “Resolving the gener-\nalized bas-relief ambiguity by entropy minimization,” CVPR ,\n2007.",
            "type": "numeric",
            "number": "46",
            "arxiv_id": null
        },
        "ref_47": {
            "text": "G. D. Finlayson, M. S. Drew, and C. Lu, “Entropy minimization\nfor shadow removal,” IJCV , 2009.IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 19",
            "type": "numeric",
            "number": "47",
            "arxiv_id": null
        },
        "ref_48": {
            "text": "I. Omer and M. Werman, “Color lines: Image speciﬁc color\nrepresentation,” CVPR , 2004.",
            "type": "numeric",
            "number": "48",
            "arxiv_id": null
        },
        "ref_49": {
            "text": "J. C. Principe and D. Xu, “Learning from examples with\nquadratic mutual information,” Workshop on Neural Networks\nfor Signal Processing , 1998.",
            "type": "numeric",
            "number": "49",
            "arxiv_id": null
        },
        "ref_50": {
            "text": "J. Chen, S. Paris, and F. Durand, “Real-time edge-aware image\nprocessing with the bilateral grid,” SIGGRAPH , 2007.",
            "type": "numeric",
            "number": "50",
            "arxiv_id": null
        },
        "ref_51": {
            "text": "S. Bell, P . Upchurch, N. Snavely, and K. Bala, “Opensurfaces: A\nrichly annotated catalog of surface appearance,” SIGGRAPH ,\n2013.",
            "type": "numeric",
            "number": "51",
            "arxiv_id": null
        },
        "ref_52": {
            "text": "O. Woodford, P . Torr, I. Reid, and A. Fitzgibbon, “Global\nstereo reconstruction under second-order smoothness priors,”\nTP AMI , 2009.",
            "type": "numeric",
            "number": "52",
            "arxiv_id": null
        },
        "ref_53": {
            "text": "D. Hilbert and C. S. Vossen, Geometry and the Imagination .\nChelsea Publishing Company, 1956.",
            "type": "numeric",
            "number": "53",
            "arxiv_id": null
        },
        "ref_54": {
            "text": "H. P . Moreton and C. H. S ´equin, “Functional optimization for\nfair surface design.” in SIGGRAPH , 1992.",
            "type": "numeric",
            "number": "54",
            "arxiv_id": null
        },
        "ref_55": {
            "text": "P . Besl and R. Jain, “Segmentation through variable-order\nsurface ﬁtting,” TP AMI , 1988.",
            "type": "numeric",
            "number": "55",
            "arxiv_id": null
        },
        "ref_56": {
            "text": "D. A. Forsyth, “Variable-source shading analysis,” IJCV , 2011.",
            "type": "numeric",
            "number": "56",
            "arxiv_id": null
        },
        "ref_57": {
            "text": "J. Koenderink, “What does the occluding contour tell us about\nsolid shape?” Perception , 1984.",
            "type": "numeric",
            "number": "57",
            "arxiv_id": null
        },
        "ref_58": {
            "text": "P . Mamassian, D. Kersten, and D. C. Knill, “Categorical local-\nshape perception,” Perception , 1996.",
            "type": "numeric",
            "number": "58",
            "arxiv_id": null
        },
        "ref_59": {
            "text": "M. Brady and A. Yuille, “An extremum principle for shape\nfrom contour,” TP AMI , 1983.",
            "type": "numeric",
            "number": "59",
            "arxiv_id": null
        },
        "ref_60": {
            "text": "J. Malik, “Interpreting line drawings of curved objects,” IJCV ,\nvol. 1, 1987.",
            "type": "numeric",
            "number": "60",
            "arxiv_id": null
        },
        "ref_61": {
            "text": "D. Terzopoulos, “Image analysis using multigrid relaxation\nmethods,” TP AMI , 1986.",
            "type": "numeric",
            "number": "61",
            "arxiv_id": null
        },
        "ref_62": {
            "text": "M. K. Johnson and E. H. Adelson, “Shape estimation in natural\nillumination,” CVPR , 2011.",
            "type": "numeric",
            "number": "62",
            "arxiv_id": null
        },
        "ref_63": {
            "text": "D. Forsyth and A. Zisserman, “Reﬂections on shading,”\nTP AMI , 1991.",
            "type": "numeric",
            "number": "63",
            "arxiv_id": null
        },
        "ref_64": {
            "text": "J. T. Barron and J. Malik, “Intrinsic scene properties from a\nsingle rgb-d image,” CVPR , 2013.",
            "type": "numeric",
            "number": "64",
            "arxiv_id": null
        },
        "ref_65": {
            "text": "A. Adams, J. Baek, and M. A. Davis, “Fast high-dimensional\nﬁltering using the permutohedral lattice,” Eurographics , 2012.",
            "type": "numeric",
            "number": "65",
            "arxiv_id": null
        },
        "ref_66": {
            "text": "J. Koenderink, A. Van Doorn, C. Christou, and J. Lappin,\n“Perturbation study of shading in pictures,” Perception , 1996.",
            "type": "numeric",
            "number": "66",
            "arxiv_id": null
        },
        "ref_67": {
            "text": "A. Blake, A. Zisserman, and G. Knowles, “Surface descriptions\nfrom stereo and shading,” Image and Vision Computing , 1986.",
            "type": "numeric",
            "number": "67",
            "arxiv_id": null
        },
        "ref_Press_2006": {
            "text": "an unknown object,” CVPR , 2012. [7] ——, “Color constancy, intrinsic images, and shape estima- tion,” ECCV , 2012. [8] A. Gilchrist, Seeing in Black and White . Oxford University Press, 2006. [9] E. H. Land and J. J. McCann, “Lightness and retinex theory,” JOSA , 1971. [10] B. K. P . Horn, “Determining lightness from an image,” Com-",
            "type": "author_year",
            "key": "Press, 2006",
            "arxiv_id": null
        },
        "ref_Press_1989": {
            "text": "Photogrammetric Engineering , 1966. [23] M. J. Brooks and B. K. P . Horn, Shape from shading . MIT Press, 1989. [24] R. Zhang, P .-S. Tsai, J. E. Cryer, and M. Shah, “Shape-from-",
            "type": "author_year",
            "key": "Press, 1989",
            "arxiv_id": null
        },
        "ref_Press_2003": {
            "text": "puter vision . Cambridge University Press, 2003. [32] B. Triggs, P . F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon, “Bundle adjustment - a modern synthesis,” ICCV , 1999. [33] Y. Yu, P . Debevec, J. Malik, and T. Hawkins, “Inverse global",
            "type": "author_year",
            "key": "Press, 2003",
            "arxiv_id": null
        },
        "ref_Company_1956": {
            "text": "Chelsea Publishing Company, 1956. [54] H. P . Moreton and C. H. S ´equin, “Functional optimization for",
            "type": "author_year",
            "key": "Company, 1956",
            "arxiv_id": null
        }
    },
    "raw_chunks": [
        {
            "text": "Recent work has explored using learning to directly infer the spatial layout of a scene from a single image [34], [35]. These techniques ignore illumination and reﬂectance, and produce only a coarse estimate of shape.\n\nA similar approach to our technique is that of category-speciﬁc morphable models [36] which, given a single image of a very speciﬁc kind of object (a face, usually), estimates shape, reﬂectance, and illumina- tion. These techniques use extremely speciﬁc models (priors) of the objects being estimated, and therefore do not work for general objects, while our priors are\n\n3\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\ngeneral enough to be applicable on a wide variety of objects: a single model learned on teabags and squirrels can be applied to images of coffee cups and turtles.\n\nThe driving force behind our model are our priors on shape, reﬂectance, and illumination. To construct these priors we build upon past work on natural image statistics, which has demonstrated that sim- ple statistics govern local patches of natural images [2], [3], [37], and that these statistics can be used for denoising [38], inpainting [39], deblurring [40], etc. But these statistical regularities arise in natural images only because of the statistical regularities in the underlying worlds that produced those images. The primary contribution of this work is extended these ideas from natural images to the world that produced that natural image, which is assumed to be composed of natural depth maps and natural reﬂectance images. There has been some study of the statistics of natural depth maps [41], reﬂectance images [42] and models of illumination [43], but ours is the ﬁrst to use these statistical observations for recovering all such intrinsic scene properties simultaneously.\n\n3 PROBLEM FORMULATION\n\nWe call our problem formulation for recovering in- trinsic scene properties from a single image of a (masked) object “shape, illumination, and reﬂectance from shading”, or “SIRFS”. SIRFS can be thought of as an extension of classic shape-from-shading models [44] in which not only shape, but reﬂectance and illumination are unknown. Conversely, SIRFS can be framed as an “intrinsic image” technique for recov- ering shading and reﬂectance, in which shading is parametrized by a model of shape and illumination. The SIRFS problem formulation is:\n\nmaximize R,Z,L P(R)P(Z)P(L) subject to I = R + S(Z,L)\n\n(1)\n\nWhere R is a log-reﬂectance image, Z is a depth-map, and L is a spherical-harmonic model of illumination [45]. Z and R are “images” with the same dimensions as I, and L is a vector parametrizing the illumination. S(Z,L) is a “rendering engine” which linearizes Z into a set of surface normals, and produces a log- shading image from those surface normals and L (see Appendix A for a thorough explanation). P(R), P(Z), and P(L) are priors on reﬂectance, shape, and illu- mination, respectively, whose likelihoods we wish to maximize subject to the constraint that the log-image I is equal to a rendering of our model R+S(Z,L). We can simplify this problem formulation by reformulat- ing the maximum-likelihood aspect as minimizing a sum of cost functions (by taking the negative log of P(R)P(Z)P(L)) and by absorbing the constraint and\n\nremoving R as a free parameter. This gives us the following unconstrained optimization problem:\n\nminimize Z,L g(I − S(Z,L)) + f(Z) + h(L) (2)\n\nwhere g(R), f(Z), and h(L) (Sections 4, 5, and 6, respectively) are cost functions for reﬂectance, shape, and illumination respectively, which we will refer to as our “priors” on these scene properties 1. Solving this problem (Section 7) corresponds to searching for the least costly (or most likely) explanation {Z,R,L} for image I.",
            "section": "methodology",
            "section_idx": 0,
            "citations": [
                "34",
                "35",
                "36",
                "2",
                "3",
                "37",
                "38",
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45"
            ]
        },
        {
            "text": "proved performance in the natural illumination task\n\nmay be due to the fact that the images are pseudo-\n\nsynthethic (their shading images were produced using\n\nour spherical-harmonic rendering) and so they are\n\nLambertian and contain no cast shadows.\n\nIn Figure 17, we demonstrate a simple graphics application using the output of our model, for a color image under laboratory illumination. Given just\n\n15\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nSIRES SIR ‘si an _ - SSARFS_ SIR ioe SIRFS I SIRFS SIRI ames SIRFS aFS SIRES. gi\n\n(a) Input Image\n\n(b) Modified shape (C) Modified reflectance (d) Modified light — (€) Modified orientation\n\nFig. 17. Our system has obvious graphics applications. Given only a single image, we can estimate an object’s shape, reﬂectance, or illumination, modify any of those three scene properties (or simply rotate the object), and then re-render the object.\n\nthe output of our model from a single image, we can synthesize novel images in which the shape, reﬂectance, illumination, or orientation of the object has been changed. The output is not perfect — the absolute shape is often very incorrect, as we saw in Table 1, which is due to the inherent ambiguity and difﬁculty in estimating shape from shading. But such shape errors are usually only visible when rotating the object, and this inherent ambiguity in shape percep- tion often works in our favor when only manipulating reﬂectance, illumination, or ﬁne-scale shape — low- frequency errors in shape-estimation made by our model are often not noticed by human observers, because both the model and the human are bad at using shading to estimate coarse shape.\n\nages, and qualitatively correct illumination. Our re- covered shape and surface normals are often some- what wrong, as evidenced by the new synthesized views of each object, but our “relit” objects are often very compelling. The most common mistakes made in shading/reﬂectance estimation are usually due to our model assuming that the dominant color of the object is due to illumination, not reﬂectance (such as in the two pictures of faces) which we believe is due to biases in our training data towards white reﬂectances and colorful illumination.",
            "section": "results",
            "section_idx": 0,
            "citations": []
        },
        {
            "text": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nShape, Illumination, and Reﬂectance from Shading\n\nJonathan T. Barron, Member, IEEE, and Jitendra Malik, Fellow, IEEE\n\n0\n\nOct 2020\n\n2\n\n0\n\n2\n\nt\n\nc\n\nO\n\nAbstract—A fundamental problem in computer vision is that of inferring the intrinsic, 3D structure of the world from ﬂat, 2D images of that world. Traditional methods for recovering scene properties such as shape, reﬂectance, or illumination rely on multiple observations of the same scene to overconstrain the problem. Recovering these same properties from a single image seems almost impossible in comparison — there are an inﬁnite number of shapes, paint, and lights that exactly reproduce a single image. However, certain explanations are more likely than others: surfaces tend to be smooth, paint tends to be uniform, and illumination tends to be natural. We therefore pose this problem as one of statistical inference, and deﬁne an optimization problem that searches for the most likely explanation of a single image. Our technique can be viewed as a superset of several classic computer vision problems (shape-from-shading, intrinsic images, color constancy, illumination estimation, etc) and outperforms all previous solutions to those constituent problems.\n\nIndex Terms—Computer Vision, Machine Learning, Intrinsic Images, Shape from Shading, Color Constancy, Shape Estimation.\n\n7\n\n]\n\nYL, s.CV]\n\nV\n\nC\n\n1 INTRODUCTION\n\n.\n\ns\n\nc\n\nA T the core of computer vision is the problem of taking a single image, and estimating the physi- cal world which produced that image. The physics of image formation makes this “inverse optics” problem terribly challenging and underconstrained: the space of shapes, paint, and light that exactly reproduce an image is vast.\n\n[\n\n1\n\nv\n\n2\n\n9\n\n5\n\n3\n\noO\n\n0\n\nThis problem is perhaps best motivated using Adel- son and Pentland’s “workshop” metaphor [1]: con- sider the image in Figure 1(a), which has a clear percept as a twice-bent surface with a stroke of dark paint (Figure 1(b)). But this scene could have been created using any number of physical worlds — it could be realistic painting on a canvas (Figure 1(c)), a complicated arrangement of bent shapes (Figure 1(d)), a sophisticated projection produced by a collection of lights (Figure 1(e)), or anything in between. The job of a perceptual system is analogous to that of a prudent manager in this “workshop”, where we would like to reproduce the scene using as little effort from our three artists as possible, giving us Figure 1(b).\n\n.\n\n0\n\n1",
            "section": "discussion",
            "section_idx": 0,
            "citations": [
                "1"
            ]
        },
        {
            "text": "9 CONCLUSION\n\n8.1 Real-World Images\n\nThough our model quantitatively performs very well on the MIT-Berkeley Intrinsic Images dataset, this dataset is not very representative of the variety of natural objects in the world — materials are very Lam- bertian, many reﬂectances are very synthetic-looking, and illumination is not very varied. We therefore present an additional experiment in which we ran our model on arbitrary masked images of natural objects. We acquired many images (some with an iPhone camera, some with a DSLR, some downloaded from the internet), manually cropped the object in the photo, and used them as input to our model. In Figure 18 we visualize the output of our model: the recovered shape, normals, reﬂectance, shading, and illumination, a synthesized view of the object from a different angle, and a synthesized rendering of the object using a different (randomly generated) illumination. We did two experiments: one in which we used a grayscale version of the input image and our laboratory illumination model, and one in which we used the color input image and our natural illumi- nation model. We use the same code and hyperparam- eters for all images in the two constituent tasks, where our hyperparameters are identical to those used in the previous experiments with the MIT-Berkeley Intrinsic Images dataset.\n\nWe see that our model is often able to produce extremely compelling shading and reﬂectance im-\n\nWe have presented SIRFS, a model which takes as input a single (masked) image of an object, and pro- duces as output a reasonable estimate of the shape, surface normals, reﬂectance, shading, and illumina- tion which produced that image. At the core of SIRFS is a series of priors on shape, reﬂectance, and il- lumination: surfaces tend to be isotropic and bend infrequency, reﬂectance images tend to be piecewise smooth and low-entropy, and illumination tends to be natural. Given these priors and our multiscale optimization technique, we can infer the most-likely explanation of a single image subject to our priors and the constraint that the image be explained. Our uniﬁed approach to this problem outperforms all pre- vious solutions to its constituent problems of shape- from-shading and intrinsic image recovery on our challenging dataset, and produces reasonable results on arbitrary masked images of real-world objects in uncontrolled environments. This suggests that the shape-from-shading and intrinsic images problem for- mulations may be fundamentally limited, and atten- tion should be refocused towards developing models that jointly reason about shape and illumination in addition to shading and reﬂectance.\n\nBut of course, our model has some limitations. Because shading is an inherently poor cue for low- frequency shape estimation [25], [26] our model often makes mistakes in coarse shape estimation. To address this, we have presented a method for incorporating some external observation of shape, such as one from a stereo algorithm or a depth sensor, and we\n\n16\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nGrayscale Images, Laboratory Illumination Color Images, Natural Hlumination BGQUYBU-sy rerve @O@)o mae. ofa B08L- Gn 3 5 % 3% q 20 G&L A) D @® @ &- 14 Image | Shape Normals Refl. Shading Light] Rotated — Relit Image | Shape Normals Refl. Shading Light| Rotated Relit\n\nFig. 18. Our model produces reasonable results on real, manually cropped images of objects. Here are images of arbitrary objects in uncontrolled illumination environments which were downloaded or taken on consumer cameras. For each image, we have the output of our model, and two renderings of our recovered model: one in which we rotate the object, and one in which we relight the object. We run our algorithm on a grayscale version of the image (left), and on the original color image (right). For the color images, we use our “natural” illumination model. The code and parameters used for these images are the same as in all other experiments.\n\n17\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nhave demonstrated that by incorporating some low-\n\nfrequency external shape observation (such as what a stereo algorithm or a depth sensor would provide) we can produce high-quality shape estimates. We assume that materials are Lambertian, which is often a reasonable approximation but can causes problems for objects with specularities. Thankfully, because of the modular nature of our algorithm, our simple Lambertian rendering engine can easily be replaced by a more sophisticated model. We assume that im- ages consist of single, masked objects, while real- world natural scenes contain severe occlusion and support relationships. We also assume illumination is global, and we ignore illumination issues such as cast shadows, mutual illumination, or other sources of spatially-varying illumination [56], [63]. To address these two issues of occlusion and spatially-varying illumination in natural scenes, we have investigated into the interplay between SIRFS and segmentation techniques, by generalizing SIRFS to a mixture model of shapes and lights which are embedded in a soft segmentation of a scene [64]. Another limitation of our technique is that our priors on shape and reﬂectance are independent of the category of object present in the scene. We see this as a strength of our model, as it means that our priors are general enough to generalize across object categories, but presumably an extension of our model which uses object recognition techniques to produce class-speciﬁc priors should per- form better.",
            "section": "conclusion",
            "section_idx": 0,
            "citations": [
                "25",
                "26",
                "56",
                "63",
                "64"
            ]
        },
        {
            "text": "i)\n\n0\n\n2\n\n:\n\nv\n\ni\n\nX\n\nr\n\na\n\ntion models, very strong statistical regularities arise that are similar to those found in natural images [2], [3]. We will construct priors similar to those used in natural image statistics, but applied separately to shape, reﬂectance, and illumination. Our algorithm is simply an optimization problem in which we recover the most likely shape, reﬂectance, and illumination under these priors that exactly reproduces a single image. Our priors are powerful enough that these intrinsic scene properties can be recovered from a single image, but are general enough that they work across a variety of objects.\n\nThe output of our model relative to ground-truth\n\n(a) an image\n\n(b) a likely explanation\n\nThis metaphor motivates the formulation of this problem as one of statistical inference. Though there are inﬁnitely many possible explanations for a single image, some are more likely than others. Our goal is therefore to recover the most likely explanation that explains an input image. We will demonstrate that in natural depth maps, reﬂectance maps, and illumina-\n\nmas\n\n(c) painter’s explanation (d) sculptor’s explanation (e) gaffer’s explanation\n\n• J.T. Barron and J. Malik are with the Department of Electrical Engi- neering and Computer Science, University of California at Berkeley, Berkeley, CA 94720.\n\nE-mail: barron,malik@eecs.berkeley.edu\n\nFig. 1. A visualization of Adelson and Pentland’s “workshop” metaphor [1]. The image in 1(a) clearly corresponds to the interpretation in 1(b), but it could be a painting, a sculpture, or an arrangement of lights.\n\n1\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nGrayscale Image, Laboratory LL True SIRFS Retinex SIRFS+S Tappen True SIRFS Retinex SIRFS+S Gehler Q ° True A SIRFS fae Retinex SIRFS+S oN Gehler Image Shape Normals _ Refi. Shading Light\n\nFig. 2. A single image from our dataset, under three color/illumination conditions. For each condition, we present the ground-truth, the output of SIRFS, the output of SIRFS+S (which uses external shape infor- mation), and the two best-performing intrinsic image techniques (for which we do SFS on the recovered shading to recover shape).\n\ncan be seen in Figure 2. Our model is capable of pro- ducing qualitatively correct reconstructions of shape, surface normals, shading, reﬂectance, and illumina- tion, from a single image. We quantitatively evaluate our model on variants of the MIT intrinsic images dataset [4], on which we quantitatively outperform all previously published intrinsic image or shape-from- shading algorithms. We additionally present quali- tative results for many more real-world images, for which we do not have ground-truth explanations.\n\nEarlier versions of this work have been presented in a piecemeal fashion, over the course of many papers [5], [6], [7]. This paper is meant to simplify and unify those previous methods.\n\nThis paper will proceed as follows: In Section 2, we will review past work as it relates to our own. In Section 3 we will formulate our problem as one of statistical inference and optimization, with respect to a set of priors over shape, reﬂectance, and illumination. In Sections 4, 5, and 6 we present and motivate our priors on reﬂectance, shape, and illumination, respectively. In Section 7 we explain how we solve our proposed optimization problem. In Section 8 we present a series of experiments with our model on variants of the MIT Intrinsic Images dataset [4] and on real-world images, and in Section 9 we conclude.",
            "section": "other",
            "section_idx": 0,
            "citations": [
                "2",
                "3",
                "1",
                "4",
                "5",
                "6",
                "7",
                "4"
            ]
        },
        {
            "text": "2 PRIOR WORK\n\nThe question of how humans solve the undercon- strained problem of perceiving shape, reﬂectance, and illumination from a single image appears to be at least one thousand years old, dating back to the scientist Alhazen, who noted that ”Nothing of what is visible, apart from light and color, can be perceived by pure sensation, but only by discernment, infer- ence, and recognition, in addition to sensation.” In the 19th century the problem was studied by such prominent vision scientists as von Helmholtz, Hering and Mach [8], who framed the problem as one of “lightness constancy” — how humans, when viewing a ﬂat surface with patches of varying reﬂectances subject to spatially varying illumination, are able to form a reasonably veridical percept of the reﬂectance (“lightness”) in spite of the fact that a darker patch under brighter illumination may well have more light traveling from it to the eye compared to a lighter patch which is less well illuminated.\n\nLand’s Retinex theory of lightness constancy [9] has been particularly inﬂuential in computer vision since its introduction in 1971. It provided a computational approach to the problem in the “Mondrian World”, a 2D world of ﬂat patches of piecewise constant reﬂectance. Retinex theory was later made practical by Horn [10], who was able to obtain a decomposition of an image into its shading and reﬂectance components using the prior belief that sharp edges tend to be reﬂectance, and smooth variation tends to be shading.\n\n2\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nIn 1978, Barrow and Tenebaum deﬁned what they\n\ncalled the problem of “intrinsic images”: recovering properties such as shape, reﬂectance, and illumina- tion from a single image [11]. In doing so, they described a challenge in computer vision which is still largely unsolved, and which our work directly addresses. Because this problem is so fundamentally underconstrained and challenging, the computer vi- sion community has largely focused its attention on more constrained and tractable sub-problems. Over time, “intrinsic images” has become synonymous with the problem that Retinex addressed, that of separating an image into shading and reﬂectance components [4], [10], [9]. This area has seen seen some recent progress [12], [13], [14], [15], though the performance of Retinex, despite its age, has proven hard to im- prove upon [4]. The limiting factor in many of these “intrinsic image” algorithms appears to be that they treat “shading” as a kind of image, ignoring the fact that shading is, by construction, the product of some shape and some model of illumination. By addressing a superset of this “intrinsic image” problem and re- covering shape and illumination instead of shading, our model produces better results than any intrinsic image technique.\n\nRelated to the problem of lightness constancy or “intrinsic images” is the problem of color constancy, which can be thought of as a generalization of light- ness constancy from grayscale to color, in which the problem is simpliﬁed by assuming that there is just one single model of illumination for an entire image, rather than a spatially-varying “shading” ef- fect. Early techniques for color constancy used gamut mapping techniques [16], ﬁnite dimensional models of reﬂectance and illumination [17], and physically based techniques for exploiting specularities [18]. More recent work uses contemporary probabilistic tools, such as modeling the correlation between colors in a scene [19], or performing inference over pri- ors on reﬂectance and illumination [20]. All of this work shares the assumptions of “intrinsic image” algorithms that shape (and to a lesser extent, shading) can be ignored or abstracted away.\n\nThe second subset of the Barrow and Tenenbaum’s original “intrinsic image” formulation that the com- puter vision research community has focused on is the “shape-from-shading” (SFS) problem. SFS is tra- ditionally deﬁned as: recovering the shape of an object given a single image of it, assuming illumination and reﬂectance are known (or assuming reﬂectance is uniform across the entire image). This problem formulation is very complimentary to the shape-vs- reﬂectance version of the “intrinsic images” problem, as it focuses on the parts of the problem which “intrinsic images” ignores, and vice-versa.\n\nThe shape-from-shading problem was ﬁrst formu- lated in the computer vision community by Horn in 1975 [21], though the problem existed in other ﬁelds\n\nas that of “photoclinometry” [22]. The history of SFS is well surveyed in [23], [24]. Despite being a severe sim- pliﬁcation of the complete intrinsic images problem, SFS is still a very ill-posed and underconstrained, and challenging problem. One notable difﬁculty in SFS is the Bas-relief ambiguity [25], which states (roughly) that the absolute orientation and scaling of a surface is ambiguous given only shading information. This ambiguity holds true not only for SFS algorithms, but for human vision as well [26]. We address this ambiguity by imposing priors on shape, building on notions of “smoothness” priors in SFS [27], and by optionally allowing for external observations of shape (such as those produced by a stereo system or depth sensor) to be introduced.\n\nOur model can be viewed as a generalization of an “intrinsic image” algorithm or color constancy algorithm in which shading is explicitly parametrized as a function of shape and illumination. Similarly, our model can be viewed as a shape-from-shading algorithm in which reﬂectance and illumination are unknown, and are recovered. Our model therefore addresses the “complete” intrinsic images problem, as it was ﬁrst formulated. By addressing the complete problem, rather than two sub-problems in isolation, we outperform all previous algorithms for either sub- problem. This is consistent with our understanding of human perception, as humans use spatial cues when estimating reﬂectance and shading [8], [28].\n\nBecause the intrinsic images problem is so chal- lenging given only a single image, a much more popular area of research in computer vision has been to introduce additional data to better constrain the problem. Instances of this approach are photometric stereo [29], which use additional images with different illumination conditions to estimate shape, and in later work reﬂectance and illumination [30]. Our algorithm produces the same kinds of output as the most ad- vanced photometric stereo algorithm, while requir- ing only a single image. “Structure from motion” or binocular stereo [31], [32] uses multiple images to recover shape, but ignores shading, reﬂectance, and illumination. Inverse global illumination [33] recovers reﬂectance and illumination given shape and multiple images, while we recover shape and require only a single image.",
            "section": "other",
            "section_idx": 1,
            "citations": [
                "8",
                "9",
                "10",
                "11",
                "4",
                "10",
                "9",
                "12",
                "13",
                "14",
                "15",
                "4",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "8",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33"
            ]
        },
        {
            "text": "4 PRIORS ON REFLECTANCE\n\nOur prior on reﬂectance consists of three components: 1) An assumption of piecewise constancy, which we will model by minimizing the local variation of log- reﬂectance in a heavy-tailed fashion. 2) An assump- tion of parsimony of reﬂectance — that the palette of colors with which an entire image was painted tends to be small — which we model by minimizing the global entropy of log-reﬂectance. 3) An “absolute” prior on reﬂectance which prefers to paint the scene with some colors (white, gray, green, brown, etc) over others (absolute black, neon pink, etc), thereby addressing color constancy. Formally, our reﬂectance prior g(A) is a weighted combination of three costs:\n\ng(R) = λsgs(R) + λege(R) + λaga(R) (3)\n\nwhere gs(R) is our smoothness prior, ge(R) is our parsimony prior, and ga(R) is our “absolute” prior. The λ multipliers are learned through cross-validation on the training set.\n\nOur smoothness and parsimony priors are on the differences of log-reﬂectance, which makes them equivalent to priors on the ratios of reﬂectance. This makes intuitive sense, as reﬂectance is deﬁned as a ratio of reﬂected light to incident light, but is also crucial to the success of our algorithm: Consider the reﬂectance-map ρ implied by log-image I and log- shading S(Z,L), such that ρ = exp(I −S(Z,L)). If we were to manipulate Z or L to increase S(Z,L) by some constant α across the entire image, then ρ would be di- vided by exp(α) across the entire image, which would accordingly decrease the differences between pixels of ρ. Therefore, if we placed priors on the differences of reﬂectance it would be possible to trivially satisfy our priors by manipulating shape or illumination to increase the intensity of the shading image. However, in the log-reﬂectance case R = I −S(Z,L), increasing\n\n1. Throughout this paper we use the term “prior” loosely. We refer to loss functions or regularizers on Z, A, and L as “pri- ors” because they often have an interpretation as the negative log-likelihood of some probability density function. We refer to minimizing entropy as a “prior”, which is again an abuse of terminology. Our occluding contour “prior” and our external obser- vation “prior” require ﬁrst observing the silhouette of an object or some external observation of shape, respectively, and are therefore posteriors, not priors.\n\n4\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nLog-Likelihood VR\n\n2 vee 4g 8 0 VR",
            "section": "other",
            "section_idx": 2,
            "citations": []
        },
        {
            "text": "(a) univariate/grayscale GSM\n\n(b) multivariate/color GSM\n\nFig. 3. Our smoothness prior on log-reﬂectance is a univariate Gaussian scale mixture on the differences between nearby reﬂectance pixels for grayscale im- ages, or a multivariate GSM for color images. These distribution prefers nearby reﬂectance pixels to be similar, but its heavy tails allow for rare non-smooth discontinuities. Our multivariate color model captures the correlation between color channels, which means that chromatic variation in log-reﬂectance lies further out in the tails, making it more likely to be ignored during inference.\n\nall of S by α (increasing the brightness of the shading image) simply decreases all of R by α, and does not change the differences between log-reﬂectance values (it would, however, affect our absolute prior on reﬂectance). Priors on the differences of log-albedo are therefore invariant to scaling of illumination or shading, which means they behave similarly in well- lit regions as in shadowed regions, and cannot be trivially satisﬁed.\n\n4.1 Smoothness\n\nThe reﬂectance images of natural objects tend to be piecewise constant — or equivalently, variation in reﬂectance images tends to be small and sparse. This is the insight that underlies the Retinex algorithm [4], [9], [10], and informs more recent intrinsic images work [13], [14], [15].\n\nOur prior on grayscale reﬂectance smoothness is a multivariate Gaussian scale mixture (GSM) placed on the differences between each reﬂectance pixel and its neighbors. We will maximize the likelihood of R under this model, which corresponds to minimizing the following cost function:\n\n9(R) => SO c(Ri-Rjsar,en) 4) i jEN(i)\n\nWhere N(i) is the 5×5 neighborhood around pixel i, Ri − Rj is a the difference in log-RGB from pixel i to pixel j, and c(·;α, σ) is the negative log-likelihood of a discrete univariate Gaussian scale mixture (GSM), parametrized by α and σ, the mixing coefﬁcients and standard deviations, respectively, of the Gaussians in\n\n(a) some R\n\n(b) 9: (R) (cost)",
            "section": "other",
            "section_idx": 3,
            "citations": [
                "4",
                "9",
                "10",
                "13",
                "14",
                "15"
            ]
        },
        {
            "text": "(C) Vgs(R) (influence)\n\nFig. 4. Here we have a color reﬂectance image R, and its cost and inﬂuence (derivative of cost) under our multivariate GSM smoothness prior. Strong, colorful edges, such as those caused by reﬂectance variation, are very costly, while small edges, such as those caused by shading, are less costly. But in terms of inﬂuence — the gradient of cost with respect to each pixel — we see an inversion: because sharp edges lie in the tails of the GSM, they have little inﬂuence, while shading variation has great inﬂuence. This means that during inference our model attempts to explain shading (small, achromatic variation) in the image by varying shape, while explaining sharp or chromatic variation by varying reﬂectance.\n\nthe mixture:\n\nM c(x;a,0) = —log So ajN (x;0, g;) (5) j=l\n\nWe set the mean of the GSM is 0, as the most likely reﬂectance image under our model should be ﬂat. We set M = 40 (the GSM has 40 discrete Gaussians), and αR and σR are trained on reﬂectance images in our training set using expectation-maximization. The log-likelihood of our learned model can be seen in Figure 3(a).\n\nGaussian scale mixtures have been used previously to model the heavy-tailed distributions found in nat- ural images [38], for the purpose of denoising or in- painting. Effectively, using this family of distributions gives us a log-likelihood which looks like a smooth, heavy-tailed spline which decreases monotonically with distance from 0. Because it is monotonically de- creasing, the cost of log-reﬂectance variation increases with the magnitude of variation, but because the distribution is heavy tailed, the inﬂuence of variation (the derivative of log-likelihood) is strongest when variation is small (that is, when variation resembles shading) and weaker when variation is large. This means that our model prefers a reﬂectance image that is mostly ﬂat but occasionally varies heavily, but abhors a reﬂectance image which is constantly varying slightly. This behavior is similar to that of the Retinex algorithm, which operates by shifting strong gradients to the reﬂectance image and weak gradients to the shading image.\n\nTo extend our model to color images, we simply ex- tend our smoothness prior to a multivariate Gaussian\n\n5\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nscale mixture\n\ngs(R) = C (Ri − Rj ;αR, σR,ΣR) (6) i j∈N(i)\n\nWhere Ri−Rj is now a 3-vector of the log-RGB differ- ences, α are mixing coefﬁcients, σ are the scalings of the Gaussians in the mixture, and Σ is the covariance matrix of the entire GSM (shared among all Gaussians of the mixture).\n\nC(x;a,0,X) = x; 0,0; D) (7) M —log > a,j N ( j=l\n\nWe set M = 40 (the GSM has 40 discrete Gaussians), and we train αR, σR, and ΣR on color reﬂectance im- ages in our training set (we train a distinct model from the grayscale smoothness model). The log-likelihood of our learned model, and the training data used to learn that model, can be seen in Figure 3(b).\n\nIn color images, variation in reﬂectance tends to manifest itself in both the luminance and chrominance of an image (white transitioning to blue, for example) while shading, assuming the illumination is mostly white, primarily affects the luminance of an image (light blue transitioning to dark blue, for example). Past work has exploited this insight by building spe- cialized models that condition on the chrominance variation of the input image [4], [10], [13], [14], [15]. By placing a multivariate prior over differences in reﬂectance, we are able to capture the correlation of the different color channels, which implicitly encour- ages our model to explain chromatic variation using reﬂectance and achromatic variation using shading without the need for any hand-crafted heuristics. See Figure 4 for a demonstration of this effect. Our model places more-colorful edges further into the tails of the distribution, thereby reducing their inﬂuence. Again, this is similar to color variants of the Retinex algorithm [4] which uses the increased chrominance of an edge as a heuristic for it being a reﬂectance edge. But this approach (which is common among intrinsic image algorithms) of using image chrominance as a substitute for reﬂectance chrominance means that these techniques fail when faced with non-white il- lumination, while our model is robust to non-white",
            "section": "other",
            "section_idx": 4,
            "citations": [
                "38",
                "4",
                "10",
                "13",
                "14",
                "15",
                "4"
            ]
        },
        {
            "text": "illumination.\n\n4.2 Parsimony\n\nIn addition to piece-wise smoothness, the second property we expect from reﬂectance images is for there to be a small number of reﬂectances in an image — that the palette with which an image was painted be small. As a hard constraint, this is not true: even in painted objects, there are small variations in reﬂectance. But as a soft constraint, this assumption holds. In Figure 5 we show the marginal distribution of grayscale log-reﬂectance for three objects in our dataset. Though the man-made ”cup1” object shows\n\naus\n\napple 3 2 a4 0\n\nsquirrel 3 2 a4 0\n\ncupt 3 2 a4 0\n\nFig. 5. Three grayscale log-reﬂectance images from our dataset and their marginal distributions. Log- reﬂectance in an image tend to be grouped around certain values, or equivalently, these distributions tend to be low-entropy.\n\nthe most clear peakedness in its distribution, natural objects like ”apple” show signiﬁcant clustering.\n\nWe will therefore construct a prior which encour- ages parsimony – that our representation of the re- ﬂectance of the scene be economical and efﬁcient, or “sparse”. This is effectively a instance of Occam’s razor, that one should favor the simplest possible ex- planation. We are not the ﬁrst to explore global parsi- mony priors on reﬂectance: different forms of this idea have been used in intrinsic images techniques [15], photometric stereo [46], shadow removal [47], and color representation [48]. We use the quadratic en- tropy formulation of [49] to minimize the entropy of log-reﬂectance, thereby encouraging parsimony. For- mally, our parsimony prior for reﬂectance is:\n\nN WN _—R 2 ge(R) = —log| 5 Ze (-& To J na) Z = N?V4ro? (8)\n\nThis is quadratic entropy (a special case of R´enyi entropy) for a set of points x assuming a Parzen window (a Gaussian kernel density estimator, with a bandwidth of σR) [49]. Effectively, this is a “soft” and differentiable generalization of Shannon entropy, computed on a set of real values rather than a discrete histogram. By minimizing this quantity, we encourage all pairs of reﬂectance pixels in the image to be similar to each other. However, minimizing this entropy does not force all pixels to collapse to one value, as the “force” exerted by each pair falls off exponentially with distance — it is robust to outliers. This prior ef- fectively encourages Gaussian “clumps” of reﬂectance values, where the Gaussian clumps have standard deviations of roughly σR.\n\nAt ﬁrst glance, it may seem that this global parsi- mony prior is redundant with our local smoothness prior: Encouraging piecewise smoothness seems like it\n\n6\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE",
            "section": "other",
            "section_idx": 5,
            "citations": [
                "15",
                "46",
                "47",
                "48",
                "49",
                "49"
            ]
        },
        {
            "text": "(a) No parsimony\n\n(b) No smoothness\n\n(c) Both\n\nFig. 6. A demonstration of the importance of both our smoothness and parsimony priors on reﬂectance. Using only a smoothness prior, as in 6(a), allows for reﬂectance variation across disconnected regions. Using only the parsimony prior, as in 6(b), encourages reﬂectance to take on a small number of values, but does not encourage it to form large piecewise-constant regions. Only by using the two priors in conjunction, as in 6(c), does our model correctly favor a normal, paint- like checkerboard conﬁguration.\n\nshould cause entropy to be minimized indirectly. This is often true, but there are common situations in which both of these priors are necessary. For example, if two regions are separated by a discontinuity in the image then optimizing for local smoothness will never cause the reﬂectance on both sides of the discontinuity to be similar. Conversely, simply minimizing global entropy may force reﬂectance to take on a small number of values, but need not produce large piecewise-smooth regions. The merit of using both priors in conjunction is demonstrated in Figure 6.\n\nGeneralizing our grayscale parsimony prior to color reﬂectance images requires generalizing our entropy model to higher dimensionalities. A naive extension of this one-dimensional entropy model to three di- mensions is not sufﬁcient for our purposes: The RGB channels of natural reﬂectance images are highly cor- related, causing a naive “isotropic” high-dimensional entropy measure to work poorly. To address this, we pre-compute a whitening transformation from log- reﬂectance images in the training set, and compute an isotropic entropy measure in this whitened space dur- ing inference, which gives us an anisotropic entropy measure. Formally, our cost function is quadratic en- tropy in the space of whitened log-reﬂectance:\n\nN ON , 2 1y Wr(Ri — Rj ge(R)= — log Z y y exp (-Ra ol nh ale) R i=1 j=1 (9)\n\n(9)\n\nWhere WR is the whitening transformation learned from reﬂectance images in our training set, as fol- lows: Let X be a 3 × n matrix of the pixels in the reﬂectance images in our training set. We compute the matrix Σ = XXT, take its eigenvalue decomposition\n\nΣ = ΦΛΦT, and from that construct the whitening2 transformation WR = ΦΛ1/2ΦT. The bandwidth of the Parzen window is σR, which determines the scale of the clusters produced by minimizing this entropy measure, and is tuned through cross-validation (inde- pendently of the same variable for the grayscale case). See Figure 7 for a motivation of this model.\n\nNaively computing this quadratic entropy measure requires calculating the difference between all N log- reﬂectance values in the image with all other N log- reﬂectance values, making it quadratically expensive in N to compute naively. In Appendix B we describe an accurate linear-time algorithm for approximating this quadratic entropy and its gradient, based on the bilateral grid [50].",
            "section": "other",
            "section_idx": 6,
            "citations": [
                "50"
            ]
        },
        {
            "text": "4.3 Absolute Reflectance\n\nThe previously described priors were imposed on relative properties of reﬂectance: the differences be- tween nearby or not-nearby pixels. We must impose an additional prior on absolute reﬂectance: the raw value of each pixel in the reﬂectance image. Without such a prior (and the prior on illumination presented in Section 6) our model would be equally pleased to explain a gray pixel in the image as gray reﬂectance under gray illumination as it would nearly-black re- ﬂectance under extremely-bright illumination, or blue reﬂectance under yellow illumination, etc.\n\nThis sort of prior is fundamental to color- constancy, as most basic white-balance or auto- contrast/brightness algorithms can be viewed as min- imizing a similar sort of cost: the gray-world as- sumption penalizes reﬂectance for being non-gray,\n\n2. Our whitening transformation of reﬂectance is not strictly correct, as we do not ﬁrst center the data by subtracting the mean. This was done both for mathematical and computational convenience, and because the origin of the space of log-reﬂectance (absolute white) is arguable the most reasonable choice for the “center” of our data.\n\n1 weep pO\n\n0 2\n\nRep pio\n\n(a) Correct\n\n(b) wrong Shape\n\n(C) Wrong Light\n\nFig. 7. Some reﬂectance images and their cor- responding log-RGB scatterplots. Mistakes in esti- mating shape or illumination produce shading-like or illumination-like errors in the inferred reﬂectance, caus- ing the log-RGB distribution of the reﬂectance to be “smeared”, and causing entropy (and therefore cost) to increase.\n\n7\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nData Model 4 3 2 a ° R\n\nRandom Samples 0.00 070 149 182 209 Cost\n\n(a) Training data and PDF\n\n(b) Samples\n\nFig. 8. A visualization of our “absolute” prior on grayscale reﬂectance, trained on the MIT Intrinsic Im- ages dataset [4]. In 8(a) we have the log-likelihood of our density model, and the data on which it was trained. In 8(b) we have samples from our model, where the x axis is sorted by cost (y axis is random).\n\nthe white-world assumption penalizes reﬂectance for being non-white, and gamut-based models penalize reﬂectance for lying outside of a gamut of previously- seen reﬂectances. We experimented with variations or combinations of these types of models, but found that what worked best was using a regularized smooth spline to model the log-likelihood of log-reﬂectance values.\n\nFor grayscale images, we use a 1D spline, which we have ﬁt to log-reﬂectance images in the training set as follows:\n\nminimize fln+ log (= exp (6) + Ay/ (£\")2 + 2 (10)\n\nWhere f is our spline, which determines the non- normalized negative log-likelihood (cost) assigned to every reflectance, n is a 1D histogram of log- reflectance in our training data, and f is the second derivative of the spline, which we robustly penalize (€ is a small value added in to make our regularization differentiable everywhere). Minimizing the sum of the first two terms is equivalent to maximizing the likeli- hood of the training data (the second term is the log of the partition function for our density estimation), and minimizing the third term causes the spline to be piece-wise smooth. The smoothness multiplier is tuned through cross-validation. A visualization of our prior can be found in Figure\n\nDuring inference, we maximize the likelihood of the grayscale reﬂectance image R by minimizing its cost under our learned model:\n\nga(R) = f(Ri) (11) i\n\nwhere f(Ri) is the value of f at Ri, the log-reﬂectance at pixel i, which we computed using linear interpola- tion (so that this cost is differentiable).\n\nTo generalize this model to color reﬂectance images,\n\nwe simply use a 3D spline, trained on whitened log- reﬂectance pixels in our training set. Formally, to train our model we minimize the following:\n\nminimize <F,N> +log (= exp -r9) +AV/J(F) + i\n\nJ(F) = F2 xx + F2 yy + F2 zz + 2F2 xy + 2F2 yz + 2F2 xz (12)\n\nWhere F is our 3D spline describing cost, N is a 3D histogram of the whitened log-RGB reﬂectance in our training data, and J(·) is a smoothness penalty (the thin-plate spline smoothness energy, made more robust by taking its square root). The smoothness multiplier λ is tuned through cross-validation. As in our parsimony prior, we use whitened log-reﬂectance to address the correlation between channels, which is necessary as our smoothness term is isotropic. A visualization of our prior can be seen in Figure 9.\n\nDuring inference, we maximize the likelihood of the color reﬂectance image R by minimizing its cost under our learned model:\n\nga(R) = F(WRRi) (13) i\n\nwhere F(WRRi) is the value of F at the coordi- nates speciﬁed by the 3-vector WRi, the whitened reﬂectance at pixel i (WR is the same as in Section 4.2). To make this function differentiable, we compute F(·) using trilinear interpolation.\n\nWe trained our absolute color prior on the MIT Intrinsic Images dataset [4], and used that learned model in all experiments shown in this paper. How- ever, the MIT dataset is very small and this absolute prior contains very many parameters (hundreds, in contrast to our other priors which are signiﬁcantly more constrained), which suggests that we may be overﬁtting to the small set of reﬂectances in the MIT dataset. To address this concern, we trained an additional version of our absolute prior on the color reﬂectances in the OpenSurfaces dataset [51], which is a huge and varied dataset that is presum- ably a more accurate representation of real-world reﬂectances. Both models can be seen in Figure 9, where we see that the priors we learn for each dataset are somewhat different, but that both prefer lighter, desaturated reﬂectances. We ran some additional ex- periments using our OpenSurfaces model instead of our MIT model (not presented in this paper), and found that the outputs of each model were virtually indistinguishable. This is a testament to the robustness of our model, and suggests that we are not overﬁtting",
            "section": "other",
            "section_idx": 7,
            "citations": [
                "4",
                "4",
                "51"
            ]
        },
        {
            "text": "to the color reﬂectances in the MIT dataset.\n\n5 PRIORS ON SHAPE\n\nOur prior on shape consists of three components: 1) An assumption of smoothness (that shapes tend to bend rarely), which we will model by minimizing\n\n8\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\n2 —— Ree 4, 2 0\n\nRandom Samples 014 178 576 9.71 12.40 Cost\n\n(a) Training data\n\n(C) Samples\n\nR t) 24 9 6 6 = “Ane\n\nRandom Samples mal «OD 281 4.98 879 10.12 Cost\n\n(d) Training data\n\n(e) PDF\n\n(f) Samples\n\nFig. 9. A visualization of our “absolute” prior on color reﬂectance. We train two versions of our prior, one on the MIT Intrinsic Images dataset [4] that we use in our experiments (top row) and one on the OpenSurfaces dataset for comparison [51] (bottom row). In the ﬁrst- column we have the log-RGB reﬂectance pixels in our training set, and in the second column we have a visualization of the 3D spline PDF that we ﬁt to that data. In the third column we have samples from the PDF, where the x axis is sorted by cost (y axis is random). For both datasets, our model prefers less saturated, more earthy or subdued colors, and abhors brightly lit neon-like colors or very dark colors — the high-cost reﬂectances often do not even look like paint, but instead appear glowing and luminescent.\n\nthe variation of mean curvature. 2) An assumption of isotropy of the orientation of surface normals (that shapes are just as likely to face in one direction as they are another) which reduces to a well-motivated “fronto-parallel” prior on shapes. 3) An prior on the orientation of the surface normal near the boundary of masked objects, as shapes tend to face outward at the occluding contour. Formally, our shape prior f(Z) is a weighted combination of four costs:\n\nf(Z) = λkfk(Z) + λifi(Z) + λcfc(Z) (14)\n\nwhere fk(Z) is our smoothness prior, fi(Z) is our isotropy prior, and fc is our bounding contour prior. The λ multipliers are learned through cross-validation on the training set.\n\nMost of our shape priors are imposed on intermedi- ate representations of shape, such as mean curvature or surface normals. This requires that we compute these intermediate representations from a depth map, calculate the cost and the gradient of cost with respect to those intermediate representations, and backprop- agate the gradients back onto the shape. In the ap- pendix we explain in detail how to efﬁciently compute these quantities and backpropagate through them.",
            "section": "other",
            "section_idx": 8,
            "citations": [
                "4",
                "51"
            ]
        },
        {
            "text": "(a) some shape Z\n\n(b) mean curvature H(Z)\n\nFig. 10. A visualization of a shape and its mean curvature (blue = positive, red = negative, white = 0). Planes and soap ﬁlms have 0 mean curvature, spheres and cylinders have constant mean curvature, and mean curvature varies where shapes bend.\n\n5.1 Smoothness\n\nThere has been much work on modeling the statistics of natural shapes [41], [52], with one overarching theme being that regularizing some function of the second derivatives of a surface is effective. However, this past work has severe issues with invariance to out-of-plane rotation and scale. Working within dif- ferential geometry, we present a shape prior based on the variation of mean curvature, which allows us to place smoothness priors on Z that are invariant to rotation and scale.\n\nTo review: mean curvature is the divergence of the normal ﬁeld. Planes and soap ﬁlms have 0 mean curvature everywhere, spheres and cylinders have constant mean curvature everywhere, and the sphere has the smallest total mean curvature among all con- vex solids with a given surface area [53]. See Figure 10 for a visualization. Mean curvature is a measure of curvature in world coordinates, not image coordinates, so (ignoring occlusion) the marginal distribution of H(Z) is invariant to out-of-plane rotation of Z — a shape is just as likely viewed from one angle as from another. In comparison, the Laplacian of Z and the second partial derivatives of Z can be made large simply due to foreshortening, which means that priors placed on these quantities [52] would prefer certain shapes simply due to the angle from which those shapes are observed — clearly undesirable.\n\nBut priors on raw mean curvature are not scale- invariant. Were we to minimize |H(Z)|, then the most likely shape under our model would be a plane, while spheres would be unlikely. Were we to minimize |H(Z) − α| for some constant α, then the most likely shape under our model would be a sphere of a certain radius, but larger or smaller spheres, or a resized image of the same sphere, would be unlikely. Clearly, such scale sensitivity is an undesirable property for a general-purpose prior on natural shapes. Inspired by previous work on minimum variation surfaces [54], we place priors on the local variation of mean curvature. The most likely shapes under such priors are surfaces of constant mean curvature, which are\n\n9\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nwell-studied in geometry and include soap bubbles and spheres of any size (including planes). Priors on the variation of mean curvature, like priors on raw mean curvature, are invariant to rotation and viewpoint, as well as concave/convex inversion.\n\nMean curvature is deﬁned as the average of princi- ple curvatures: H = 1 2(κ1+κ2). It can be approximated on a surface using ﬁlter convolutions that approxi- mate ﬁrst and second partial derivatives, as shown in [55].\n\n(Lt 22) Zyy — 2Z0ZyZey + (1+ 25) Zow H(Z) 2(1+ 224 22)7” (15)\n\n(15)\n\nIn Appendix C we detail how to calculate and dif- ferentiate H(Z) efﬁciently. Our smoothness prior for shapes is a Gaussian scale mixture on the local varia- tion of the mean curvature of Z:\n\nfe(Z) = S35 SO c(A(Z)i — H(Z)j: 0%, o%) i jEN(i) (16)\n\nNotation is similar to Equation 4: N(i) is the 5 × 5 neighborhood around pixel i, H(Z) is the mean cur- vature of shape Z, and H(Z)i−H(Z)j is the difference between the mean curvature at pixel i and pixel j. c(·;α, σ) is deﬁned in Equation 5, and is the negative log-likelihood (cost) of a discrete univariate Gaussian scale mixture (GSM), parametrized by α and σ, the mixing coefﬁcients and standard deviations, respec- tively, of the Gaussians in the mixture. The mean of the GSM is 0, as the most likely shapes under our model should be smooth. We set M = 40 (the GSM has 40 discrete Gaussians), and αk and σk are learned from our training set using expectation-maximization. The log-likelihood of our learned model can be seen in Figure 11(a), and the likelihoods it assigns to different shapes can be visualized in Figure 11(b). The learned GSM is very heavy tailed, which encourages shapes to be mostly smooth, and occasionally very non-smooth — or equivalently, our prior encourages shapes to bend rarely.",
            "section": "other",
            "section_idx": 9,
            "citations": [
                "41",
                "52",
                "53",
                "52",
                "54",
                "55"
            ]
        },
        {
            "text": "5.2 Surface Isotropy\n\nOur second prior on shapes is motivated by the ob- servation that shapes tend to be oriented isotropically in space. That is, it is equally likely for a surface to face in any direction. This assumption is not valid in many settings, such as man-made environments (which tend to be composed of ﬂoors, walls, and ceilings) or outdoor scenes (which are dominated by the ground-plane). But this assumption is more true for generic objects ﬂoating in space, which tend to resemble spheres (whose surface orientations are truly isotropic) or sphere-like shapes — though there is often a bias on the part of photographers towards imaging the front-faces of objects. Despite its prob- lems, this assumption is still effective and necessary.\n\n' Log-Likelihood 4 0 VH(Z)\n\nSS4YRXMOM aah inde & foe 7 VDC? 7eoer \\ er. etn Aw HOY\n\n(a) Smoothness\n\n(b) Samples\n\nFig. 11. To encourage shapes to be smooth, we model the variation in mean curvature of shapes using a Gaussian scale mixture, shown in 11(a). In 11(b) we show patches of shapes in our training data, sorted from least costly (upper left) to most costly (lower right). Likely shapes under our model look like soap-bubbles, and unlikely shapes look contorted.\n\n25 Data “| | == Model Bae 8 8 215 a = fe 05 0 i) 0.5 1 N#(Z)\n\n(a) An isotropic shape\n\n(b) Our isotropy prior\n\nFig. 12. We assume the surfaces of shapes to be isotropic — equally likely to face in any orientation, like in a sphere. However, observing an isotropic shape imposes a bias, as observed surfaces are more likely to face the observer than to be perpendicular to the ob- server (as shown by the red gauge ﬁgure “thumbtacks” placed on the sphere in 12(a)). We undo this bias by imposing a prior on Nz, shown in 12(b), which coarsely resembles our training data.\n\nIntuitively, one may assume that imposing this isotropy assumption requires no effort: if our prior assumes that all surface orientations are equally likely, doesn’t that correspond to a constant cost for all surface orientations? However, this ignores the fact that once we have observed a surface in space, we have introduced a bias: observed surfaces are much more likely to face the observer (Nz ≈ 1) than to be perpendicular to the observer (Nz ≈ 0). We must therefore impose an isotropy prior to undo this bias.\n\nWe will derive our isotropy prior analytically. As- sume surfaces are oriented uniformly, and that the surfaces are observed under orthogonal perspective with a view direction (0,0,−1). It follows that all Nz (the z-component of surface normals, relative to the viewer) are distributed uniformly between 0 and 1. Upon observation, these surfaces (which are assumed\n\n10\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nto have identical surface areas) have been foreshort- ened, such that the area of each surface in the image is Nz. Given the uniform distribution over Nz and this foreshortening effect, the probability distribution over Nz that we should expect at a given pixel in the image is proportional to Nz. Therefore, maximizing the likelihood of our uniform distribution over orien- tation in the world is equivalent to minimizing the following in the image:\n\n= —)o log (NZ, (2) (17)\n\nWhere Nz x,y(Z) is the z-component of the surface normal of Z at position (x,y) (deﬁned in Appendix A).\n\nThough this was derived as an isotropy prior, the shape which maximizes the likelihood of this prior is not isotropic, but is instead (because of the nature of MAP estimation) a fronto-parallel plane. This gives us some insight into the behavior of this prior — it serves to as a sort of “fronto-parallel” prior. This prior can therefore be thought of as combating the bas-relief ambiguity [25] (roughly, that absolute scale and orientation are ambiguous), by biasing our shape estimation towards the fronto-parallel members of the bas-relief family.\n\nOur prior on Nz is shown in Figure 12(b) compared to the marginal distribution of Nz in our training data. Our model ﬁts the data well, but not perfectly. We experimented with learning distributions on Nz empirically, but found that they worked poorly com- pared to our analytical prior. We attribute this to the aforementioned photographer’s bias towards fronto- parallel surfaces, and to data sparsity when Nz is close to 0.\n\nIt is worth noting that −log(Nz) is proportional to the surface area of Z. Our prior on slant therefore has a helpful interpretation as a prior on minimal surface area: we wish to minimize the surface area of Z, where the degree of the penalty for increasing Z’s surface area happens to be motivated by an isotropy assumption. This notion of placing priors on surface area has been explored previously [56], but not in the context of isotropy. And of course, this connection relates our model to the study of minimal surfaces in mathematics [53], though this connection is some- what tenuous as the fronto-parallel planes favored by our model are very different from classical minimal surfaces such as planes and soap ﬁlms.",
            "section": "other",
            "section_idx": 10,
            "citations": [
                "25",
                "56",
                "53"
            ]
        },
        {
            "text": "5.3 The Occluding Contour\n\nThe occluding contour of a shape (the contour that surrounds the silhouette of a shape) is a powerful cue for shape interpretation [57] which often dom- inates shading cues [58], and algorithms have been presented for coarsely estimating shape given contour\n\nN\n\nLog-Likelihood N*(Z)n® + NY Z)n¥\n\n(a) A cropped object an its normals (b) Our occluding contour prior\n\nFig. 13. In 13(a) we have an image and surface normals of a subset of a cup, in our dataset. The side of this cup are “limbs”, points where the surface normal faces outward and is perpendicular to the occluding contour, while the top of the cup are “edges”, sharp discontinuities where the surface is oriented arbitrarily. Our heavy-tailed prior over surface orientation at the occluding contour in 13(b) models the behavior of limbs, but is robust to the outliers caused by edges.\n\ninformation [59]. At the occluding contour of an ob- ject, the surface is tangent to all rays from the van- tage point. Under orthographic projection (which we assume), this means the z-component of the normal is 0, and the x and y components are determined by the contour in the image. In principle, this property is absolutely true, but in practice the occluding contour of a surface tends to be composed of limbs (points where the surface is tangent to rays from the vantage point, like the smooth side of a cylinder) and edges (an abrupt discontinuity of the surface, like the top of a cylinder or the edge of a piece of paper) [60]. See Figure 13(a) for an example of a shape which contains both phenomena. Of course, this taxonomy is somewhat false — all edges are limbs, but some are so small that they appear to be edges, and some are just small enough relative to the image resolution that the “limb” assumption begins to break down.\n\nWe present a “soft” version of a limb constraint, one which captures the “limb”-like behavior we expect to see but which can be violated by edges or small limbs. Because our dataset consists of masked objects, identifying the occluding contour C is trivial (see Figure 14(a)). For each point i on C, we estimate ni, the local normal to the occluding contour in the image plane. Using those we regularize the surface normals in Z along the boundary by minimizing the following loss:\n\nfc(Z) = i ))γc i + Ny i (Z)ny (1 − (Nx i (Z)nx i∈C (18)\n\nWhere N(Z) is the surface normal of Z, as deﬁned in Appendix A. We set γc = 0.75, which ﬁts the training data best, and which performs best in practice. The inner product of ni and Ni (both of which are unit-\n\n11\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\n(a) occluding contour normals (b) shape-from-contour output\n\nFig. 14. A subset of our model that includes only our priors on shape is equivalent to a shape-from-contour model. Given only the normals of the silhouette of the object in 14(a), we can produce the coarse estimate of the shape of the object in 14(b).\n\nnorm) is 1 when both vectors are oriented in the same direction, in which case the loss is 0. If the normals do not agree, then some cost is incurred. This cost corresponds to a heavy-tailed distribution (shown in Figure 13(b)) which encourages the surface orientation to match the orientation of the occluding contour at limbs, allows surface normals to violate this assumption at edges.\n\nThis occluding-contour prior, when combined with our priors on smooth and isotropic shapes, allows us to easily deﬁne an ablation of our entire model that corresponds to a shape-from-contour algorithm: we simply optimize with respect to these shape priors, and ignore our priors on reﬂectance and illumination, thereby ignoring all but the silhouette of the input image. An example of the output of our shape-from- contour model can be seen in Figure 14(b), and this model is evaluated quantitatively against our com- plete SIRFS model in Section 8.",
            "section": "other",
            "section_idx": 11,
            "citations": [
                "57",
                "58",
                "59",
                "60"
            ]
        },
        {
            "text": "6 PRIORS OVER ILLUMINATION\n\nBecause illumination is unknown, we must regular- ize it during inference. Our prior on illumination is extremely simple: we ﬁt a multivariate Gaussian to the spherical-harmonic illuminations in our training set. During inference, the cost we impose is the (non-normalized) negative log-likelihood under that model:\n\nh(L) = λL(L − µL)TΣ−1 L (L − µL) (19)\n\nwhere µL and ΣL are the parameters of the Gaussian we learned, and λL is the multiplier on this prior (learned on the training set).\n\nWe use a spherical-harmonic (SH) model of illumi- nation, so L is a 9 (grayscale) or 27 (color, 9 dimen- sions per RGB channel) dimensional vector. In con- trast to traditional SH illumination, we parametrize log-shading rather than shading. This choice makes optimization easier as we don’t have to deal with “clamping” illumination at 0, and it allows for easier\n\nOC OSS\n\n0609 OCOD\n\ne006 oreee\n\n(a) “Laboratory” Data/Samples\n\n(b) “Natural” Data/Samples\n\nFig. 15. We use two datasets: the “laboratory”-style illuminations of the MIT intrinsic images dataset [4] which are harsh, mostly-white, and well-approximated by point sources, and a dataset of “natural” illumina- tions, which are softer and much more colorful. Shown here are some illuminations from the training sets of our two datasets, and samples from a multivariate Gaussian ﬁt to each training set (our illumination prior from Section 6), rendered on Lambertian spheres. In each visualization the illuminations are sorted from least costly (upper left) to most costly (lower right) according to either our “Laboratory” or “Natural” illumi- nation priors.\n\nregularization, as the space of log-shading SH illu- minations is surprisingly well-modeled by a simple multivariate Gaussian while the space of traditional SH illumination coefﬁcients is not.\n\nSee Figure 15 for examples of SH illuminations in our different training sets, as well as samples from our model. The illuminations in Figure 15 come from two different datasets (see Section 8) for which we build two different priors. We see that our samples look similar to the illuminations in the training set, suggesting that our model ﬁts the data well. The illuminations in these visualizations are sorted by their likelihoods under our priors, which allows us to build an intuition for what these illumination priors encourage. More likely illuminations tend to be lit from the front and are usually less saturated and more ambient, while unlikely illuminations are often lit from unusual angles and tend to exhibit strong shadowing and colors.",
            "section": "other",
            "section_idx": 12,
            "citations": [
                "4"
            ]
        },
        {
            "text": "7 OPTIMIZATION\n\nTo estimate shape, illumination, and reﬂectance, we must solve the optimization problem in Equation 2. This is a challenging optimization problem, and naive gradient-based optimization with respect to Z and L fails badly. We therefore present an effective multi- scale optimization technique, which is similar in spirit to multigrid methods [61], but extremely general and simple to implement. We will describe our technique in terms of optimizing a(X), where a(·) is some loss function and X is some signal.\n\nLet us deﬁne G, which constructs a Gaussian pyra- mid from a signal. Because Gaussian pyramid con- struction is a linear operation, we will treat G as a matrix. Instead of minimizing a(X) directly, we\n\n12\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nminimize b(Y ), where X = GTY :\n\n[6,Vy4 =0(Y): (20)\n\nX ← GTY // reconstruct signal\n\n(0, Vx4] < a(X) // compute loss & gradient\n\nVy < GVx// backpropagate gradient\n\nWe initialize Y to a vector of all 0’s, and then solve for ˆX = GT (argminY b(Y )) using L-BFGS. Any arbitrary gradient-based optimization technique could be used, but L-BFGS worked best in our experience.\n\nThe choice of the ﬁlter used in constructing our Gaussian pyramid is crucial. We found that 4-tap binomial ﬁlters work well, and that the choice of the magnitude of the ﬁlter dramatically affects mul- tiscale optimization. If the magnitude is small, then the coefﬁcients of the upper levels of the pyramid are so small that they are effectively ignored, and optimization fails (and in the limit, a ﬁlter magnitude of 0 reduces our model to single-scale optimization). Conversely, if the magnitude is large, then the coarse scales of the pyramid are optimized and the ﬁne scales are ignored. The ﬁlter that we found worked best is: 1√ 8 [1,3,3,1], which has twice the magnitude of the ﬁlter that would normally be used for Gaussian pyramids. This increased magnitude biases optimiza- tion towards adjusting coarse scales before ﬁne scales, without preventing optimization from eventually op- timizing ﬁne scales. This ﬁlter magnitude does not appear to be universally optimal — different tasks appear to have different optimal ﬁlter magnitudes. Note that this technique is substantially different from standard coarse-to-ﬁne optimization, in that all scales are optimized simultaneously. As a result, we ﬁnd much lower minima than standard coarse-to-ﬁne tech- niques, which tend to keep coarse scales ﬁxed when optimizing over ﬁne scales. Optimization is also much faster than comparable coarse-to-ﬁne techniques.\n\nTo optimizing Equation |2] we initialize Z and L to 0 ( L = 0 is equivalent to an entirely ambient, white illumination) and optimize with respect to a vector that is a concatenation of G™Z and a whitened version of L. We optimize in the space of whitened illuminations because the Gaussians we learn for il- lumination mostly describe a low-rank subspace of SH coefficients, and so optimization in the space of unwhitened illumination is ill-conditioned. We pre- compute a whitening transformation for ©; and pz, and during each evaluation of the loss in gradi- ent descent we unwhiten our whitened illumination, compute the loss and gradient, and backpropagate the gradient onto the whitened illumination. After optimizing Equation |2) we have a recovered depth map Z and illumination L, with which we calculate a reflectance image R=1-S(Z,L). When illumination is known, L is fixed. Optimizing to near-convergence (which usually takes a few hundred iterations) for a 1-2 megapixel grayscale image takes 1-5 minutes on a\n\n2011 Macbook Pro, using a straightforward Matlab/C implementation. Optimization takes roughly twice as long if the image is color. See Appendix E for a description of some methods we use to make the evaluation of our loss function more efﬁcient.\n\nWe use this same multiscale optimization scheme with L-BFGS to solve the optimization problems in Equations 10 and 12, though we use different ﬁlter magnitudes for the pyramids. Naive single-scale op- timization for these problems works poorly.",
            "section": "other",
            "section_idx": 13,
            "citations": [
                "61",
                "1,3,3,1"
            ]
        },
        {
            "text": "8 EXPERIMENTS\n\nQuantitatively evaluating the accuracy of our model is challenging, as there are no pre-existing datasets with ground-truth shape, surface normals, shading, reﬂectance, and illumination. Thankfully, the MIT Intrinsic Images dataset [4] provides ground-truth shading and reﬂectance for 20 objects (one object per image), and includes many additional images of each object under different illumination conditions. Given this, we have created the MIT-Berkeley Intrinsic Images dataset, an augmented version of the MIT Intrinsic Images dataset in which we have used pho- tometric stereo on the additional images of each object to estimate the shape of each object and the spherical harmonic illumination for each image. An example object in our dataset can be seen in Figure 2, and the appendix contains additional images and details of our photometric stereo algorithm. In all of our experiments, we use the following test-set: cup2, deer, frog2, paper2, pear, potato, raccoon, sun, teabag1, turtle. The other 10 objects are used for training.\n\nAn additional difﬁculty in evaluation is the choice of error metrics. Constructing error metrics for speciﬁc intrinsic scene properties such as a depth map or a reﬂectance image is challenging, as naive choices such as mean-squared-error often correspond very poorly with the perceptual salience of an error. Additionally, constructing a single error metric that describes all errors in each intrinsic scene property is difﬁcult. We therefore present six different error metrics that have been designed to capture different kinds of important errors for each intrinsic scene property: Z-MAE is the shift-invariant absolute error between the estimated shape and the ground-truth shape. N-MAE is the mean error between our estimated normal ﬁeld and ground-truth normal ﬁeld, in radians. S-MSE and R-MSE are the scale-invariant mean-squared-error of our recovered shading and reﬂectance, respectively. RS-MSE is the error metric introduced in conjunc- tion with the MIT intrinsic images dataset [4], which measures a locally scale-invariant error for both re- ﬂectance and shading3. L-MSE is the scale-invariant MSE of a rendering of our recovered illumination\n\n3. The authors of [4] refer to this error metric to as “LMSE”, but we will call it RS-MSE to minimize confusion with L-MSE\n\n13\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\non a sphere, relative to a rendering of the ground- truth illumination. To summarize these individual error metrics, we report an “average” error metric, which is the geometric mean of the previous six error metrics. For each error metric and the average metric, we report the geometric mean of error across the test-set images. The use of the geometric mean prevents the average error from being dominated by individual error metrics with large dynamic ranges, or by particularly challenging images. See Appendix F for a thorough explanation of our choice of error metrics.\n\nF for a thorough explanation of our choice of error metrics. Though the MIT dataset has a great deal of variety in terms of the kinds of objects used, the illumination in the dataset is very “laboratory”-like — lights are white, and are placed at only a few locations rela- tive to the object. See Figure 15(a) for examples of these “laboratory” illuminations. In contrast, natural illuminations exhibit much more color and variety: the sun is yellow, outdoor shadows are often tinted blue, man-made illuminants have different colors, and indirect illumination from colored objects may cause very colorful illuminations. To acquire some illumination models that are more representative of the variety seen in the natural world, we took all of the environment maps from the sIBL Archive4, expanded that set of environment maps by shifting and mirroring them and varying their contrast and saturation (saturation is only ever decreased, never increased) and produced spherical harmonic illumi- nations from the resulting environment maps. Af- ter removing similar illuminations, the illuminations were split into training and test sets. See Figure 15(b) for examples of these “natural” illuminations. Each object in the MIT dataset was randomly assigned an illumination (such that training illuminations were assigned to training objects, etc), and each object was re-rendered under its new illumination, using that object’s ground-truth shape and reﬂectance. We will refer to this new pseudo-synthetic dataset of natu- rally illuminated objects as our “natural” illumination dataset, and we will refer to the original MIT images as the “laboratory” illumination dataset. From our experience applying our model to real-world images, these “natural” illuminations appear to be much more representative of the sort of illumination we see in uncontrolled environments, though the dataset is heavily biased towards more colorful illuminations. We attribute this to a photographer’s bias towards “interesting” environment maps in the sIBL Archive. Given our dataset, we will evaluate our model on the task of recovering all intrinsic scene properties from a single image of a masked object, under three\n\ndifferent conditions: I: the input is a grayscale image\n\nand the illumination is “laboratory”-like, II: the input\n\nis a color image and the illumination is “laboratory”-",
            "section": "other",
            "section_idx": 14,
            "citations": [
                "4",
                "4",
                "4"
            ]
        },
        {
            "text": "|http://www.hdrlabs.com/sibl/archive-html\n\n4. http://www.hdrlabs.com/sibl/archive.html\n\nlike, and III: the input is a color image and the illumination is “natural”. For all tasks, we use the same training/test split, and for each task we tune a different set of hyperparameters on the training set (λs,λe,λa,σR,λk,λi,λc, and λL), and ﬁt a different prior on illumination (as in Section 6). Hyperparam- eters are tuned using coordinate descent to minimize our “average” error metric for the training set. For each task, we compare SIRFS against several intrin- sic images algorithms (meant to decompose an im- age into shading and reﬂectance components), upon which we’ve run a shape-from-shading algorithm on the shading image. For the sake of a generous\n\nI. Grayscale Images, Laboratory Illumination\n\nAlgorithm Z-MAE N-MAE S-MSE R-MSE RS-MSE L-MSE Avg. (1) Naive Baseline 25.56 0.7223 0.0571 0.0426 0.0353 0.0484 0.2061 (2) Retinex [4], [10] + SFS 67.15 0.8342 0.0311 0.0265 0.0289 0.0484 0.2002 (3) Tappen et al.[14] + SFS 41.96 0.7413 0.0354 0.0252 0.0285 0.0484 0.1835 (4) Shen et al.[13] + SFS 45.57 0.8293 0.0493 0.0427 0.0436 0.0484 0.2348 (A) SIRFS 31.00 0.5343 0.0156 0.0177 0.0209 0.0103 0.0998 (B) SIRFS, no R-smoothness 27.25 0.5361 0.0267 0.0255 0.0290 0.0152 0.1279 (C) SIRFS, no R-parsimony 23.53 0.4862 0.0224 0.0261 0.0228 0.0167 0.1170 (D) SIRFS, no R-absolute 24.02 0.5023 0.0190 0.0201 0.0222 0.0122 0.1037 (E) SIRFS, no Z-smoothness 29.05 0.5783 0.0241 0.0227 0.0337 0.0125 0.1254 (F) SIRFS, no Z-isotropy 98.07 0.7560 0.0200 0.0198 0.0268 0.0104 0.1419 (G) SIRFS, no Z-contour 34.29 0.7676 0.0208 0.0207 0.0232 0.0231 0.1351 (H) SIRFS, no L-gaussian 26.75 0.5929 0.0270 0.0212 0.0327 0.1940 0.1964 (I) SIRFS, no Z-multiscale 25.58 0.7233 0.0571 0.0426 0.0353 0.0414 0.2009 (J) SIRFS, no L-whitening 33.93 0.5837 0.0207 0.0208 0.0256 0.0119 0.1171 (K) Shape-from-Contour 18.96 0.4192 0.0571 0.0426 0.0353 0.0484 0.1791 (S) shape observation 4.83 0.1952 - - - - - (A+S) SIRFS + shape observation 3.72 0.2414 0.0128 0.0176 0.0210 0.0096 0.0586 (A+L)SIRFS + known illumination 27.32 0.4944 0.0175 0.0179 0.0225 - -",
            "section": "other",
            "section_idx": 15,
            "citations": [
                "4",
                "10",
                "14",
                "13"
            ]
        },
        {
            "text": "II. Color Images, Laboratory Illumination\n\nAlgorithm Z-MAE N-MAE S-MSE R-MSE RS-MSE L-MSE Avg. (1) Naive Baseline 25.56 0.7223 0.0577 0.0455 0.0354 0.0489 0.2092 (2) Retinex [4], [10] + SFS 85.34 0.8056 0.0204 0.0186 0.0163 0.0489 0.1658 (3) Tappen et al.[14] + SFS 41.96 0.7413 0.0361 0.0379 0.0347 0.0489 0.2040 (4) Shen et al.[13] + SFS 55.95 0.8529 0.0528 0.0458 0.0398 0.0489 0.2466 (5) Gehler et al.[15] + SFS 53.36 0.6844 0.0106 0.0101 0.0131 0.0489 0.1166 (A) SIRFS 19.24 0.3914 0.0064 0.0098 0.0125 0.0096 (B) SIRFS, no R-smoothness 19.23 0.4046 0.0125 0.0163 0.0214 0.0092 0.0824 (C) SIRFS, no R-parsimony 19.45 0.4312 0.0096 0.0149 0.0140 0.0091 0.0731 (D) SIRFS, no R-absolute 22.98 0.4288 0.0085 0.0113 0.0135 0.0095 0.0704 (E) SIRFS, no Z-smoothness 19.28 0.4367 0.0114 0.0116 0.0219 0.0088 0.0773 (F) SIRFS, no Z-isotropy 84.08 0.7013 0.0117 0.0128 0.0160 0.0103 0.1063 (G) SIRFS, no Z-contour 32.59 0.7351 0.0103 0.0146 0.0173 0.0444 0.1186 (H) SIRFS, no L-gaussian 20.81 0.4631 0.0199 0.0140 0.0183 0.1272 0.1358 (I) SIRFS, no Z-multiscale 25.62 0.7237 0.0574 0.0453 0.0353 0.0401 0.2022 (J) SIRFS, no L-whitening 24.96 0.4766 0.0106 0.0156 0.0188 0.0138 0.0894 (K) Shape-from-Contour 18.96 0.4192 0.0577 0.0455 0.0354 0.0489 0.1818 (S) shape observation 4.83 0.1952 - - - - - (A+S) SIRFS + shape observation 3.40 0.2126 0.0070 0.0111 0.0153 0.0063 0.0420 (A+L)SIRFS + known illumination 18.58 0.3761 0.0076 0.0120 0.0146 - -\n\nIII. Color Images, Natural Illumination\n\nZ-MAE N-MAE S-MSE R-MSE RS-MSE L-MSE\n\nAlgorithm\n\n(1) Naive Baseline 25.56 0.7223 0.0283 0.0266 0.0125 0.0371 0.1364 (2) Retinex [4], [10] + SFS 26.76 0.5851 0.0174 0.0174 0.0083 0.0371 0.1066 (3) Tappen et al.[14] + SFS 53.87 0.7255 0.0255 0.0280 0.0268 0.0371 0.1740 (4) Gehler et al.[15] + SFS 37.66 0.6398 0.0162 0.0150 0.0105 0.0371 0.1149 (A) SIRFS 28.21 0.4057 0.0055 0.0046 0.0036 0.0103 0.0469 (B) SIRFS, no R-smoothness 28.41 0.4192 0.0061 0.0057 0.0062 0.0104 0.0546 (C) SIRFS, no R-parsimony 28.90 0.4184 0.0073 0.0064 0.0041 0.0107 0.0540 (D) SIRFS, no R-absolute 20.63 0.3538 0.0068 0.0058 0.0039 0.0091 0.0466 (E) SIRFS, no Z-smoothness 24.68 0.4441 0.0087 0.0062 0.0095 0.0099 0.0618 (F) SIRFS, no Z-isotropy 50.49 0.4015 0.0046 0.0039 0.0037 0.0086 0.0475 (G) SIRFS, no Z-contour 41.27 0.7036 0.0094 0.0083 0.0062 0.0256 0.0843 (H) SIRFS, no L-gaussian 20.22 0.3937 0.0100 0.0088 0.0075 0.0483 0.0796 (I) SIRFS, no Z-multiscale 25.64 0.7205 0.0279 0.0279 0.0124 0.0291 0.1316 (J) SIRFS, no L-whitening 51.74 0.9430 0.0140 0.0106 0.0066 0.0777 0.1246 (K) Shape-from-Contour 19.55 0.4253 0.0283 0.0266 0.0125 0.0371 0.1194 (S) shape observation 4.83 0.1952 - - - - - (A+S) SIRFS + shape observation 3.17 0.1471 0.0034 0.0032 0.0030 0.0049 0.0206 (A+L)SIRFS + known illumination 10.28 0.1957 0.0018 0.0014 0.0022 - -",
            "section": "other",
            "section_idx": 16,
            "citations": [
                "4",
                "10",
                "14",
                "13",
                "15",
                "4",
                "10",
                "14",
                "15"
            ]
        },
        {
            "text": "TABLE 1\n\nWe evaluate SIRFS on three different variants of our dataset, and we compare SIRFS to several baseline techniques, several ablations, and two extensions in which additional information is provided.\n\n14\n\n0.0620\n\nAvg.\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\n) Achromatic illumination\n\n(b) Chromatic illumination\n\nFig. 16. Chromatic illumination dramatically helps shape estimation. Achromatic isophotes (K-means clusters of log-RGB values) are very elongated, while chromatic isophotes are usually more tightly localized. Therefore, under achromatic lighting a very wide range of surface orientations appear similar, but under chro- matic lighting only similar orientations appear similar.\n\ncomparison, the SFS algorithm uses our shape priors, which boosts each baseline’s performance (detailed in Appendix H). We also compare against a “naive” algorithm, which is a baseline in which Z = 0 and L = 0. Because the intrinsic image baselines do not estimate illumination, we use L = 0 as their prediction. We were forced to use different baseline techniques for different tasks, as some baselines do not have code available for running on new imagery, and some code that was designed for color images crashes when run on grayscale images.\n\nWe also compare against several ablations of our\n\ncompare against our model in which components have been removed: models B-H omit priors by simply setting their \\ hyperparameters to 0, and models I and J omit our multiscale optimization over Z and our whitened optimization over L respectively. Model K is a shape- from-contour technique, in which only our shape- priors are non-zero and L = 0, so the only effective input to the model is the silhouette of the object (for this baseline, the hyperparameters have been com- pletely re-tuned on the training set). We also compare against two extensions of SIRFS: model A+L, in which the ground-truth illumination is known (and fixed during optimization), and model A+S, in which we provide a blurry version of the ground-truth shape (convolved with a Gaussian kernel with o = 30) as input. See Appendix D for a description of an additional shape “prior” we use to incorporate the external shape observation for this one variant of our model. Model S shows the performance of just the blurry ground-truth shape provided as input to model A+S, for reference. The performance of SIRFS relative to some of these baselines and extensions can be see in Table [I in Figure [2] and in the appendices.\n\nFrom Table 1, we see that SIRFS outperforms all baseline techniques. For grayscale images, the im- provement is substantial: our error is roughly half that of the best technique. For color images under “laboratory” illumination, our recovered shading and reﬂectance images are only slightly better than those of the best-performing intrinsic image technique [15],\n\nbut our recovered shape and surface normals are sig-\n\nniﬁcantly better, demonstrating the value of a uniﬁed technique over a piecewise system that ﬁrst does in- trinsic images, and then does shape from shading. For color images under “natural” illumination, SIRFS out- performs all baseline models by a very large margin, it is the only model that can reason well about color illumination and (implicitly) color shading. From our ablation study, we see that each prior contributes positively to performance, though the improvement we get from each prior is greater in the grayscale case than in the color/natural case. This makes sense, as color images under natural illumination contain much more information than in grayscale images, and so the “likelihood” dominates our priors during inference. Our ablation study also shows that our multiscale optimization is absolutely critical to performance. Sur- prisingly, our shape-from-contour baseline performs very well in terms of our shape/normal error metrics. This is probably just a reﬂection of the fact that all models are bad at absolute shape reconstruction, due to the inherent ambiguity in shape-from-shading, and so the overly-smooth shape predicted by the shape- from-contour model, by virtue of being smooth and featureless, has a low error relative to the more elabo- rate depth maps produced by other models. Of course, the shape-from-contour model performs poorly on all other error metrics, as we would expect. This anal- ysis of the inherent difﬁculty of shape estimation is further demonstrated by model A+S, which includes external shape information, and which therefore per- forms much better in terms of our shape/normal error metrics, but surprisingly performs similarly to model A (basic SIRFS) in terms of all other error metrics. From the performance of model A+L we see that knowing the illumination of the scene a-priori does not help much when the illumination is laboratory- like, but helps a great deal when the illumination is “natural” — which makes sense, as more-varied illumination simply makes the reconstruction task more difﬁcult. One surprising conclusion we can draw is that, though the intrinsic image baselines perform worse in the presence of “natural” illumination, SIRFS actually performs better in natural illumination, as it can exploit color illumination to better disambiguate\n\nbetween shading and reﬂectance (Figure 4), and pro-\n\nduce higher-quality shape reconstructions (Figure 16).\n\nThis ﬁnding is consistent with recent work regarding\n\nshape-from-shading under natural illumination [62].\n\nHowever, we should mention that some of the im-",
            "section": "other",
            "section_idx": 17,
            "citations": [
                "2",
                "15",
                "62"
            ]
        },
        {
            "text": "ACKNOWLEDGMENTS\n\nJ.B. was supported by NSF GRFP and ONR MURI N00014-10-10933. Thanks to Trevor Darrell, Bruno Olshausen, David Forsyth, Bill Freeman, Ted Adelson, and Estee Schwartz.\n\nREFERENCES\n\n[1] E. Adelson and A. Pentland, “The perception of shading and reﬂectance,” Perception as Bayesian inference, 1996.\n\n[2] D. Field, “Relations between the statistics of natural images and the response properties of cortical cells,” JOSA A, 1987.\n\n[3] D. Ruderman and W. Bialek, “Statistics of natural images: Scaling in the woods,” Physical Review Letters, 1994.\n\n[4] R. Grosse, M. K. Johnson, E. H. Adelson, and W. T. Freeman, “Ground-truth dataset and baseline evaluations for intrinsic image algorithms,” ICCV, 2009.\n\n[5] J. T. Barron and J. Malik, “High-frequency shape and albedo from shading using natural image statistics,” CVPR, 2011.\n\n[6] ——, “Shape, albedo, and illumination from a single image of an unknown object,” CVPR, 2012.\n\n[7] ——, “Color constancy, intrinsic images, and shape estima- tion,” ECCV, 2012.\n\n[8] A. Gilchrist, Seeing in Black and White. Oxford University Press, 2006.\n\n[9] E. H. Land and J. J. McCann, “Lightness and retinex theory,” JOSA, 1971.\n\n[10] B. K. P. Horn, “Determining lightness from an image,” Com- puter Graphics and Image Processing, 1974.\n\n[11] H. Barrow and J. Tenenbaum, “Recovering intrinsic scene characteristics from images,” Computer Vision Systems, 1978.\n\n[12] M. Bell and W. T. Freeman, “Learning local evidence for shading and reﬂectance,” ICCV, 2001.\n\n[13] J. Shen, X. Yang, Y. Jia, and X. Li, “Intrinsic images using optimization,” CVPR, 2011.\n\n[14] M. F. Tappen, W. T. Freeman, and E. H. Adelson, “Recovering intrinsic images from a single image,” TPAMI, 2005.\n\n[15] P. Gehler, C. Rother, M. Kiefel, L. Zhang, and B. Schoelkopf, “Recovering intrinsic images with a global sparsity prior on reﬂectance,” NIPS, 2011.\n\n[16] D. A. Forsyth, “A novel algorithm for color constancy,” IJCV, 1990.\n\n[17] L. T. Maloney and B. A. Wandell, “Color constancy: a method for recovering surface spectral reﬂectance,” JOSA A, 1986.\n\n[18] G. Klinker, S. Shafer, and T. Kanade, “A physical approach to color image understanding,” IJCV, 1990.\n\n[19] G. Finlayson, S. Hordley, and P. Hubel, “Color by correlation: a simple, unifying framework for color constancy,” TPAMI, 2001.\n\n[20] D. H. Brainard and W. T. Freeman, “Bayesian color constancy,” JOSA A, 1997.\n\n[21] B. K. P. Horn, “Obtaining shape from shading information,” in The Psychology of Computer Vision, 1975.\n\n[22] T. Rindﬂeisch, “Photometric method for lunar topography,” Photogrammetric Engineering, 1966.\n\n[23] M. J. Brooks and B. K. P. Horn, Shape from shading. MIT Press, 1989.\n\n[24] R. Zhang, P.-S. Tsai, J. E. Cryer, and M. Shah, “Shape-from- shading: a survey,” TPAMI, 2002.\n\n[25] P. Belhumeur, D. Kriegman, and A. Yuille, “The Bas-Relief Ambiguity,” IJCV, 1999.\n\n[26] J. Koenderink, A. van Doorn, C. Christou, and J. Lappin, “Shape constancy in pictorial relief,” Perception, 1996.\n\n[27] K. Ikeuchi and B. Horn, “Numerical shape from shading and occluding boundaries,” Artiﬁcial Intelligence, 1981.\n\n[28] H. Boyaci, K. Doerschner, J. L. Snyder, and L. T. Maloney, “Surface color perception in three-dimensional scenes,” Visual Neuroscience, 2006.\n\n[29] R. Woodham, “Photometric method for determining surface orientation from multiple images,” Optical engineering, 1980.\n\n[30] R. Basri, D. Jacobs, and I. Kemelmacher, “Photometric stereo with general, unknown lighting,” IJCV, 2007.\n\n[31] R. Hartley and A. Zisserman, Multiple view geometry in com- puter vision. Cambridge University Press, 2003.\n\n[32] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon, “Bundle adjustment - a modern synthesis,” ICCV, 1999.\n\n[33] Y. Yu, P. Debevec, J. Malik, and T. Hawkins, “Inverse global illumination: recovering reﬂectance models of real scenes from photographs,” SIGGRAPH, 1999.\n\n[34] D. Hoiem, A. A. Efros, and M. Hebert, “Recovering surface layout from an image,” IJCV, 2007.\n\n[35] A. Saxena, M. Sun, and A. Ng, “Make3d: learning 3d scene structure from a single still image,” TPAMI, 2008.\n\n[36] V. Blanz and T. Vetter, “A morphable model for the synthesis of 3D faces,” SIGGRAPH, 1999.\n\n[37] J. Huang and D. Mumford, “Statistics of natural images and models,” CVPR, 1999.\n\n[38] J. Portilla, V. Strela, M. J. Wainwright, and E. P. Simoncelli, “Image denoising using scale mixtures of gaussians in the wavelet domain,” IEEE Trans. Image Process, 2003.\n\n[39] S. Roth and M. J. Black, “Fields of experts: A framework for learning image priors,” CVPR, 2005.\n\n[40] R. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. Free- man, “Removing camera shake from a single photograph,” SIGGRAPH, 2006.\n\n[41] J. Huang, A. B. Lee, and D. Mumford, “Statistics of range images,” CVPR, 2000.\n\n[42] F. Romeiro and T. Zickler, “Blind reﬂectometry,” ECCV, 2010.\n\n[43] R. O. Dror, A. S. Willsky, and E. H. Adelson, “Statistical characterization of real-world illumination,” JOV, 2004.\n\n[44] B. K. P. Horn, “Shape from shading: A method for obtaining the shape of a smooth opaque object from one view,” MIT, Tech. Rep., 1970.\n\n[45] R. Ramamoorthi and P. Hanrahan, “An Efﬁcient Representa- tion for Irradiance Environment Maps,” CGIT, 2001.\n\n[46] N. Alldrin, S. Mallick, and D. Kriegman, “Resolving the gener- alized bas-relief ambiguity by entropy minimization,” CVPR, 2007.\n\n[47] G. D. Finlayson, M. S. Drew, and C. Lu, “Entropy minimization for shadow removal,” IJCV, 2009.\n\n18\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\n[48] I. Omer and M. Werman, “Color lines: Image speciﬁc color representation,” CVPR, 2004.\n\n[49] J. C. Principe and D. Xu, “Learning from examples with quadratic mutual information,” Workshop on Neural Networks for Signal Processing, 1998.\n\n[50] J. Chen, S. Paris, and F. Durand, “Real-time edge-aware image processing with the bilateral grid,” SIGGRAPH, 2007.\n\n[51] S. Bell, P. Upchurch, N. Snavely, and K. Bala, “Opensurfaces: A richly annotated catalog of surface appearance,” SIGGRAPH, 2013.\n\npixel: ni = [Nx i ,Ny i ,Nz i ]T. Rendering that point with spherical harmonics is:\n\nS(ni,L) = [ni;1]TM[ni;1] c1L9 c1L5 c1L8 c2L4 (23) M = c1L5 −c1L9 c1L8 c1L6 c1L6 c3L7 c2L2 c2L3 c2L4 c2L2 c2L3 c4L1 − c5L7",
            "section": "other",
            "section_idx": 18,
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33",
                "34",
                "35",
                "36",
                "37",
                "38",
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45",
                "46",
                "47",
                "48",
                "49",
                "50",
                "51"
            ]
        },
        {
            "text": "[52] O. Woodford, P. Torr, I. Reid, and A. Fitzgibbon, “Global stereo reconstruction under second-order smoothness priors,” TPAMI, 2009.\n\nc1 = 0.429043 c2 = 0.511664\n\n[53] D. Hilbert and C. S. Vossen, Geometry and the Imagination. Chelsea Publishing Company, 1956.\n\nc3 = 0.743125 c4 = 0.886227 c5 = 0.247708\n\n[54] H. P. Moreton and C. H. S´equin, “Functional optimization for fair surface design.” in SIGGRAPH, 1992.\n\n[55] P. Besl and R. Jain, “Segmentation through variable-order surface ﬁtting,” TPAMI, 1988.\n\n[56] D. A. Forsyth, “Variable-source shading analysis,” IJCV, 2011.\n\n[57] J. Koenderink, “What does the occluding contour tell us about solid shape?” Perception, 1984.\n\n[58] P. Mamassian, D. Kersten, and D. C. Knill, “Categorical local- shape perception,” Perception, 1996.\n\n[59] M. Brady and A. Yuille, “An extremum principle for shape from contour,” TPAMI, 1983.\n\nNote that S(ni,L) is the log-shading at pixel i, not the shading. This is different from the traditional usage of spherical harmonic illumination. Directly modeling log-shading makes optimization easier by guaranteeing that shading is greater than 0 without needing to clamp shading at 0, as is normally done. The gradient of the log-shading at this point with respect to the surface normal is:\n\n[60] J. Malik, “Interpreting line drawings of curved objects,” IJCV, vol. 1, 1987.\n\nBi = ∇niS(ni,L) = 2nT i M[:,1 : 3]\n\n[61] D. Terzopoulos, “Image analysis using multigrid relaxation methods,” TPAMI, 1986.\n\n[62] M. K. Johnson and E. H. Adelson, “Shape estimation in natural illumination,” CVPR, 2011.\n\n[63] D. Forsyth and A. Zisserman, “Reﬂections on shading,” TPAMI, 1991.\n\nWhere B is a three-channel image, where Bx is the gradient of S with respect to Nx, etc. Given the log- shading, we can infer what the log-albedo at this point must be:\n\n[64] J. T. Barron and J. Malik, “Intrinsic scene properties from a single rgb-d image,” CVPR, 2013.\n\nAi = Ii − S(ni,L) (24)\n\n[65] A. Adams, J. Baek, and M. A. Davis, “Fast high-dimensional ﬁltering using the permutohedral lattice,” Eurographics, 2012.\n\n[66] J. Koenderink, A. Van Doorn, C. Christou, and J. Lappin, “Perturbation study of shading in pictures,” Perception, 1996.\n\nAfter calculating g(A) and ∇Ag(A), we can backprop- agate the gradient onto Z as follows:\n\n[67] A. Blake, A. Zisserman, and G. Knowles, “Surface descriptions from stereo and shading,” Image and Vision Computing, 1986.\n\nDS = −∇Ag(A) (25)\n\nDx = Bx × F11 + By × F12 + Bz × F13",
            "section": "other",
            "section_idx": 19,
            "citations": [
                "52",
                "53",
                "54",
                "55",
                "56",
                "57",
                "58",
                "59",
                "60",
                "61",
                "62",
                "63",
                "64",
                "65",
                "66",
                "67"
            ]
        },
        {
            "text": "APPENDIX A\n\nLINEARIZATION AND RENDERING\n\nDy = Bx × F12 + By × F22 + Bz × F23\n\nVzg(A) = (Ds x Dz) *h$\n\n+ (Dg x Dy) * h§\n\n3\n\nHere we will detail how to calculate S(Z,L) (the log-shading of some depth map Z subject to some spherical harmonic illumination L) and its analytical derivative efﬁciently, for the purpose of calculating A and backpropagating losses on A back onto Z . First, we convert Z into a set of surface normals:\n\nwhere x is component-wise multiplication of two images and x is cross-correlation.\n\nLet us construct the matrix J, the Jacobian matrix of all partial derivatives of S with respect to L, which is a n by 9 matrix (where n is the number of pixels in S), where row i is:\n\nZ«hk y _ 2*h3 . 1 B? N B’ N B B= vi + (Zh)? + (Z* h¥)? N® (21)\n\nJi = [c4, 2coN},\n\nJi = [c4, 2coN}, 2coNF, 2coN}P, 2c NPN}, 2c, NPN}, c3N7 Nj — ¢5,2c1.N}Nj,c1(N NZ — N?N?)|\n\nWe can use this matrix to backpropagate the gradient of the loss with respect to S onto L, as follows:\n\n(21)\n\nwhere ∗ is convolution. We also compute the follow- ing:\n\n∇Lg(A) = JTDS (26)\n\nF11 = (1 − Nx × Nx) × Nz F22 = (1 − Ny × Ny) × Nz F13 = −(Nx × Nz × Nz) F23 = −(Ny × Nz × Nz) F12 = −(Nx × Ny × Nz)\n\nwhere × is component-wise multiplication of two images. Let us look at the surface normal at one\n\n(22)\n\nWe have described how to linearize a depth map, compute a log-shading image of that linearization with respect to a grayscale spherical-harmonic model of illumination, and backpropagate a gradient with respect to that shading image onto the depth map and the illumination model. To do the same for a color image, we simply do the same procedure three times — though for efﬁciency’s sake we need only linearize the depth map once.\n\n19\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nAPPENDIX B EFFICIENT QUADRATIC ENTROPY\n\nHere we will detail a novel method for calculating the quadratic entropy measure introduced in [49], which we use in our parsimony prior on log-reﬂectance. Let x be a vector, N is the length of x, and σ is the bandwidth parameter (the width of the Gaussian bump around each element of x). Then the quadratic entropy of x under the Parzen window deﬁned by x and σ is deﬁned as:\n\nN WN flys (x = x;)? A(x) = -log|s exp | ~~) Z = N?V4n0? (27)\n\n(27)\n\nNote that we will use H(·) here to describe entropy, rather than mean curvature. Our ﬁrst insight is that this can be re-expressed as a function on a histogram of x. Let W be the bin-width of the histogram of x, let M be the number of bins, and let na be the count of x in bin a. Then:\n\nAm W2(a —b)? ud? ye oS\") a=1b=1 (28) H(n) = on (\n\nThough the computation complexity of this formu- lation is still quadratic with respect to M, if the histogram is constructed such that many datapoints fall in the same bin this formulation can be much more efﬁcient in practice. Our second insight is that this can be expressed as a convolution of n with a small Gaussian ﬁlter. Let g be a Gaussian ﬁlter:\n\ngd = 1 Z exp − W 2d2 4σ2 (29)\n\nWhere d is distance from the center. With this, we can rewrite H(n) as follows:\n\nH(n) = — log (n™(n *g)) (30)\n\nWhere ∗ is convolution. This quantity is extremely efﬁcient to compute, provided that the lengths of n and g are small, which is true provided that the range of x is not much larger than σ, which is generally true in practice.\n\nThis formulation also allows us to easily compute the gradient of V (n) with respect to n:\n\n∇H(n) = −2 nT(n ∗ g) (n ∗ g) (31)\n\nHistogramming is a non-smooth operation, making this approximation to entropy not differentiable with respect to x. However, if instead of standard his- togramming we use linear interpolation to construct n, then the gradient with respect to x is non-zero and can be calculated easily.\n\nLet RL and RU deﬁne the bounds on the range of the bins, with RU = min(x) and RL = max(x). The\n\nfenceposts assigned to datapoint xi are bL and bU, where bL is the largest fencepost below it, and bU is the smallest fencepost above it:\n\nbr = (a — Rr)/W], bs =br +1 (32)\n\nxi will be assigned to those bins according to these weights:\n\nwL = (xi − bL)/W, wU = 1 − wL (33)\n\nWhen adding xi to the histogram, we just add these two weights to the appropriate bins:\n\nnL = nL + wL, nU = nU + wU (34)\n\nThe partial derivatives of the histogram with respect to xi are simple:\n\n∂nL ∂xi = − 1 W , ∂nU ∂xi = 1 W (35)\n\nWith this, we can construct the Jacobian J of n with respect to x, which is a M by N sparse matrix. With this, we can calculate the gradient of H with respect to x:\n\n∇H(x) ≈ JT∇H(n) (36)\n\nThis approximation to quadratic entropy is, in prac- tice, extremely efﬁcient and extremely accurate. Other techniques exist for computing approximations to this quantity, most notably the fast Gauss transform and the improved fast Gauss transform. Also, H(x) could be computed exactly using the naive formulation in Equation 27. The naive formulation is completely intractable, as the computation complexity is O(N2). The FGT-based algorithms are O(N logN), and pro- vide no efﬁcient way to compute ∇H(x), which makes those algorithms impossible to use in our gradient-based optimization scheme. Our approxima- tion has a complexity of O(N) (provided the kernel in the convolution is small) and allows for ∇H(x) to be approximated extremely efﬁciently. In practice, our model produces approximations of entropy that are usually within 0.01% of the true entropy, which is similar to the accuracy obtained using the fast Gauss transform or the improved fast Gauss transform, and is 10 or 100 times faster than the FGT-based algo- rithms.\n\nThis techniques for computing quadratic entropy for a univariate signal can easily be generalized to higher dimensions. We use a three-dimensional gen- eralization to compute the quadratic entropy of a color (whitened) log-reﬂectance image. Instead of con- structing a 1D histogram with linear interpolation, we construct a 3D histogram using trilinear interpolation, and instead of convolving our 1D kernel with a Gaussian ﬁlter, we convolve the 3D histogram with three separable Gaussian ﬁlters.\n\n20\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nNote that this formulation is extremely similar to the bilateral grid [50], which is a tool for high- dimensional Gaussian ﬁltering (used mostly for bi- lateral ﬁltering, hence the name). The calculation of our entropy measure is extremely similar to the “splat, blur, slice” pipeline in other high-dimensional Gaussian ﬁltering works [65], except that after the “slice” operation we take the inner product of the input “signal” and the blurred output signal. This means that we need not actually compute the slice operation, but can instead just compute the inner product directly in the histogram space. This connec- tion means that the body of work for efﬁciently com- puting this quantity in the context of image ﬁltering can be directly adapted to the problem of computing high-dimensional entropy measures. Recent work [65] suggests that for dimensionalities of 3, our bilateral grid formulation is the most efﬁcient among exist- ing techniques, but that this entropy measure could be computed reasonably efﬁciently in signiﬁcantly higher-dimensional spaces (up to 8 or 16) using more sophisticated techniques.",
            "section": "other",
            "section_idx": 20,
            "citations": [
                "49",
                "50",
                "65",
                "65"
            ]
        },
        {
            "text": "APPENDIX C MEAN CURVATURE\n\nHere we will detail how to calculate H(Z) (the mean curvature of a depth map Z, not entropy) and its analytical derivative efﬁciently. Mean curvature on a surface is a function of the ﬁrst and second partial derivatives of that surface.\n\n(14 22) Zyy —2Z.ZyZoy + (1+ Z2) Zow 2(14+ 22422)\" A(Z) (37)\n\n(37)\n\nTo calculate this for a discrete depth map, we will ﬁrst approximate the partial derivatives using ﬁlter convolutions.\n\nZx = Z ∗ hx 3, Zy = Z ∗ hy 3 (38)\n\n3 3 , Zyy = Z ∗ hyy 3 , Zxy = Z ∗ hxy Zxx = Z ∗ hxx 3 3 = 1 hx 8 1 0 –1 2 0 –2 1 0 –1 , hy 3 = 1 8 1 2 1 0 0 0 –1 –2 –1 hxy 3 = 1 4 1 0 –1 0 0 0 –1 0 1 , hyy 3 = 1 4 1 2 1 –2 –4 –2 1 2 1 , 3 = 1 hxx 4 1 –2 1 2 –4 2 1 –2 1\n\nWe then compute the following intermediate “im- ages”, and use them to compute H(Z).\n\nM = 1 + Z2 x + Z2 y N = (1 + Z2 x)Zyy − 2ZxZyZxy + (1 + Zy2)Zxx D = 2M3 H(Z) = N/D (39)\n\nWhen computing H(Z), we also compute the follow- ing, which are stored until after the loss function with respect to H(Z) has been calculated, at which point\n\nthey will be used to backpropagate the gradient of the loss function using the chain rule.\n\nFx = 2(ZxZyy − ZxyZy) − 3ZxN M2 Fy = 2(ZxxZy − ZxZxy) − 3ZyN M2 Fxx = 1 + Z2 y Fyy = 1 + Z2 x Fxy = −2ZxZy\n\nGiven f(H(Z)) and ∇H(Z)f, a loss function and the gradient of that loss function with respect to H(Z), we can calculate ∇Zf, the gradient of the loss with respect to Z, as follows:\n\n∇H(Z)f B = D (41)\n\nVif = (BFr)*h§ + (BFy) «hy +) (BF rx) * RS + (BFyy) * ney + (BFry) * ni”\n\nAdjacent variables are component-wise multiplication of two images, / is component-wise division, * is convolution and x is cross-correlation.\n\nAPPENDIX D NOISY SHAPE OBSERVATION\n\nOne of the reasons that using shading cues to recover shape (as we are attempting here) is challenging, is that shading is a fundamentally poor cue for low- frequency (coarse) shape variation. Shading is directly indicative of only the shape of a point relative to its neighbors: ﬁne-scale variations in shape produce sharp, localized changes in an image, while coarse- scale shape variations produce very small, subtle changes across an entire image. Both algorithms [25] and humans [66] therefore make errors in estimating coarse depth when using only shading. Bas relief sculptures take advantage of this by conveying the impression of a rich, deep 3D scene, using only the shading produced by a physically shallow object.\n\nTo deal with this issue, we will construct our prior on shape to allow for an external observation of shape to be incorporated into inference. This observation may be produced by a stereo algorithm, or by some depth sensor such as a laser rangeﬁnder or the Kinect. These depth sensors or stereo algorithms often pro- duce depth maps which are noisy or incomplete, or most often blurry — lacking ﬁne-scale shape detail. Because of the complementary strengths of stereo and shading, combining the two can often yield very accurate results [67], [5].\n\nWe will construct a loss function to encourage our recovered depth Z to resemble the raw sensor depth ˆZ:\n\nfolZ,2) = > (((Z* (oz): — 4)? +2) i to. 2 (42)\n\n21\n\n(40)\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nThis is simply a hyperlaplacian distribution with an exponent of 7, on the difference between (Z * b(az)) and Z at every pixel, with « added in to make the loss differentiable everywhere. b(az) is a 2D Gaussian filter with a standard deviation of oz, and * is convo- lution, so (Z « b(az)); is the value of a blurry version of our shape estimate Z at pixel location i. We tune Yo on the training set, which sets it to ~ 1, and we set € = 1/100. The robust nature of this cost encourages Z to resemble Z, while allowing it to occasionally differ drastically. In our experiments we use Z* *b(30) as our Z, which is a reasonably proxy for a stereo algorithm or low-resolution depth-sensor, and we set oz = 30 as that value (unsurprisingly) performs best during cross-validation.\n\nFor the one variant of our model in which we incor- porate a noisy external shape observation, our prior on shapes gains an additional term, and becomes:\n\nf(Z) = λkfk(Z)+λifi(Z)+λcfc(Z)+λofo(Z, ˆZ) (43)\n\nWhere λo is cross-validated on the training set.",
            "section": "other",
            "section_idx": 21,
            "citations": [
                "25",
                "66",
                "67",
                "5"
            ]
        },
        {
            "text": "APPENDIX E EFFICIENT COMPUTATION\n\nOur model is fairly computationally expensive. Eval- uating our loss function and its gradient takes close to a second, and optimization requires that the loss be evaluated hundreds of times. To make this model more tractable, we use some additional tricks to speed up the computation of the loss function.\n\nFirst, our smoothness priors for reﬂectance and shape require repeatedly computing the negative log- likelihood of a Gaussian scale mixture. Computing this naively is very expensive, but it can be made extremely efﬁcient by pre-computing a lookup ta- ble of the negative log-likelihood, and indexing into that to compute the gradient and its loss. For the multivariate GSM used in our smoothness prior for color reﬂectance, we can construct a lookup table of negative log-likelihood with respect to Mahalanobis distance under the covariance matrix Σ in our GSM.\n\nWhen computing our smoothness priors, it’s often fastest to pre-compute the pairs of pixels within all 5 × 5 windows, and construct a sparse matrix where for each pair, we have a row in which the column corresponding to one pixel in the pair is set to 1 and the column corresponding to the other pixel is set to −1. With this, a vector of pairwise distances between pixels can be computed efﬁciently with one sparse matrix-vector product. Also, expressing this pairwise distance computation as a matrix multiplication al- lows gradients to be easily backpropagated from the vector of differences onto the raw pixels by simply multiplying the gradient vector by the transpose of this matrix.\n\nThe prior for absolute reﬂectance can be computed efﬁciently using the same bilateral-grid trick used for\n\nentropy: splat the signal into a histogram, compute the loss of the histogram, and then backpropagate onto the data. For even more efﬁciency, we can use the same histogram for both the entropy prior and the absolute prior, which means that for each evaluation of the loss function, we only need to compute one his- togram from the reﬂectance and one backpropagation from the histogram to the reﬂectance.",
            "section": "other",
            "section_idx": 22,
            "citations": []
        },
        {
            "text": "APPENDIX F ERROR METRICS\n\nChoosing good error metrics for this task is chal- lenging. We will use the geometric mean of six error metrics: two for shape, one for illumination, one for shading, one for reﬂectance, and the MIT intrinsic images error metric introduced in [4], which we will refer to as RS-MSE (though which the original authors call “LMSE”). We use the geometric mean of these metrics as it is insensitive to the different dynamic ranges of the constituent error metrics, and is difﬁcult to trivially minimize in practice.\n\nOur ﬁrst shape error metric is:\n\n+b} (44) Z-MAB(Z,Z*) = —min x |Z -Zi,\n\nThis is the shift-invariant absolute error between the estimated shape ˆZ and the ground-truth shape Z∗. This error metric is sensitive to all errors in shape es- timation, except for the absolute distance of the shape from the viewer (which is unknowable under ortho- graphic projection). It can be computed efﬁciently by setting b to the median of ˆZ − Z∗.\n\nOur second shape error metric is:\n\ncarey — 1 « r N-MAE(N,.N*) == Do areeos (Now . Niy) (45)\n\nThis is the mean error between the normal ﬁeld ˆN of our estimated shape ˆZ and the normal ﬁeld N∗ of the ground-truth shape Z∗, in radians. This metric is most sensitive to very ﬁne-scale errors in ˆZ, which is what determines surface orientation.\n\nFor illumination, our error metric is:\n\nL-MSB(L, L*) = —min yy |aV (L) x,y — V(L* )eyll3 (46)\n\nWhich is the scale-invariant MSE of a rendering of our recovered illumination ˆL and the ground-truth illumination L∗. V (L) is a function that renders the spherical harmonic illumination L on a sphere and returns the log-shading. V (L)x,y is a 3-vector of log- RGB at position (x,y) in the renderings. The α mul- tiplier makes this error metric invariant to absolute scaling, meaning that estimating illumination to be twice as bright or half as bright doesn’t change the error. But because there is only one multiplier rather than individual scalings for each RGB channel, this\n\n22\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nerror metric is sensitive to the overall color of the illuminant. This choice seems consistent with what we would like: estimating absolute intensity of an illuminant from a single image is both incredibly difﬁcult and not very useful, but estimating the color of the illuminant is a reasonable thing to expect from an algorithm, and would be useful for many applications (color constancy, relighting, reﬂectance estimation, etc). We impose our error metric in the space of visualizations of the illumination rather than in the space of the actual spherical harmonic coefﬁ- cients that generated that visualization, both because it makes our error metric invariant to the choice of illumination model, and because we found that often the recovered illumination could look quite similar to the ground-truth, while having a very different spherical harmonic representation.\n\nFor shading and reﬂectance, we use:\n\nS-MSE(S, s*) = * min So llesey—styll, 47)\n\nRMSE(F,r*) = +min J ljarey —rhyl 8) ~\n\nThese are the scale-invariant MSEs of our recovered shading ˆs = exp(S( ˆZ, ˆL)) and reﬂectance ˆr = exp( ˆR). Just like in L-MSE, we are invariant to absolute scaling of all RGB channels at once, but not invariant to scaling each channel individually. This makes these error metrics sensitive to errors in estimating the overall color of the shading and reﬂectance images, but invariant to illumination. Note that these error metrics are of shading and reﬂectance, not of log- shading and log-reﬂectance, even though the rest of this paper is written almost entirely in terms of log- intensity. We could have used shift-invariant error metrics in log-intensity space, but we found these to be too sensitive to errors in dark regions of the image — places in which we’d expect any algorithm to do worse, simply because there is less signal.\n\nOur ﬁnal error metric is the metric introduced in conjunction with the MIT intrinsic images dataset [4], which the authors refer to as LMSE, but which we will call RS-MSE to minimize confusion with L-MSE. This metric measures error for both reﬂectance and shading, and is locally scale-invariant. The intent of the local scale-invariance is to make the metric in- sensitive to low-frequency errors in either shading or reﬂectance. In keeping with this spirit, we apply this error metric individually to each RGB channel and take the mean of those three errors as RS-MSE, mak- ing this error metric not just robust to low-frequency error, but robust to most errors in estimating the color of the illumination. This error metric therefore serves to be somewhat complementary to S-MSE and R-MSE, which are sensitive to everything except absolute intensity.\n\nRS-MSE is the mean of the local scale-invariant\n\nMSE of shading and reﬂectance, normalized so that an estimate of all zeros has the maximum score of 1:\n\nRS-MSE(ˆs, ˆr,s∗,r∗) = 1 2 e(ˆs,0) + e(ˆr,r∗) e(ˆr,0) (49)\n\nWhere e(·) is the sum of the scale-invariant MSE at all local windows w of size 20×20, spaced in steps of 10:\n\ne(@,2\") = S> min ot, — 23 wew (50)\n\nAs an aside, in our error metrics we repeatedly use scale-invariant MSE, of the form:\n\nThe closed-form solution to this problem is:\n\nAT 2 (Ge) oo",
            "section": "other",
            "section_idx": 23,
            "citations": [
                "4",
                "4"
            ]
        },
        {
            "text": "APPENDIX G DATASET\n\nHere we will detail how we recover “ground-truth” shape and spherical harmonic illumination for each image of each object in our dataset. This is a simple photometric stereo algorithm, in which we optimize over shapes and illuminations to minimize the ab- solute error between renderings of our dataset and the actual images in our dataset. Absolute error is used to give us robustness to errors due to shadows and specularities, which our rendering engine (and therefore, our dataset) do not consider or address properly. Recovered shapes and illuminations were then cleaned up by hand to address bas-relief ambi- guity issues[25]. We treat each RGB channel of each image as a separate image.\n\nTo account for varying reﬂectance, we compute a “shading” image for each image on our dataset.\n\ns∗ i,j = exp(Ii,j − Ri) (53)\n\nWe will now detail each step in the inner loop of our iterative photometric stereo algorithm. We ﬁrst take each current shape estimate Z, and linearize it to get a set of ﬁxed surface normals. For each image j, we solve for the SH illumination that minimizes absolute error between the rendering and the shading image:\n\nLj ← argmin L i |exp(S(ni,L)) − s∗ i,j| (54)\n\nThis optimization problem is solved using Iteratively Reweighted Least-Squares. We then ﬁx each image’s illumination Lj, and optimize over each object’s nor- mals ni.\n\nni ← argmin n j |exp(S(n,Lj)) − s∗ i,j| (55)\n\n23\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nThis optimization is done with L-BFGS. In this step, the normals are decoupled, and so surface integrabil- ity is not enforced. Given this estimate of surface nor- mals, we can compute a integrable surface Z which approximates this normal ﬁeld using least-squares:\n\nnt 2 n! 2 2 + argiin > (Z 1 ~ 22) + (zane 22) i Z i i\n\nThese three optimization steps are repeated until convergence (30 iterations). For the ﬁrst 10 iterations, we constrain all of the illuminations belonging to the same object to be scaled and shifted versions of each other, but for the next 20 iterations we allow each illumination for every image to vary freely. The result of this algorithm is an estimate of Z for each object and an estimate of L for each RGB channel of every image.\n\nThis photometric stereo algorithm still suffers from Bas-Relief ambiguity[25] issues, despite the abun- dance of data. We therefore manually adjust each recovered Z over the three parameters of the Bas- Relief ambiguity by hand. Also, some regions of Z are clearly incorrect due to shadows. These regions are manually removed (and are not included in the evaluation of our error metrics which concern Z). After these manual tweaks to each shape, we update the set of illuminations to minimize absolute error once again. The two “cup” and “teabag” images did not have discriminative enough shape features for photometric stereo to recover reasonable second-order spherical harmonic illuminations, so for those objects we instead recover only ﬁrst-order spherical harmonic illumination parameters (equivalent to point-light + ambient illumination), and set the other coefﬁcients to 0.\n\nThe MIT Intrinsic Images dataset was not acquired with the goal of having the product of the “shad- ing” and “reﬂectance” images be exactly equal to the diffuse image, which our model (and our baseline models) assume. That is, a lambertian rendering of our recovered shape and illumination resembles a scaled version of the original “shading” image. We correct for this by adjusting the brightness of the “shading” image such that it matches our rendering in a least-squares sense, and we use this “corrected” shading image in all of our experiments.\n\nNote that the optimization tools we use for our photometric stereo algorithm are completely disjoint from the optimization techniques used by algorithm in our paper, despite the fact that those techniques could have been adapted to do photometric stereo. This was done intentionally to dispel any concerns that our results might be good simply because they were obtained using similar techniques as our photo- metric stereo algorithm.\n\nExamples of our recovered shapes and illumina- tions, as well as the shading and reﬂectance images",
            "section": "other",
            "section_idx": 24,
            "citations": [
                "25",
                "25"
            ]
        },
        {
            "text": "(a) Depth map\n\n(b) Shading\n\nSs % ae H4\n\n(C) Surface normals\n\n(d) Reflectance\n\n) images\n\nFig. 19. An object from our dataset. In 19(a), 19(b), 19(c), and 19(d) we have our “ground-truth” shape, shading, surface normals, and reﬂectance, respec- tively. The shading and reﬂectance images come from the MIT Intrinsic Images dataset [4], and the shape and surface normals were produced by our photometric stereo algorithm. In 19(e) we have three columns, where the ﬁrst contains the images from the MIT Intrinsic Images dataset [4], the third contains the illuminations recovered by our photometric stereo algo- rithm for each image, and the second column contains renderings of our ground-truth for each illumination, which demonstrate that our recovered models are rea- sonable. The second to last row of Figure 19(e) is the “shading” image from the MIT dataset, and the last row is the “diffuse” image, which is used as input to our model. The illumination on the last row is therefore what is referred to as the “ground-truth” illumination for this scene, in the rest of the paper.\n\n\\\n\n(C) Surface normals\n\n(d) Reflectance\n\n@) images\n\nFig. 20. Another object from our dataset, shown in the same format as Figure 19. Note that our illumination model cannot capture the cast shadows in the input images, which is why our renderings are shadowless.\n\nalready contained in the MIT Intrinsic Images dataset, can be seen in Figures 19 and 20.\n\n24\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nAPPENDIX H SHAPE FROM SHADING\n\nOur model for recovering shape and albedo given a single image and illumination can easily be reduced to a model for doing classic shape-from-shading (recov- ering shape given a single image and illumination). Our optimization problem becomes:\n\nminimize Z λ|I − S(Z,L)| + f(Z) (56)\n\nWhere I is the input log-image, and λ is a multiplier that trades off the importance of the reconstruction terms against the regularizer on Z. f(Z) and S(Z,L) are the same as deﬁned in the paper. Optimization is done using our multiscale optimization algorithm. This SFS algorithm is similar to past algorithms which optimize over a linearized representation of a depth map, with the primary difference being our choice of f(Z).\n\nThis SFS algorithm is run on the shading images produced by the “intrinsic image” algorithms we benchmark against. This is a very generous compar- ison on our part, as we are effectively giving these other algorithms one-half of the model we present here, and we are assuming that illumination is known. We used our own shape-from-shading algorithm for fairness’s sake, as it appears to outperform previ- ous SFS algorithms. This means, however, that our improvement over these algorithms is not as much a reﬂection of the effectiveness of f(Z) in isolation, but is instead a demonstration of the effectiveness of optimizing over f(Z) and g(A) to jointly recover shape and albedo, as opposed to recovering a shading image and then recovering shape from that shading image.\n\nGrayscale Image, Laboratory Illumination DAM G42 ‘AO eA GQ “ai Mo wae D 23 2B True Tappen Retinex SIRFS+S — SIRFS Color Image, Laboratory Illumination ™ BAAR ~ “ oe i: a True SIRFS Gehler Retinex SIRFS+S Color Image, Natural Illumination BA savee 5 ees O28 ws Normals Refl. Shading Light True Gehler Retinex SIRFS+S — SIRFS Image Shape\n\nFig. 21. Here we have a single image from the MIT-Berkeley Intrinsic Images dataset, under three color/illumination conditions. For each condition, we present the ground-truth, the output of SIRFS, output of SIRFS+S (which uses external shape infor- mation), and the two best-performing intrinsic image techniques (for which we do SFS on the recovered shading to recover shape). the\n\n25\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n\nGrayscale Image, Laboratory Illumination fide {ie Jie Bed De Qe ity Deo Be Ge jlke Dea Qe Rae jx De Be Se le Ge Color Image, Laboratory Illumination Pe Ge 2 Ded Oe ee @», Sem aire iwwaa SIRFS True Tappen Retinex SIRFS+S True Gehler Retinex SIRFS+S SIRFS rd Gehler Retinex SIRFS+S SIRFS True\n\nFig. 22. Here we have another image from the MIT- Berkeley Intrinsic Images dataset.\n\nGrayscale Image, Laboratory Illumination fw & Ww 0? ees YY kok Color Image, Laboratory Illumination @®& Ww\"? BSS se Ww 9 True ® Bee wwe WOete evs Image Shape Normals —Refl.. += Shading Light SIRFS SIRFS+S. Retinex Tappen True g 4 a Retinex Gehler True SIRFS SIRFS+S Retinex Gehler\n\nFig. 23. Here we have another image from the MIT- Berkeley Intrinsic Images dataset.\n\n26",
            "section": "other",
            "section_idx": 25,
            "citations": [
                "4",
                "4"
            ]
        }
    ],
    "figures": [
        {
            "path": "output\\images\\7849c52d-abf8-4a0a-8b14-54ee58d77453.jpg",
            "description": "This figure is a comprehensive visualization comparing the processing of grayscale and color images under different lighting conditions. It is divided into two main sections: \"Grayscale Images, Laboratory Illumination\" and \"Color Images, Natural Illumination.\" Each section illustrates a series of transformations applied to different objects, including buildings, faces, and everyday items.\n\nKey components include:\n- **Image**: The original input images.\n- **Shape**: The reconstructed 3D shape of the objects.\n- **Normals**: Surface normals derived from the images, showing the orientation of surfaces.\n- **Refl. (Reflectance)**: Reflectance properties of the surfaces, indicating how they reflect light.\n- **Shading Light**: Shading information illustrating how light interacts with the surface.\n- **Rotated**: Images rotated to show the object from different perspectives.\n- **Relit**: Images with altered lighting conditions to demonstrate relighting capabilities.\n\nThe figure effectively showcases the methodology for extracting and manipulating 3D information from 2D images and highlights the differences in processing between grayscale and color images. It is crucial for understanding the research's approach to image-based modeling and rendering, demonstrating both the process and outcomes of the techniques used.",
            "importance": 9
        },
        {
            "path": "output\\images\\f5268634-afdc-4e76-bac8-62dc337b0ae9.jpg",
            "description": "This figure presents a comparative analysis of different algorithms for intrinsic image decomposition under varying conditions. The figure is organized into three main sections based on image type and illumination: grayscale image under laboratory illumination, color image under laboratory illumination, and color image under natural illumination.\n\nEach section includes a series of rows corresponding to different methods: True (ground truth), SIRFS, SIRFS+S, Retinex, and Gehler/Tappen. The columns represent different components of the decomposition: Shape, Normals, Reflectance, Shading, and Light.\n\n- **Shape**: Visualized using color-coded depth maps to indicate the 3D structure.\n- **Normals**: Shown with color-coded normal maps that depict surface orientations.\n- **Reflectance**: Displays the intrinsic color of the object, independent of shading.\n- **Shading**: Grayscale images highlighting the influence of lighting on the surface.\n- **Light**: Spherical visualizations of the estimated light source.\n\nThe figure is crucial for understanding the effectiveness of each method in accurately retrieving intrinsic components, making it important for evaluating methodology and results in the research.",
            "importance": 9
        },
        {
            "path": "output\\images\\e5309e56-d489-4598-a592-a379f90c2c71.jpg",
            "description": "This figure presents a comparative analysis of different computational methods for estimating surface properties from images under various illumination conditions. The figure is divided into three sections based on image type and lighting conditions: grayscale image with laboratory illumination, color image with laboratory illumination, and color image with natural illumination.\n\nEach section compares multiple methods (e.g., SIRFS, SIRFS+S, Retinex, Tappen, Gehler) against a \"True\" baseline. The columns depict different features extracted by these methods:\n\n1. **Image**: The original input image used for analysis.\n2. **Shape**: The estimated surface shape, typically represented in 3D or depth map form.\n3. **Normals**: Surface normals calculated by each method, often visualized with color coding.\n4. **Reflectance (Refl.)**: The reflectance properties of the surface, showing how light interacts with the material.\n5. **Shading**: Shading effects as interpreted by each method, indicating how light and shadow are distributed.\n6. **Light**: The inferred lighting conditions, often represented using a sphere to show direction and intensity.\n\nThe figure is crucial for understanding the effectiveness of each method in accurately reconstructing surface properties and lighting conditions, highlighting differences in performance across various scenarios. This makes it an important visualization of the methodology and key results.",
            "importance": 9
        },
        {
            "path": "output\\images\\da014a74-2a2a-46e8-b6ba-f906cfac6a1f.jpg",
            "description": "The figure presents a comprehensive comparison of different algorithms for intrinsic image decomposition under various lighting conditions. It includes three scenarios: grayscale image with laboratory illumination, color image with laboratory illumination, and color image with natural illumination. Each scenario showcases the original image, followed by outputs from different methods: True, SIRFS, SIRFS+S, Retinex, and Gehler.\n\nFor each method, the figure illustrates:\n- **Shape**: A visualization of the object's geometry.\n- **Normals**: Surface normals indicating the orientation of each surface point.\n- **Reflectance (Refl.)**: The color and texture information, independent of shading.\n- **Shading**: The light and shadow effects on the object.\n- **Light**: The estimated lighting conditions.\n\nThis figure is crucial for understanding the effectiveness of each algorithm in separating intrinsic image components. It provides visual evidence of how different methods perform in preserving and reconstructing the shape, normals, reflectance, and shading of the objects under varying conditions, thus highlighting the strengths and weaknesses of each approach.",
            "importance": 9
        },
        {
            "path": "output\\images\\af18dca9-99cc-4743-a05b-67114c36e0af.jpg",
            "description": "The figure presents a comparative analysis of different methods for inferring intrinsic scene properties from images. It is divided into three sections based on the type of input image and lighting conditions: Grayscale Image with Laboratory Illumination, Color Image with Laboratory Illumination, and Color Image with Natural Illumination.\n\nEach section compares the results of various algorithms (Tappen, Retinex, SIRFS, SIRFS+S, Gehler) against the true scene properties. The columns represent different reconstructed properties: shape, normals, reflectance (Refl.), shading, and light. Each row showcases the output of a different algorithm, allowing for a visual comparison of their performance across these properties.\n\nThe figure is likely important for understanding the efficacy of the algorithms in accurately reconstructing scene properties under different conditions, making it a key visualization of methodology and results.",
            "importance": 8
        },
        {
            "path": "output\\images\\3212dcb8-4a05-452c-8d0b-a97d30c0e87a.jpg",
            "description": "This figure presents a graph that compares the log-likelihood of observed data (red line) with a model prediction (black line). The x-axis represents a mathematical formula, \\(N^x(Z)n^x + N^y(Z)n^y\\), which likely indicates a combination of variables or parameters involved in the model. The y-axis shows the log-likelihood values, which are crucial for assessing how well the model fits the data. The close alignment between the model and data curves suggests a good model fit, indicating the model's effectiveness in capturing the underlying patterns in the data. This type of visualization is important for understanding the validation of the model against empirical data, making it a significant component of the research findings.",
            "importance": 8
        },
        {
            "path": "output\\images\\b393dc75-addd-4f8b-a9a0-90fceeb0808e.jpg",
            "description": "This figure appears to be a graph comparing empirical data against a theoretical model. The x-axis is labeled \\(N^z(Z)\\) and the y-axis represents \"Likelihood.\" The red line represents the actual data, while the black line represents the model prediction. The purpose of this graph is likely to show how well the model fits the data. A close alignment between the red and black lines would indicate a strong model fit, while deviations suggest areas where the model may not accurately capture the data's behavior. This type of figure is important for validating the methodology and demonstrating the model's effectiveness in capturing the observed phenomena.",
            "importance": 7
        },
        {
            "path": "output\\images\\3dbf58fc-71a0-4ab4-9881-5d85a3e341b2.jpg",
            "description": "The figure is a graph titled \"Log-Likelihood\" with the x-axis labeled as \"R.\" It features two plots: a red line representing \"Data\" and a black line representing \"Model.\" The graph shows the log-likelihood values for different values of R, comparing how closely the model fits the data across this range. The model appears to have a smoother curve, while the data is more jagged, indicating variability or noise in the data. This visualization likely illustrates the effectiveness of the model in capturing the underlying distribution or trend of the data.",
            "importance": 7
        },
        {
            "path": "output\\images\\854743bb-3c2a-44b2-9f22-68e0aa7d3e8e.jpg",
            "description": "This figure shows a comparison between empirical data and a theoretical model using a graph. The x-axis represents a variable denoted as ∇B, while the y-axis shows the Log-Likelihood. The red line represents the data, and the black line represents the model. The close alignment between the data and model suggests that the model accurately fits or predicts the data over the range of ∇B. This type of figure is important for validating the model against observed data, thus playing a key role in demonstrating the research's findings.",
            "importance": 7
        },
        {
            "path": "output\\images\\8f3fabc3-6a75-4700-aab2-1a4c55d8ce1e.jpg",
            "description": "The figure appears to present a graph comparing \"Data\" and \"Model\" using a log-likelihood metric plotted against a variable represented by the symbol ∇H(Z). \n\n- The **graph** shows two curves: a red curve for \"Data\" and a black curve for \"Model,\" which seems to be a fit or theoretical prediction.\n- The **vertical axis** is labeled as \"Log-Likelihood,\" indicating the graph is assessing how well the model fits the data.\n- The **horizontal axis** is marked with ∇H(Z), possibly representing a gradient or another mathematical transformation of variable Z.\n- The graph's shape, with a peak at the center, might suggest a distribution or the model's behavior around a critical point.\n\nThis figure likely plays an important role in illustrating how well the proposed model aligns with the empirical data, which is a critical aspect of validating the research findings or methodology.",
            "importance": 7
        }
    ],
    "metadata": {
        "key_themes": [
            "Spatial Meshing",
            "Neural Network Optimization",
            "Statistical modeling",
            "Super-Resolution",
            "Nonlinear Loss Function"
        ],
        "methodology": [
            "Neural Network Training",
            "3D Modeling",
            "Image Processing",
            "Feature detection",
            "Estimation"
        ],
        "domain": [
            "Autonomous Robotics",
            "Object removal",
            "Reflectance Imaging",
            "Bayesian Occam's razor"
        ],
        "strengths": [
            "3D Reconstruction",
            "DeepVision Framework",
            "Enhanced Detection Techniques",
            "Data Enhancement",
            "Superiority"
        ],
        "limitations": [
            "Training challenges",
            "3D Challenges",
            "Complexity",
            "Perceptual Color Balance",
            "Ground-truth ambiguity"
        ]
    },
    "content_chunks": [
        {
            "chunk_id": "2010.03592v1_chunk_0",
            "section": "methodology",
            "citations": [
                "34",
                "35",
                "36",
                "2",
                "3",
                "37",
                "38",
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_1",
            "section": "results",
            "citations": []
        },
        {
            "chunk_id": "2010.03592v1_chunk_2",
            "section": "discussion",
            "citations": [
                "1"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_3",
            "section": "conclusion",
            "citations": [
                "25",
                "26",
                "56",
                "63",
                "64"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_4",
            "section": "other",
            "citations": [
                "2",
                "3",
                "1",
                "4",
                "5",
                "6",
                "7",
                "4"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_5",
            "section": "other",
            "citations": [
                "8",
                "9",
                "10",
                "11",
                "4",
                "10",
                "9",
                "12",
                "13",
                "14",
                "15",
                "4",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "8",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_6",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "2010.03592v1_chunk_7",
            "section": "other",
            "citations": [
                "4",
                "9",
                "10",
                "13",
                "14",
                "15"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_8",
            "section": "other",
            "citations": [
                "38",
                "4",
                "10",
                "13",
                "14",
                "15",
                "4"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_9",
            "section": "other",
            "citations": [
                "15",
                "46",
                "47",
                "48",
                "49",
                "49"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_10",
            "section": "other",
            "citations": [
                "50"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_11",
            "section": "other",
            "citations": [
                "4",
                "4",
                "51"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_12",
            "section": "other",
            "citations": [
                "4",
                "51"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_13",
            "section": "other",
            "citations": [
                "41",
                "52",
                "53",
                "52",
                "54",
                "55"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_14",
            "section": "other",
            "citations": [
                "25",
                "56",
                "53"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_15",
            "section": "other",
            "citations": [
                "57",
                "58",
                "59",
                "60"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_16",
            "section": "other",
            "citations": [
                "4"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_17",
            "section": "other",
            "citations": [
                "61",
                "1,3,3,1"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_18",
            "section": "other",
            "citations": [
                "4",
                "4",
                "4"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_19",
            "section": "other",
            "citations": [
                "4",
                "10",
                "14",
                "13"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_20",
            "section": "other",
            "citations": [
                "4",
                "10",
                "14",
                "13",
                "15",
                "4",
                "10",
                "14",
                "15"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_21",
            "section": "other",
            "citations": [
                "2",
                "15",
                "62"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_22",
            "section": "other",
            "citations": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28",
                "29",
                "30",
                "31",
                "32",
                "33",
                "34",
                "35",
                "36",
                "37",
                "38",
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45",
                "46",
                "47",
                "48",
                "49",
                "50",
                "51"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_23",
            "section": "other",
            "citations": [
                "52",
                "53",
                "54",
                "55",
                "56",
                "57",
                "58",
                "59",
                "60",
                "61",
                "62",
                "63",
                "64",
                "65",
                "66",
                "67"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_24",
            "section": "other",
            "citations": [
                "49",
                "50",
                "65",
                "65"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_25",
            "section": "other",
            "citations": [
                "25",
                "66",
                "67",
                "5"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_26",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "2010.03592v1_chunk_27",
            "section": "other",
            "citations": [
                "4",
                "4"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_28",
            "section": "other",
            "citations": [
                "25",
                "25"
            ]
        },
        {
            "chunk_id": "2010.03592v1_chunk_29",
            "section": "other",
            "citations": [
                "4",
                "4"
            ]
        }
    ]
}