{
    "basic_info": {
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "authors": [
            "Sergey Ioffe",
            "Christian Szegedy"
        ],
        "paper_id": "1502.03167v3",
        "published_year": 2015,
        "references": []
    },
    "detailed_references": {
        "ref_2011": {
            "text": "ISSN1532-4435.\nG¨ ulc ¸ehre, C ¸aglar and Bengio, Yoshua. Knowledge mat-\nters: Importanceof prior informationfor optimization.\nCoRR,abs/1301.4083,2013.\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving Deep\ninto Rectiﬁers: Surpassing Human-Level Performance\non ImageNet Classiﬁcation. ArXiv e-prints , February",
            "type": "numeric",
            "number": "2011",
            "arxiv_id": null
        },
        "ref_2015": {
            "text": "Hyv¨ arinen, A. and Oja, E. Independentcomponent anal-\nysis: Algorithms and applications. Neural Netw. , 13\n(4-5):411–430,May2000.\nJiang, Jing. A literature survey on domain adaptation of\nstatistical classiﬁers, 2008.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.\nGradient-based learning applied to document recog-\nnition.Proceedings of the IEEE , 86(11):2278–2324,\nNovember1998a.\nLeCun, Y., Bottou, L., Orr, G., and Muller, K. Efﬁcient\nbackprop. InOrr,G.andK.,Muller(eds.), NeuralNet-\nworks: Tricks ofthetrade .Springer,1998b.\nLyu, S and Simoncelli, E P. Nonlinear image representa-\ntion using divisive normalization. In Proc. Computer\nVision and Pattern Recognition , pp. 1–8. IEEE Com-\nputer Society, Jun 23-28 2008. doi: 10.1109/CVPR.",
            "type": "numeric",
            "number": "2015",
            "arxiv_id": "10.1109/CVPR."
        },
        "ref_2014": {
            "text": "Sutskever, Ilya, Martens, James, Dahl, George E., and\nHinton, Geoffrey E. On the importance of initial-\nization and momentum in deep learning. In ICML\n(3), volume 28 of JMLR Proceedings , pp. 1139–1147.\nJMLR.org,2013.\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,\nPierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-\nmitru, Vanhoucke, Vincent, and Rabinovich, An-\ndrew. Going deeper with convolutions. CoRR,\nabs/1409.4842,2014.\nWiesler, Simon and Ney, Hermann. A convergenceanal-\nysis of log-lineartraining. In Shawe-Taylor,J., Zemel,\nR.S.,Bartlett,P.,Pereira,F.C.N.,andWeinberger,K.Q.\n(eds.),AdvancesinNeuralInformationProcessingSys-\ntems24,pp.657–665,Granada,Spain,December2011.\nWiesler, Simon, Richard, Alexander, Schl¨ uter, Ralf, and\nNey, Hermann. Mean-normalized stochastic gradient\nfor large-scale deep learning. In IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning,pp.180–184,Florence,Italy,May2014.\nWu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and\nSun,Gang. Deepimage: Scalingupimagerecognition,",
            "type": "numeric",
            "number": "2014",
            "arxiv_id": null
        },
        "ref_July_2011": {
            "text": "8optimization. J.Mach.Learn.Res. ,12:2121–2159,July 2011. ISSN1532-4435.",
            "type": "author_year",
            "key": "July, 2011",
            "arxiv_id": null
        },
        "ref_February_2015": {
            "text": "on ImageNet Classiﬁcation. ArXiv e-prints , February 2015.",
            "type": "author_year",
            "key": "February, 2015",
            "arxiv_id": null
        },
        "ref_January_2014": {
            "text": "ting.J. Mach. Learn. Res. , 15(1):1929–1958, January 2014.",
            "type": "author_year",
            "key": "January, 2014",
            "arxiv_id": null
        },
        "ref_Scalingupimagerecognition_2015": {
            "text": "Sun,Gang. Deepimage: Scalingupimagerecognition, 2015.",
            "type": "author_year",
            "key": "Scalingupimagerecognition, 2015",
            "arxiv_id": null
        }
    },
    "raw_chunks": [
        {
            "text": "1 Introduction\n\nℓ = F2(F1(u,Θ1),Θ2)\n\nDeep learning has dramatically advanced the state of the art in vision, speech, and many other areas. Stochas- tic gradient descent (SGD) has proved to be an effec- tive way of training deep networks, and SGD variants such as momentum (Sutskever et al., 2013) and Adagrad (Duchi et al., 2011) have been used to achieve state of the art performance. SGD optimizes the parameters Θ of the network, so as to minimize the loss\n\nwhere F1 and F2 are arbitrary transformations, and the parameters Θ1,Θ2 are to be learned so as to minimize the loss ℓ. Learning Θ2 can be viewed as if the inputs x = F1(u,Θ1) are fed into the sub-network\n\nℓ = F2(x,Θ2).\n\nFor example, a gradient descent step\n\nΘ = argmin Θ 1 N N Xi=1 ℓ(xi,Θ)\n\nΘ2 ← Θ2 − α m m Xi=1 ∂F2(xi,Θ2) ∂Θ2\n\nwhere x1...N is the training data set. With SGD, the train- ing proceeds in steps, and at each step we consider a mini- batch x1...m of size m. The mini-batch is used to approx- imate the gradient of the loss function with respect to the parameters, by computing\n\n1 m ∂ℓ(xi,Θ) ∂Θ .\n\n(forbatch size m and learning rate α) is exactly equivalent to that for a stand-alone network F2 with input x. There- fore, the input distribution properties that make training more efﬁcient – such as having the same distribution be- tween the training and test data – apply to training the sub-network as well. As such it is advantageous for the distribution of x to remain ﬁxed over time. Then, Θ2 does\n\n1\n\nnot have to readjust to compensate for the change in the distribution of x.\n\nFixed distribution of inputs to a sub-network would have positive consequences for the layers outside the sub- network, as well. Consider a layer with a sigmoid activa- tion function z = g(Wu + b) where u is the layer input, the weight matrix W and bias vector b are the layer pa- 1 rameters to be learned, and g(x) = 1+exp(−x). As increases, g′(x) tends to zero. This means that for all di- | x mensions of x = Wu+b except those with small absolute values, the gradient ﬂowing down to u will vanish and the model will train slowly. However, since x is affected by W,b and the parameters of all the layers below, changes to those parameters during training will likely move many dimensions of x into the saturated regime of the nonlin- earity and slow down the convergence. This effect is ampliﬁed as the network depth increases. In practice, the saturation problem and the resulting vanishing gradi- ents are usually addressed by using Rectiﬁed Linear Units (Nair & Hinton, 2010) ReLU(x) = max(x,0), careful initialization (Bengio & Glorot, 2010; Saxe et al., 2013), and small learning rates. If, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate. |\n\nWe refer to the change in the distributions of internal nodes of a deep network, in the course of training, as In- ternal Covariate Shift. Eliminating it offers a promise of faster training. We propose a new mechanism, which we call Batch Normalization, that takes a step towards re- ducing internal covariate shift, and in doing so dramati- cally accelerates the training of deep neural nets. It ac- complishes this via a normalization step that ﬁxes the means and variances of layer inputs. Batch Normalization also has a beneﬁcial effect on the gradient ﬂow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates with- out the risk of divergence. Furthermore, batch normal- ization regularizes the model and reduces the need for Dropout (Srivastava et al., 2014). Finally, Batch Normal- ization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the satu- rated modes.\n\nIn Sec. 4.2, we apply Batch Normalization to the best- performing ImageNet classiﬁcation network, and show that we can match its performance using only 7% of the training steps, and can further exceed its accuracy by a substantial margin. Using an ensemble of such networks trained with Batch Normalization, we achieve the top-5 error rate that improves upon the best known results on ImageNet classiﬁcation.\n\n2",
            "section": "introduction",
            "section_idx": 0,
            "citations": []
        },
        {
            "text": "4.2.3 Ensemble Classiﬁcation\n\nThe current reported best results on the ImageNet Large Scale Visual Recognition Competition are reached by the Deep Image ensemble of traditional models (Wu et al., 2015) and the ensemble model of (He et al., 2015). The latter reports the top-5 error of 4.94%, as evaluated by the ILSVRC server. Here we reporta top-5 validation error of 4.9%, and test error of 4.82% (according to the ILSVRC server). This improves upon the previous best result, and exceeds the estimated accuracy of human raters according to (Russakovsky et al., 2014).\n\nFor our ensemble, we used 6 networks. Each was based onBN-x30,modiﬁedvia someof thefollowing: increased initial weights in the convolutional layers; using Dropout (with the Dropout probability of 5% or 10%, vs. 40% for the original Inception); and using non-convolutional, per-activation Batch Normalization with last hidden lay- ers of the model. Each network achieved its maximum 106 training steps. The ensemble accuracy after about 6 · prediction was based on the arithmetic average of class probabilities predicted by the constituent networks. The details of ensemble and multicrop inference are similar to (Szegedy et al., 2014).\n\nSep\n\n201\n\nWe demonstrate in Fig. 4 that batch normalization al- lows us to set new state-of-the-art by a healthy margin on the ImageNet classiﬁcation challenge benchmarks.\n\n5 Conclusion\n\nWe have presented a novel mechanism for dramatically accelerating the training of deep networks. It is based on the premise that covariate shift, which is known to com- plicate the training of machine learning systems, also ap-\n\nModel GoogLeNet ensemble 224 144 7 - Deep Image low-res 256 - 1 - Deep Image high-res 512 - 1 24.88 Deep Image ensemble variable - - - BN-Inception single crop 224 1 1 25.2% BN-Inception multicrop 224 144 1 21.99% BN-Inception ensemble 224 144 6 20.1%\n\nResolution Crops Models Top-1 error Top-5 error\n\n6.67%\n\n7.96%\n\n7.42%\n\n5.98%\n\n7.82%\n\n5.82%\n\n4.9%*\n\nFigure 4: Batch-Normalized Inception comparison with previous state of the art on the provided validation set com- prising 50000 images. *BN-Inception ensemble has reached 4.82% top-5 error on the 100000 images of the test set of the ImageNet as reported by the test server.\n\nplies to sub-networks and layers, and removing it from internal activations of the network may aid in training. Our proposed method draws its power from normalizing activations, and from incorporating this normalization in the network architecture itself. This ensures that the nor- malization is appropriately handled by any optimization method that is being used to train the network. To en- able stochastic optimization methods commonly used in deep network training, we perform the normalization for each mini-batch, and backpropagatethe gradients through the normalization parameters. Batch Normalization adds only two extra parameters per activation, and in doing so preserves the representation ability of the network. We presented an algorithm for constructing, training, and per- forming inference with batch-normalized networks. The resulting networks can be trained with saturating nonlin- earities, are more tolerant to increased training rates, and often do not require Dropout for regularization.\n\nMerely adding Batch Normalization to a state-of-the- art imageclassiﬁcation modelyields a substantialspeedup in training. By further increasing the learning rates, re- moving Dropout, and applying other modiﬁcations af- forded by Batch Normalization, we reach the previous state of the art with only a small fraction of training steps – and then beat the state of the art in single-network image classiﬁcation. Furthermore, by combining multiple mod- els trained with Batch Normalization, we perform better than the best known system on ImageNet, by a signiﬁcant margin.\n\nInterestingly, our method bears similarity to the stan- dardization layer of (G¨ulc¸ehre & Bengio, 2013), though the two methods stem from very different goals, and per- form different tasks. The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the ﬁrst and second moments is more likely to result in a stable distribution. On the contrary, (G¨ulc¸ehre & Bengio, 2013) apply the standardization layer to the output of the nonlinearity, which results in sparser activations. In our large-scale image classiﬁcation experiments, we have not observed the nonlinearity inputs to be sparse, neither with nor without Batch Normalization. Other notable differ-\n\nentiating characteristics of Batch Normalization include the learned scale and shift that allow the BN transform to represent identity (the standardization layer did not re- quirethis since it was followed by the learned linear trans- form that, conceptually, absorbs the necessary scale and shift), handling of convolutional layers, deterministic in- ferencethat does not dependon the mini-batch,and batch- normalizing each convolutional layer in the network.\n\nIn this work, we have not explored the full range of possibilities that Batch Normalization potentially enables. Our future work includes applications of our method to Recurrent Neural Networks (Pascanu et al., 2013), where the internal covariate shift and the vanishing or exploding gradients may be especially severe, and which would al- low us to morethoroughlytest the hypothesisthat normal- ization improves gradient propagation (Sec. 3.3). We plan to investigate whether Batch Normalization can help with domain adaptation, in its traditional sense – i.e. whether the normalization performed by the network would al- low it to more easily generalize to new data distribu- tions, perhaps with just a recomputation of the population means and variances (Alg. 2). Finally, we believe that fur- ther theoretical analysis of the algorithm would allow still more improvements and applications.",
            "section": "results",
            "section_idx": 0,
            "citations": []
        },
        {
            "text": "5\n\n1 0 2 r a M 2 ] G L . s c [ 3 v 7 6 1 3 0 . 2 0 5 1 : v\n\ni\n\nX\n\nr\n\na\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n\nSergey Ioffe Google Inc., sioffe@google.com\n\nChristian Szegedy Google Inc., szegedy@google.com\n\nAbstract\n\nTraining Deep Neural Networks is complicatedby the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameterinitialization, and makes it no- toriously hard to train models with saturating nonlineari- ties. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer in- puts. Our method draws its strength from making normal- ization a part of the modelarchitectureand performingthe normalization for each training mini-batch. Batch Nor- malization allows us to use much higherlearningrates and be less careful about initialization. It also acts as a regu- larizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classiﬁcation model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a signiﬁcant margin. Using an ensemble of batch- normalizednetworks,we improveupon thebest published result on ImageNet classiﬁcation: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the ac- curacy of human raters.\n\nUsing mini-batchesof examples, as opposed to one exam- ple at a time, is helpful in several ways. First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. Second, computation over a batch can be much more efﬁcient than m computations for individual examples, due to the parallelism afforded by the modern computing platforms.\n\nWhile stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, speciﬁcally the learning rate used in optimization, as well as the initial values for the model parameters. The train- ing is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers – so that small changes to the network parameters amplify as the network becomes deeper.\n\nThe change in the distributions of layers’ inputs presents a problem because the layers need to continu- ously adapt to the new distribution. When the input dis- tribution to a learning system changes, it is said to experi- ence covariate shift (Shimodaira, 2000). This is typically handled via domain adaptation (Jiang, 2008). However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply to its parts, such as a sub-network or a layer. Consider a network computing",
            "section": "other",
            "section_idx": 0,
            "citations": []
        },
        {
            "text": "2 Towards\n\nReducing\n\nInternal\n\nCovariate Shift\n\nWe deﬁne Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. To improve the train- ing, we seek to reduce the internal covariate shift. By ﬁxing the distribution of the layer inputs x as the training progresses,we expectto improvethe trainingspeed. It has been long known (LeCun et al., 1998b; Wiesler & Ney, 2011) that the network training converges faster if its in- puts are whitened – i.e., linearly transformed to have zero means and unit variances, and decorrelated. As each layer observesthe inputs producedby the layers below, it would be advantageous to achieve the same whitening of the in- puts of each layer. By whitening the inputs to each layer, we would take a step towards achieving the ﬁxed distri- butions of inputs that would remove the ill effects of the internal covariate shift.\n\nWe could consider whitening activations at every train- ing step or at some interval, either by modifying the network directly or by changing the parameters of the optimization algorithm to depend on the network ac- tivation values (Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins & Kavukcuoglu). How- ever, if these modiﬁcations are interspersed with the op- timization steps, then the gradient descent step may at- tempt to update the parameters in a way that requires the normalization to be updated, which reduces the ef- fect of the gradient step. For example, consider a layer with the input u that adds the learned bias b, and normal- izes the result by subtracting the mean of the activation computed over the training data: x = x E[x] where − is the set of values of x over x = u + b, = x1...N} X { b N the training set, and E[x] = 1 If a gradient i=1 xi. N descent step ignores the dependence of E[x] on b, then it P will update b b + ∆b, where ∆b x. Then ∂ℓ/∂ ← ∝ − E[u + (b + ∆b)] = u + b E[u + b]. u + (b + ∆b) − − b Thus, the combination of the update to b and subsequent change in normalization led to no change in the output of the layer nor, consequently, the loss. As the training continues, b will grow indeﬁnitely while the loss remains ﬁxed. This problem can get worse if the normalization not only centers but also scales the activations. We have ob- served this empirically in initial experiments, where the model blows up when the normalization parameters are\n\ncomputed outside the gradient descent step.\n\nThe issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place. To address this issue, we would like to ensure that, for any parameter values, the network always produces activations with the desired distribution. Doing so would allow the gradient of the loss with respect to the model parameters to account for the normalization, and for its dependence on the model parameters Θ. Let again x be a layer input, treated as a\n\nvector, and be the set of these inputs over the training X data set. The normalization can then be written as a trans- formation\n\nx = Norm(x, )",
            "section": "other",
            "section_idx": 1,
            "citations": []
        },
        {
            "text": "X\n\nwhich depends not only on the given training example x b but on all examples – each of which depends on Θ if X x is generated by another layer. For backpropagation, we would need to compute the Jacobians\n\n∂Norm(x, X ∂x ) and ∂Norm(x, X ∂ ) ;\n\nX\n\nignoring the latter term would lead to the explosion de- scribed above. Within this framework,whitening the layer inputs is expensive, as it requires computing the covari- ance matrix Cov[x] = Ex∈X[xxT] E[x]E[x]T and its − inverse square root, to produce the whitened activations Cov[x]−1/2(x E[x]), as well as the derivatives of these − transforms for backpropagation. This motivates us to seek an alternative that performs input normalization in a way that is differentiable and does not require the analysis of the entire training set after every parameter update.\n\nSome of the previous approaches (e.g. (Lyu & Simoncelli, 2008)) use statistics computed over a single training example, or, in the case of image networks, over different feature maps at a given location. However, this changes the representation ability of a network by discarding the absolute scale of activations. We want to a preserve the information in the network, by normalizing the activations in a training example relative to the statistics of the entire training data.\n\n3 Normalization via Mini-Batch Statistics\n\nSince the full whitening of each layer’s inputs is costly and not everywhere differentiable, we make two neces- sary simpliﬁcations. The ﬁrst is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently,by making it have the mean of zero and the variance of 1. For a layer with d-dimensional input x = (x(1) ...x(d)), we will nor- malize each dimension\n\nx(k) = x(k) E[x(k)] − Var[x(k)]\n\np\n\nb\n\nwhere the expectation and variance are computed over the training data set. As shown in (LeCun et al., 1998b), such normalization speeds up convergence, even when the fea- tures are not decorrelated.\n\nNote that simply normalizing each input of a layer may change what the layer can represent. For instance, nor- malizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address this, we make sure that the transformation inserted in the network can represent the identity transform. To accomplish this,\n\nweintroduce,foreach activationx(k), a pairofparameters γ(k),β(k), which scale and shift the normalized value:\n\ny(k) = γ(k) x(k) + β(k).\n\nThese parameters are learned along with the original b model parameters, and restore the representation power of the network. Indeed, by setting γ(k) = Var[x(k)] and β(k) = E[x(k)], we could recover the original activations, p if that were the optimal thing to do.\n\nIn the batch setting where each training step is based on the entire training set, we would use the whole set to nor- malize activations. However, this is impractical when us- ing stochastic optimization. Therefore, we make the sec- ond simpliﬁcation: since we use mini-batches in stochas- tic gradient training, each mini-batch produces estimates of the mean and variance of each activation. This way, the statistics used for normalization can fully participate in the gradient backpropagation. Note that the use of mini- batches is enabled by computation of per-dimension vari- ances rather than joint covariances; in the joint case, reg- ularization would be required since the mini-batch size is likely to be smaller than the number of activations being whitened, resulting in singular covariance matrices.\n\nConsider a mini-batch of size m. Since the normal- B ization is applied to each activation independently, let us focus on a particular activation x(k) and omit k for clarity. We have m values of this activation in the mini-batch,\n\n= .\n\nx1...m}",
            "section": "other",
            "section_idx": 2,
            "citations": []
        },
        {
            "text": "B\n\n{\n\nLet the normalized values be x1...m, and their linear trans- formations be y1...m. We refer to the transform\n\nb\n\nBNγ,β : x1...m →\n\ny1...m\n\nas the Batch Normalizing Transform. We present the BN Transformin Algorithm1. In the algorithm,ǫ is a constant added to the mini-batch variance for numerical stability.\n\nInput: Values of x over a mini-batch: ; x1...m} = { B Parameters to be learned: γ, β Output: yi = BNγ,β(xi) } { m 1 // mini-batch mean xi µB m ← Xi=1 m 1 µB)2 σ2 // mini-batch variance (xi − B m ← Xi=1 µB xi − // normalize xi ← σ2 B + ǫ p b // scale and shift BNγ,β(xi) γ yi ← xi + β ≡\n\nb\n\nAlgorithm 1: Batch Normalizing Transform, applied to activation x over a mini-batch.\n\nThe BN transform can be added to a network to manip- ulate any activation. In the notation y = BNγ,β(x), we\n\n3\n\nindicate that the parameters γ and β are to be learned,\n\nbut it should be noted that the BN transform does not independently process the activation in each training ex- ample. Rather, BNγ,β(x) depends both on the training example and the other examples in the mini-batch. The scaled and shifted values y are passed to other network layers. The normalized activations x are internal to our transformation, but their presence is crucial. The distri- butions of values of any b x has the expected value of 0 and the variance of 1, as long as the elements of each b mini-batch are sampled from the same distribution, and if we neglect ǫ. This can be seen by observing that m m xi = 0 and 1 x2 i = 1, and taking expec- i=1 i=1 m x(k) can be viewed as tations. Each normalized activation P P b b an input to a sub-network composed of the linear trans- form y(k) = γ(k) b x(k) + β(k), followed by the other pro- cessing done by the original network. These sub-network b inputs all have ﬁxed means and variances, and although the joint distribution of these normalized x(k) can change over the course of training, we expect that the introduc- b tion of normalized inputs accelerates the training of the sub-network and, consequently, the network as a whole.\n\nDuring training we need to backpropagate the gradi- ent of loss ℓ through this transformation, as well as com- pute the gradients with respect to the parameters of the BN transform. We use chain rule, as follows (before sim- pliﬁcation):\n\nae _ ae. ak _ vm ae . =1/,2 —3/2 Bom = Lint Bay’ (ti — Ms) (OB + €)*/ oe _ ym of 1 4 06, hy —2(@:i-HB) due i=1 Oa; ae daz ™ Ok _ dol 1 (aie Hs + ar = Dar” Torre f + one =v: i=1 Oy: op",
            "section": "other",
            "section_idx": 3,
            "citations": []
        },
        {
            "text": "P\n\nThus, BN transform is a differentiable transformation that introduces normalized activations into the network. This ensures that as the model is training, layers can continue learningon inputdistributionsthat exhibitless internalco- variate shift, thus accelerating the training. Furthermore, the learned afﬁne transform applied to these normalized activations allows the BN transform to represent the iden- tity transformation and preserves the network capacity.\n\n3.1 Training and Inference with Batch- Normalized Networks\n\nTo Batch-Normalize a network, we specify a subset of ac- tivations and insert the BN transform for each of them, according to Alg. 1. Any layer that previously received x as the input, now receives BN(x). A model employing Batch Normalization can be trained using batch gradient descent, or Stochastic Gradient Descent with a mini-batch size m > 1, or with any of its variants such as Adagrad\n\n(Duchi et al., 2011). The normalization of activations that depends on the mini-batch allows efﬁcient training, but is neither necessary nor desirable during inference; we want the output to depend only on the input, deterministically. For this, once the network has been trained, we use the normalization\n\nx = E[x] x − Var[x] + ǫ\n\np\n\nb\n\nusing the population, rather than mini-batch, statistics. Neglecting ǫ, these normalized activations have the same mean 0 and variance 1 as during training. We use the un- biased variance estimate Var[x] = m EB[σ2 B], where m−1 · theexpectationis overtraining mini-batchesofsize m and σ2 B are their sample variances. Using moving averages in- stead, we can track the accuracy of a model as it trains. Since the means and variances are ﬁxed during inference, the normalization is simply a linear transform applied to each activation. It may further be composed with the scal- ing by γ and shift by β, to yield a single linear transform that replaces BN(x). Algorithm 2 summarizes the proce- dure for training batch-normalized networks.\n\nInput: Network N with trainable parameters O; subset of activations {a} Output: Batch-normalized network for inference, N24, 1: Ngx <— N_ // Training BN network 2: fork =1...K do Add transformation y“) = BN) gc) (x 0K)) sn (Alg. 1) Modify each layer in Nf with input x”) to take y) instead 5: end for 6: Train Ngy to optimize the parameters © (9), 80}, 7: Net < Ngy_ // Inference BN network with frozen // parameters\n\n3:\n\n4:\n\n8: fork =1...K do // For clarity, 2 = 2), y¥ =, we = nw, etc. Process multiple training mini-batches B, each of size m, and average over them: E[z] — Es[us] Var[x] — 4 Eg(o3]\n\n9:\n\n10:",
            "section": "other",
            "section_idx": 4,
            "citations": []
        },
        {
            "text": "ll:\n\nIn N3X, replace the transform y = BN,,g(a) with = ~L.-r+(B- Ele] ¥ a/Var[x]+e 7 ( at)\n\n12: end for\n\nAlgorithm 2: Training a Batch-Normalized Network\n\n3.2 Batch-Normalized Convolutional Net- works\n\nBatch Normalization can be applied to any set of acti- vations in the network. Here, we focus on transforms\n\n4\n\nto\n\nU\n\nthat consist of an afﬁne transformation followed by an element-wise nonlinearity:\n\nz = g(Wu + b)\n\nwhere W and b are learned parameters of the model, and ) is the nonlinearity such as sigmoid or ReLU. This for- g( · mulation covers both fully-connected and convolutional layers. We add the BN transform immediately before the nonlinearity, by normalizing x = Wu+b. We could have also normalized the layer inputs u, but since u is likely the output of another nonlinearity, the shape of its distri- butionis likely to changeduringtraining,and constraining its ﬁrst and second moments would not eliminate the co- variate shift. In contrast, Wu + b is more likely to have a symmetric, non-sparse distribution, that is “more Gaus- sian” (Hyv¨arinen & Oja, 2000); normalizing it is likely to produce activations with a stable distribution.\n\nNote that, since we normalizeWu+b, the bias b can be ignored since its effect will be canceled by the subsequent mean subtraction (the role of the bias is subsumed by β in Alg. 1). Thus, z = g(Wu + b) is replaced with\n\nz = g(BN(Wu))\n\nwhere the BN transform is applied independently to each dimension of x = Wu, with a separate pair of learned parameters γ(k), β(k) per dimension.\n\nFor convolutional layers, we additionally want the nor- malization to obey the convolutional property – so that different elements of the same feature map, at different locations, are normalized in the same way. To achieve this, we jointly normalize all the activations in a mini- batch, over all locations. In Alg. 1, we let be the set of B all values in a feature map across both the elements of a mini-batch and spatial locations – so for a mini-batch of size m and feature maps of size p q, we use the effec- × tive mini-batch of size m′ = pq. We learn a = m |B| · pair of parameters γ(k) and β(k) per feature map, rather than per activation. Alg. 2 is modiﬁed similarly, so that during inference the BN transform applies the same linear transformation to each activation in a given feature map.",
            "section": "other",
            "section_idx": 5,
            "citations": []
        },
        {
            "text": "3.3 Batch Normalization enables higher learning rates\n\nIn traditional deep networks, too-high learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. Batch Normaliza- tion helps address these issues. By normalizing activa- tions throughout the network, it prevents small changes to the parameters from amplifying into larger and subop- timal changes in activations in gradients; for instance, it prevents the training from getting stuck in the saturated regimes of nonlinearities.\n\nBatch Normalization also makes training more resilient to the parameter scale. Normally, large learning rates may increase the scale of layer parameters, which then amplify\n\n5\n\nthe gradientduring backpropagationand lead to the model explosion. However, with Batch Normalization, back- propagation through a layer is unaffected by the scale of its parameters. Indeed, for a scalar a,\n\nBN(Wu) = BN((aW)u)\n\nand we can show that\n\n∂BN((aW)u) ∂u = ∂BN(Wu) ∂u ∂BN((aW)u) ∂(aW) = 1 ∂BN(Wu) ∂W\n\na ·\n\nThe scale does not affect the layer Jacobian nor, con- sequently, the gradient propagation. Moreover, larger weights lead to smaller gradients, and Batch Normaliza- tion will stabilize the parameter growth.\n\nWe further conjecture that Batch Normalization may lead the layer Jacobians to have singular values close to 1, which is known to be beneﬁcial for training (Saxe et al., 2013). Consider two consecutive layers with normalized inputs, and the transformation between these normalized vectors: z = F( x). Ifwe assume that x and z are Gaussian and uncorrelated, and that F( x is a linear transfor- x) J ≈ b b b b mation for the given model parameters, then both x and z b b x]JT = have unit covariances, and I = Cov[ z] = JCov[ b b JJT. Thus, JJT = I, and so all singular values of J b b are equal to 1, which preserves the gradient magnitudes during backpropagation. In reality, the transformation is not linear, and the normalizedvalues are not guaranteedto be Gaussian nor independent, but we nevertheless expect Batch Normalization to help make gradient propagation better behaved. The precise effect of Batch Normaliza- tion on gradient propagation remains an area of further study.",
            "section": "other",
            "section_idx": 6,
            "citations": []
        },
        {
            "text": "3.4 Batch Normalization regularizes the model\n\nWhen training with Batch Normalization, a training ex- ample is seen in conjunction with other examples in the mini-batch, and the training network no longer produc- ing deterministic values for a given training example. In our experiments, we found this effect to be advantageous to the generalization of the network. Whereas Dropout (Srivastava et al., 2014) is typically used to reduce over- ﬁtting, in a batch-normalizednetwork we found that it can be either removed or reduced in strength.\n\n4 Experiments\n\n4.1 Activations over time\n\nTo verify the effects of internal covariate shift on train- ing, and the ability of Batch Normalization to combat it, we considered the problem of predicting the digit class on the MNIST dataset (LeCun et al., 1998a). We used a very simple network, with a 28x28 binary image as input, and\n\n1 2 2 0.9 0.8 Without BN 0 0 With BN 0.7 10K 20K 30K 40K 50K −2 −2 (a) (b) Without BN (c) With BN\n\nFigure 1: (a) The test accuracy of the MNIST network trained with and without Batch Normalization, vs. the number of training steps. Batch Normalization helps the network train faster and achieve higher accuracy. (b, c) The evolution of input distributions to a typical sig- moid, over the course of training, shown as 15,50,85 { percentiles. Batch Normalization makes the distribution } th more stable and reduces the internal covariate shift.\n\n3 fully-connectedhidden layers with 100 activations each. Each hidden layercomputesy = g(Wu+b) with sigmoid nonlinearity, and the weights W initialized to small ran- dom Gaussian values. The last hidden layer is followed by a fully-connected layer with 10 activations (one per class) and cross-entropy loss. We trained the network for 50000 steps, with 60 examples per mini-batch. We added Batch Normalization to each hidden layer of the network, as in Sec. 3.1. We were interested in the comparison be- tween the baseline and batch-normalized networks, rather than achieving the state of the art performance on MNIST (which the described architecture does not).\n\nFigure 1(a) shows the fraction of correct predictions by the two networks on held-out test data, as training progresses. The batch-normalized network enjoys the higher test accuracy. To investigate why, we studied in- puts to the sigmoid, in the original network Nand batch- normalizednetworkNtr BN (Alg. 2) overthe course of train- ing. In Fig. 1(b,c)we show, foronetypicalactivation from the last hidden layer of each network, how its distribu- tion evolves. The distributions in the original network change signiﬁcantly over time, both in their mean and the variance, which complicates the training of the sub- sequent layers. In contrast, the distributions in the batch- normalized network are much more stable as training pro- gresses, which aids the training.",
            "section": "other",
            "section_idx": 7,
            "citations": []
        },
        {
            "text": "4.2 ImageNet classiﬁcation\n\nWe applied Batch Normalization to a new variant of the Inception network (Szegedy et al., 2014), trained on the ImageNet classiﬁcation task (Russakovsky et al., 2014). The network has a large number of convolutional and pooling layers, with a softmax layer to predict the image class, out of 1000 possibilities. Convolutional layers use ReLU as the nonlinearity. The main difference to the net- work described in (Szegedy et al., 2014) is that the 5 convolutional layers are replaced by two consecutive lay- × 5 ers of 3 3 convolutions with up to 128 ﬁlters. The net- × 106 parameters, and, other than the work contains 13.6 · top softmax layer, has no fully-connected layers. More\n\n6\n\ndetails are given in the Appendix. We refer to this model as Inceptionin the rest of the text. The model was trained using a version of Stochastic Gradient Descent with mo- mentum(Sutskever et al., 2013), using the mini-batchsize of32. Thetrainingwas performedusing a large-scale,dis- tributed architecture (similar to (Dean et al., 2012)). All networks are evaluated as training progresses by comput- ing the validation accuracy @1, i.e. the probability of predicting the correct label out of 1000 possibilities, on a held-out set, using a single crop per image.\n\nIn our experiments, we evaluated several modiﬁcations ofInception with Batch Normalization. In all cases, Batch Normalization was applied to the input of each nonlinear- ity, in a convolutional way, as described in section 3.2, while keeping the rest of the architecture constant.\n\n4.2.1 Accelerating BN Networks\n\nSimply adding Batch Normalization to a network does not take full advantage of our method. To do so, we further changed the network and its training parameters, as fol- lows:\n\nIncrease learning rate. In a batch-normalized model, we have been able to achieve a training speedup from higher learning rates, with no ill side effects (Sec. 3.3).\n\nRemove Dropout. As described in Sec. 3.4, Batch Nor- malization fulﬁlls some of the same goals as Dropout. Re- moving Dropout from Modiﬁed BN-Inception speeds up training, without increasing overﬁtting.\n\nReduce the L2 weight regularization. While in Incep- tion an L2 loss on the model parameters controls overﬁt- ting, in Modiﬁed BN-Inception the weight of this loss is reduced by a factor of 5. We ﬁnd that this improves the accuracy on the held-out validation data.\n\nAccelerate the learning rate decay. In training Incep- tion, learning rate was decayed exponentially. Because our network trains faster than Inception, we lower the learning rate 6 times faster.\n\nRemove Local Response Normalization While Incep- tion and other networks (Srivastava et al., 2014) beneﬁt from it, we found that with Batch Normalization it is not necessary.\n\nShufﬂetraining examples more thoroughly. We enabled within-shard shufﬂing of the training data, which prevents the same examples from always appearing in a mini-batch together. This led to about 1% improvements in the val- idation accuracy, which is consistent with the view of Batch Normalization as a regularizer (Sec. 3.4): the ran- domization inherent in our method should be most bene- ﬁcial when it affects an example differently each time it is seen.\n\nReduce the photometric distortions. Because batch- normalized networks train faster and observe each train- ing example fewer times, we let the trainer focus on more “real” images by distorting them less.\n\n0.8 0.7 0.6 Inception BN−Baseline 0.5 BN−x5 BN−x30 BN−x5−Sigmoid Steps to match Inception 0.4 5M 10M 15M 20M 25M 30M\n\nFigure 2: Single crop validation accuracy of Inception and its batch-normalized variants, vs. the number of training steps.",
            "section": "other",
            "section_idx": 8,
            "citations": []
        },
        {
            "text": "4.2.2 Single-Network Classiﬁcation\n\nWe evaluated the following networks, all trained on the LSVRC2012 training data, and tested on the validation data:\n\nInception: the network described at the beginning of Section 4.2, trained with the initial learningrate of0.0015.\n\nBN-Baseline: Same as Inception with Batch Normal- ization before each nonlinearity.\n\nBN-x5: Inception with Batch Normalization and the modiﬁcations in Sec. 4.2.1. The initial learning rate was increased by a factor of 5, to 0.0075. The same learning rate increase with original Inception caused the model pa- rameters to reach machine inﬁnity.\n\nBN-x30: Like BN-x5, but with the initial learning rate 0.045 (30 times that of Inception).\n\nBN-x5-Sigmoid: Like BN-x5, but with sigmoid non- linearity g(t) = 1 1+exp(−x) instead of ReLU. We also at- tempted to train the original Inception with sigmoid, but the model remained at the accuracy equivalent to chance.\n\nIn Figure 2, we show the validation accuracy of the networks, as a function of the number of training steps. Inception reached the accuracy of 72.2% after 31 training steps. The Figure 3 shows, for each network, · 106 the number of training steps required to reach the same 72.2% accuracy, as well as the maximum validation accu- racy reached by the network and the number of steps to reach it.\n\nBy only using Batch Normalization (BN-Baseline), we match the accuracy of Inception in less than half the num- ber of training steps. By applying the modiﬁcations in Sec. 4.2.1, we signiﬁcantly increase the training speed of the network. BN-x5needs 14 times fewer steps than In- ception to reach the 72.2% accuracy. Interestingly, in- creasing the learning rate further (BN-x30) causes the model to train somewhat slower initially, but allows it to reach a higherﬁnal accuracy. It reaches 74.8%after 6 steps, i.e. 5 times fewer steps than required by Inception · 106 to reach 72.2%.\n\nWe also veriﬁed that the reduction in internal covari- ate shift allows deep networks with Batch Normalization\n\n7\n\nModel Steps to 72.2% Max accuracy 106 Inception 72.2% 31.0 · 106 BN-Baseline 72.7% 13.3 · 106 BN-x5 73.0% 2.1 · 106 74.8% BN-x30 2.7 · 69.8% BN-x5-Sigmoid\n\nFigure 3: For Inception and the batch-normalized variants, the number of training steps required to reach the maximum accuracy of Inception (72.2%), and the maximum accuracy achieved by the net- work.\n\nto be trained when sigmoid is used as the nonlinearity, despite the well-known difﬁculty of training such net- works. Indeed, BN-x5-Sigmoidachieves the accuracy of 69.8%. Without Batch Normalization, Inception with sig- moid never achieves better than 1/1000 accuracy.",
            "section": "other",
            "section_idx": 9,
            "citations": []
        },
        {
            "text": "References\n\nBengio, Yoshua and Glorot, Xavier. Understanding the difﬁculty of training deep feedforwardneural networks. In Proceedings of AISTATS 2010, volume 9, pp. 249– 256, May 2010.\n\nDean, Jeffrey,Corrado,Greg S., Monga,Rajat, Chen, Kai, Devin, Matthieu, Le, Quoc V., Mao, Mark Z., Ranzato, Marc’Aurelio,Senior, Andrew, Tucker, Paul, Yang, Ke, and Ng, Andrew Y. Large scale distributed deep net- works. In NIPS, 2012.\n\nDesjardins, Guillaume and Kavukcuoglu, Koray. Natural neural networks. (unpublished).\n\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\n\n8\n\noptimization. J. Mach.Learn.Res., 12:2121–2159,July 2011. ISSN 1532-4435.\n\nG¨ulc¸ehre, C¸aglar and Bengio, Yoshua. Knowledge mat- ters: Importance of prior information for optimization. CoRR, abs/1301.4083, 2013.\n\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving Deep into Rectiﬁers: Surpassing Human-Level Performance on ImageNet Classiﬁcation. ArXiv e-prints, February 2015.\n\nHyv¨arinen, A. and Oja, E. Independent component anal- ysis: Algorithms and applications. Neural Netw., 13 (4-5):411–430,May 2000.\n\nJiang, Jing. A literature survey on domain adaptation of statistical classiﬁers, 2008.\n\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recog- nition. Proceedings of the IEEE, 86(11):2278–2324, November 1998a.\n\nLeCun, Y., Bottou, L., Orr, G., and Muller, K. Efﬁcient backprop. In Orr, G. and K., Muller (eds.), Neural Net- works: Tricks of the trade. Springer, 1998b.\n\nLyu, S and Simoncelli, E P. Nonlinear image representa- tion using divisive normalization. In Proc. Computer Vision and Pattern Recognition, pp. 1–8. IEEE Com- puter Society, Jun 23-28 2008. doi: 10.1109/CVPR. 2008.4587821.\n\nNair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines. In ICML, pp. 807–814. Omnipress, 2010.\n\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the difﬁculty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16- 21 June 2013, pp. 1310–1318, 2013.\n\nPovey, Daniel, Zhang, Xiaohui, and Khudanpur, San- jeev. Parallel training of deep neural networks with natural gradient and parameter averaging. CoRR, abs/1410.7455, 2014.\n\nRaiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep learning made easier by linear transformations in per- ceptrons. In International Conference on Artiﬁcial In- telligence and Statistics (AISTATS),pp. 924–932,2012.\n\nRussakovsky,Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa- thy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge, 2014.\n\n9\n\nSaxe, Andrew M., McClelland, James L., and Ganguli, Surya. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013.\n\nShimodaira, Hidetoshi. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference, 90(2):227–244,October 2000.\n\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁt- ting. J. Mach. Learn. Res., 15(1):1929–1958, January 2014.\n\nSutskever, Ilya, Martens, James, Dahl, George E., and Hinton, Geoffrey E. On the importance of initial- ization and momentum in deep learning. In ICML (3), volume 28 of JMLR Proceedings, pp. 1139–1147. JMLR.org, 2013.\n\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du- mitru, Vanhoucke, Vincent, and Rabinovich, An- drew. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.\n\nWiesler, Simon and Ney, Hermann. A convergence anal- ysis of log-linear training. In Shawe-Taylor, J., Zemel, R.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Sys- tems 24,pp.657–665,Granada,Spain,December2011.\n\nWiesler, Simon, Richard, Alexander, Schl¨uter, Ralf, and Ney, Hermann. Mean-normalized stochastic gradient for large-scale deep learning. In IEEE International Conference on Acoustics, Speech, and Signal Process- ing, pp. 180–184, Florence, Italy, May 2014.\n\nWu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and Sun, Gang. Deep image: Scaling up image recognition, 2015.",
            "section": "other",
            "section_idx": 10,
            "citations": []
        },
        {
            "text": "Appendix\n\nVariant of the Inception Model Used\n\nFigure 5 documents the changes that were performed compared to the architecture with respect to the GoogleNet archictecture. For the interpretation of this table, please consult (Szegedy et al., 2014). The notable architecture changes compared to the GoogLeNet model include:\n\n• The 5 5 convolutional layers are replaced by two × consecutive 3 3 convolutional layers. This in- × creases the maximum depth of the network by 9\n\nweight layers. Also it increases the number of pa- rameters by 25% and the computational cost is in- creased by about 30%.\n\n• The number 28 from 2 to 3. × 28 inception modules is increased\n\n• Inside the modules, sometimes average, sometimes maximum-pooling is employed. This is indicated in the entries corresponding to the pooling layers of the table.\n\n• There are no across the board pooling layers be- tween any two Inception modules, but stride-2 con- volution/pooling layers are employed before the ﬁl- ter concatenation in the modules 3c, 4e.\n\nOur model employed separable convolution with depth multiplier 8 on the ﬁrst convolutional layer. This reduces the computational cost while increasing the memory con- sumption at training time.\n\n10\n\npatch size/ output double #3×3 depth #1×1 type Pool +proj #3×3 size reduce stride reduce 7×7/2 112×112×64 convolution* 1 3×3/2 56×56×64 max pool 0 3×3/1 56×56×192 64 convolution 192 1 3×3/2 28×28×192 max pool 0 28×28×256 96 64 64 64 3 inception (3a) 64 avg + 32 28×28×320 64 64 96 64 96 avg + 64 3 inception (3b) 28×28×576 stride 2 3 64 128 inception (3c) 96 max + pass through 160 0 14×14×576 224 96 128 64 96 avg + 128 3 inception (4a) 14×14×576 128 3 96 128 96 avg + 128 inception (4b) 192 14×14×576 160 128 160 128 3 inception (4c) 160 avg + 128 14×14×576 192 160 96 128 3 inception (4d) 192 avg + 128 14×14×1024 256 192 0 128 3 stride 2 inception (4e) 192 max + pass through 7×7×1024 224 160 352 192 3 inception (5a) 320 avg + 128 7×7×1024 224 192 352 192 3 inception (5b) 320 max + 128 7×7/1 1×1×1024 avg pool 0\n\n#3×3",
            "section": "other",
            "section_idx": 11,
            "citations": []
        },
        {
            "text": "double #3×3\n\nFigure 5: Inception architecture\n\n11",
            "section": "other",
            "section_idx": 12,
            "citations": []
        }
    ],
    "figures": [
        {
            "path": "output\\images\\a7de2cea-f705-4677-a362-796078d91605.jpg",
            "description": "This figure presents a comparison of the effects of using Batch Normalization (BN) in a neural network, highlighting its impact on training dynamics and performance.\n\n- **Subfigure (a):** A line graph comparing training performance with and without Batch Normalization (BN). The x-axis represents the number of iterations (ranging from 10K to 50K), and the y-axis indicates performance metrics (likely accuracy or loss). The graph shows two curves: one for training without BN (dashed line) and one with BN (solid line). The curve with BN demonstrates better performance, indicating faster convergence and possibly higher final accuracy.\n\n- **Subfigures (b) and (c):** These are plots of the distribution of activations in the network layers during training. Subfigure (b) shows the distribution without BN, where the activations have wider variance and are more spread out. Subfigure (c) illustrates the distribution with BN, showing more stable and centered activations. This highlights BN's role in stabilizing learning by maintaining the mean and variance of activations.\n\nOverall, this figure is crucial for understanding the benefits of Batch Normalization in neural networks, showcasing both improved training performance and stability of activations.",
            "importance": 9
        },
        {
            "path": "output\\images\\61d23380-aad9-4055-b743-1c8128af2756.jpg",
            "description": "The figure illustrates a neural network architecture designed for image processing tasks. It consists of several layers, including convolutional layers, pooling layers, and fully connected layers. The input is an image, which undergoes a series of transformations. The convolutional layers apply filters to detect features, followed by pooling layers to reduce dimensionality. The fully connected layers further process the extracted features to make predictions or classifications.\n\nAdditionally, the figure includes a block diagram showing data flow through the network, highlighting key components such as activation functions and loss calculation. The output is a set of probabilities or labels indicating the network's predictions.\n\nThis figure is crucial for understanding the network's structure and the data flow, making it a key part of the methodology and results sections in the research.",
            "importance": 9
        },
        {
            "path": "output\\images\\60d6536a-35c7-4923-a2df-098553185768.jpg",
            "description": "This figure is a graph depicting the performance comparison of several neural network architectures over a number of steps (up to 30 million). The y-axis represents a performance metric, likely accuracy or some similar measure, ranging from 0.4 to 0.8. The x-axis shows the number of training steps in millions.\n\nThe legend indicates different models or configurations being compared:\n\n- **Inception** (dashed line): A baseline architecture.\n- **BN–Baseline** (dash-dot line): A baseline model with batch normalization.\n- **BN–x5** (dotted line): A model using batch normalization with a scaling factor.\n- **BN–x30** (solid line): A model with a significantly higher scaling factor.\n- **BN–x5–Sigmoid** (plus markers): Similar to BN–x5 but using a sigmoid activation function.\n- **Steps to match Inception** (diamond markers): Points where the models reach the performance of the Inception baseline.\n\nThe graph shows how different configurations affect the performance over time, with each line representing how quickly and effectively the models learn. The figure highlights the influence of batch normalization and scaling on the training process, indicating that certain configurations converge faster or achieve higher performance compared to the baseline. This visualization is key for understanding the methodology and results in the research, as it directly compares the effectiveness of different approaches.",
            "importance": 8
        },
        {
            "path": "output\\images\\794a2122-ad70-410f-8269-7b406208166d.jpg",
            "description": "The figure illustrates a flowchart detailing the architecture of a machine learning model, specifically a Convolutional Neural Network (CNN). The diagram breaks down the CNN into distinct layers: Convolutional Layers, Pooling Layers, Flattening, and Fully Connected Layers. Each layer is annotated with its function and the dimensional changes occurring at each step, highlighting the transformation of input data into the final output. The flowchart emphasizes the sequential processing pipeline, showing how data progresses through the network. This visualization is crucial for understanding the methodology employed in the research, providing a clear and organized representation of the CNN's architecture, which is central to the study's innovation.",
            "importance": 8
        }
    ],
    "metadata": {
        "key_themes": [
            "Spatial Meshing",
            "Neural Network Optimization",
            "Nonlinear Loss Function",
            "Smoothness and Robustness",
            "Material transformation"
        ],
        "methodology": [
            "Neural Network Training",
            "3D Modeling",
            "Image Processing",
            "Estimation",
            "Loss functions"
        ],
        "domain": [
            "Autonomous Robotics",
            "Celebrity dataset"
        ],
        "strengths": [
            "3D Reconstruction",
            "DeepVision Framework",
            "Enhanced Detection Techniques",
            "Efficient Optimization",
            "Fairness Evaluation"
        ],
        "limitations": [
            "Training challenges",
            "Complexity",
            "Perceptual Color Balance",
            "Implicit Fields",
            "Suboptimal Efficiency"
        ]
    },
    "content_chunks": [
        {
            "chunk_id": "1502.03167v3_chunk_0",
            "section": "introduction",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_1",
            "section": "results",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_2",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_3",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_4",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_5",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_6",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_7",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_8",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_9",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_10",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_11",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_12",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_13",
            "section": "other",
            "citations": []
        },
        {
            "chunk_id": "1502.03167v3_chunk_14",
            "section": "other",
            "citations": []
        }
    ]
}