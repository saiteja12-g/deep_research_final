{
  "papers": [
    {
      "id": "2004.12989v2",
      "chunks": [
        {
          "text": "5 Conclusions\n\nWe made three contributions to methods for reconstructing the shape of a sin- gle object given one RBG image as input: (1) ray-traced skip connections that propagate local 2D information to the output 3D volume in a physically correct manner; (2) a hybrid 3D volume representation that enables building transla- tion equivariant models, while at the same time producing \ufb01ne object details with limited memory; (3) a reconstruction loss tailored to capture overall ob- ject geometry. We then adapted our model to reconstruct multiple objects. By doing so jointly in a single pass, we produce a coherent reconstruction with all objects in one consistent 3D coordinate frame, and without intersecting in 3D space. Finally, we validated the impact of our contributions on synthetic data from ShapeNet as well as real images from Pix3D, including a full quantitative evaluation of 3D shape reconstruction of multiple objects in the same image.\n\nCoReNet: Coherent 3D scene reconstruction from a single RGB image\n\nReferences\n\n1. https://github.com/google-research/corenet\n\n2. Berman, M., Triki, A.R., Blaschko, M.B.: The lov\u00b4asz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In: CVPR (2018)\n\n3. Chang, A.X., Dai, A., Funkhouser, T.A., Halber, M., Nie\u00dfner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matterport3d: Learning from RGB-D data in indoor environments. In: 2017 International Conference on 3D Vision (2017)\n\n4. Chang, A.X., Funkhouser, T.A., Guibas, L.J., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: Shapenet: An information-rich 3d model repository. CoRR abs/1512.03012 (2015), http:// arxiv.org/abs/1512.03012\n\n5. Chen, Z., Tagliasacchi, A., Zhang, H.: Bsp-net: Generating compact meshes via binary space partitioning. In: CVPR (2020)\n\n6. Chen, Z., Zhang, H.: Learning implicit \ufb01elds for generative shape modeling. In: CVPR (2019)\n\n7. Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A uni\ufb01ed approach for single and multi-view 3d object reconstruction. In: ECCV (2016)\n\n8. Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d object recon- struction from a single image. In: CVPR (2017)\n\n9. Georgia Gkioxari, Jitendra Malik, J.J.: Mesh r-cnn. ICCV (2019)\n\n10. Girdhar, R., Fouhey, D.F., Rodriguez, M., Gupta, A.: Learning a predictable and generative vector representation for objects. In: ECCV (2016)\n\n11. Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: A papier-m\u02c6ach\u00b4e approach to learning 3d surface generation. In: CVPR (2018)\n\n12. Hartley, R.I., Zisserman, A.: Multiple View Geometry in Computer Vision. Cam- bridge University Press, ISBN: 0521623049 (2000)\n\n13. He, K., Gkioxari, G., Doll\u00b4ar, P., Girshick, R.B.: Mask R-CNN. In: ICCV (2017)\n\n14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)\n\n15. Izadinia, H., Shan, Q., Seitz, S.M.: IM2CAD. In: CVPR. pp. 2422\u20132431 (2017)",
          "section": "results",
          "distance": 0.19979596138000488
        },
        {
          "text": "2 Related work\n\nSingle object reconstruction. In the last few years there has been a surge of methods for reconstructing the 3D shape of one object from a single RGB image. Many of them [7,10,48,50,51] employ voxel grids in their internal repre- sentation, as they can be handled naturally by convolutional neural networks. Some works have tried to go beyond voxels: (1) by using a di\ufb00erentiable voxels- to-mesh operation [21]; (2) by producing multiple depth-maps and/or silhouettes from \ufb01xed viewpoints that can be subsequently fused [38,35,33,52]; (3) by op- erating on point clouds [8,24], cuboidal primitives [45,30], and even directly on meshes [47,5]. A recent class of methods [31,25,6] use a continuous volume rep- resentation through implicit functions. The model receives a query 3D point as part of its input and returns the occupancy at that point.\n\nWe build on principles from these works and design a new type of hybrid rep- resentation that is both regular like voxels and continuous like implicit functions (sec. 3.1). We also address more complex reconstruction tasks: we reconstruct objects in the pose as depicted in the image and we also tackle scenes with multi- ple objects, predicting the semantic class of each object. Finally, we experiment with di\ufb00erent levels of rendering realism.\n\n3\n\n4\n\nStefan Popov, Pablo Bauszat, Vittorio Ferrari\n\nMulti-object reconstruction. IM2CAD [15] places multiple CAD models from a database in their appropriate position in the scene depicted in the in- put image. It only reconstructs the pose of the objects and copies over their whole CAD models, without trying to reconstruct their particular 3D shapes as they appear in the input image. 3D-RCNN [18] learns a per-class linear shape basis from a training dataset of 3D models. It then uses a render-and-compare approach to \ufb01t the coe\ufb03cients of this basis to objects detected in the test im- age. This method only outputs 3D shapes that lie on a simple linear subspace spanning the training samples. Instead our model can output arbitrary shapes, and the mapping between image appearance and shape is more complex as it is modeled by a deep neural network. Tulsiani et. al. [44] \ufb01rst detects object proposals [53] and then reconstructs a pose and a voxel grid for each, based on local features for the proposal and a global image descriptor. Mesh-RCNN [9] extends Mask-RCNN [13] to predict a 3D mesh for each detected object in an image. It tries to predict the objects positions in the image plane correctly, but it cannot resolve the fundamental scale/depth ambiguity along the Z-axis.\n\nAll four methods [15,18,44,9] \ufb01rst detect objects in the 2D image, and then reconstruct their 3D shapes independently. Instead, we reconstruct all objects jointly and without relying on a detection stage. This allows us to enforce space exclusion constraints and thus produce a globally coherent reconstruction.\n\nThe concurrent work [29] predicts the 3D pose of all objects jointly (after 2D object detection). Yet, it still reconstructs their 3D shape independently, and so the reconstructions might overlap in 3D space. Moreover, in contrast to [29,44,15,18] our method is simpler as it sidesteps the need to explicitly predict per-object poses, and instead directly outputs a joint coherent reconstruction.\n\nImportantly, none of these works [15,18,9,29] o\ufb00ers true quantitative eval- uation of 3D shape reconstruction on multiple object scenes. One of the main reasons for this is the lack of datasets with complete and correct ground truth data. One exception is [44] by evaluating on SunCG [39], which is now banned. In contrast, we evaluate our method fully, including the 3D shape of multiple objects in the same image. To enable this, we create two new datasets of scenes assembled from pairs and triplets of ShapeNet objects, and we report perfor- mance with a full scene evaluation metric (sec. 4.3).\n\nFinally, several works tackle multiple object reconstruction from an RGB-D image [28,40], exploiting the extra information that depth sensors provides.\n\nNeural scene representations. Recent works [34,36,26,27,37,23] on neural scene representations and neural rendering extract latent representations of the scene geometry from images and share similar insights to ours. In particular, [16,46] use unprojection, a technique to accumulate latent scene information from multiple views, related to our ray-traced skip connections. Others [34,37] can also reconstruct (single-object) geometry from one RGB image.",
          "section": "other",
          "distance": 0.22247636318206787
        },
        {
          "text": "r\n\ne\n\nU o I model m e n a l p r i a h c n e b t e n i b a c r a c r i a h c y a l p s i d p m a l k a e p s d u o l e \ufb02 i r a f o s e l b a t e n o h p e l e t ONN\u2217 52.6 45.8 45.1 43.8 54.0 58.5 55.4 39.5 57.0 48.0 68.0 50.7 68.3 49.9 h2 53.0 46.9 44.3 44.7 56.4 57.4 53.8 35.9 58.1 53.4 67.2 49.7 70.9 49.9 h5 57.9 53.0 50.8 50.9 57.3 63.0 57.2 42.1 60.8 64.6 70.6 55.5 73.1 54.0 3D-R2N2 49.3 42.6 37.3 66.7 66.1 43.9 44.0 28.1 61.1 37.5 62.6 42.0 61.1 48.2 Pix2Mesh 48.0 42.0 32.3 66.4 55.2 39.6 49.0 32.3 59.9 40.2 61.3 39.5 66.1 39.7 ONN 57.1 57.1 48.5 73.3 73.7 50.1 47.1 37.1 64.7 47.4 68.0 50.6 72.0 53.0 l e s s e v\n\nTable 2: Comparison to state of the art. The \ufb01rst three rows compare ONN [25] to our models h2 and h5, all trained on our data. The next three rows are taken from [25] and report performance of 3D-R2N2 [7], Pix2Mesh [47], and ONN [25] on their data.\n\nComparison to state-of-the-art. We compare our models to state-of-the art single object reconstruction methods [25,50,7,47]. We start with an exact com- parison to ONN [25]. For this we use the open source implementation provided by the authors to train and test their model on our low-realism images train and test sets. We then use our evaluation procedure on their output predictions. As table 2 shows, ONN achieves 52.6% mIoU on our data with our evaluation (and 51.5% with ONN\u2019s evaluation procedure). This number is expectedly lower than the 57.1% reported in [25] as we ask ONN to reconstruct each shape at the pose depicted in the input image, instead of the canonical pose. From ONN\u2019s per- spective, the training set contains 24 times more di\ufb00erent shapes, one for each rendered view of an object. Our best model for low-realism renderings h5 out- performs ONN on every class and achieves 57.9% mIoU. ONN\u2019s performance is comparable to h2, our best model that, like ONN, does not use skip connections.\n\nWe then compare to 3D-R2N2 [7], Pix2Mesh [47], and again ONN [25], using their mIoU as reported by [25] (table 2). Our model h5 clearly outperforms 3D- R2N2 (+8.6%) and Pix2Mesh (+9.9%). It also reaches a slightly better mIoU than ONN (+0.8%), while reconstructing in the appropriate pose for each input image, as opposed to a \ufb01xed canonical pose. We also compare on the Chamfer Distance surface metric, implemented exactly as in [25]. We obtain 0.15, which is better than 3D-R2N2 (0.278), Pix2Mesh (0.216), and ONN (0.215), all compared with the same metric (as reported by [25]).1\n\nFinally, we compare to Pix2Vox [50] and its extension Pix2Vox++ [51] (con- current work to ours). For a fair comparison we evaluate our h5 model on a 323 grid of points, matching the 323 voxel grid output by [50,51]. We compare directly to the mIoU they report. Our model h5 achieves 68.9% mIoU in this case, +2.8% higher than Pix2Vox (66.1% for their best Pix2Vox-A model) and +1.9% higher than Pix2Vox++ (67.0% for their best Pix2Vox++/A model).\n\n1 Several other works [8,11], including very recent ones [5,52], report Chamfer Distance and not IoU. They adopt subtly di\ufb00erent implementations, varying the underlying point distance metric, scaling, point sampling, and aggregation across points. Thus, they report di\ufb00erent numbers for the same works, preventing direct comparison.\n\nCoReNet: Coherent 3D scene reconstruction from a single RGB image\n\nReconstructing at high resolutions. Our model can perform reconstruction at a higher resolution than the one used during training (sec. 3.1). We study this here by reconstructing at 2\u00d7 and 4\u00d7 higher resolution. We train one model (y1) using a 323 grid and one (y2) using a 643 grid, with ray-traced skip connections, images with low realism, and focal loss. We then reconstruct a 1283 discretization from each model, by running inference multiple times at di\ufb00erent grid o\ufb00sets (64 and 8 times, respectively, sec. 3.5). At test time, we always measure performance on the 1283 reconstruction, regardless of training resolution.\n\nFig. 4 shows example reconstructions. We compare performance to h4 from table 1, which was trained with same settings but at native grid resolution 1283. Our \ufb01rst model (trained on 323 and producing 1283) achieves 53.1% mIoU. The second model (trained on 643 and producing 1283) gets to 56.1%, comparable to h4 (56.6%). This demonstrates that we can reconstruct at substantially higher resolution than the one used during training.",
          "section": "other",
          "distance": 0.22894787788391113
        }
      ],
      "themes": [
        "Multivariable calculus",
        "Tri-hybrid Representation",
        "Performance comparison",
        "Convolutional Neural Networks (CNNs)",
        "RGB-D Data",
        "semantic segmentation",
        "Photometric loss",
        "Geometric Network (GN)",
        "3D evaluation metric",
        "Cross-category generalization",
        "Diffusion Models",
        "contour detection",
        "3D Reconstruction",
        "Constraint-based modeling",
        "Classifier Training",
        "Synthetic dataset generation",
        "Loss functions",
        "Translation equivariance",
        "Distance Metrics",
        "Realism"
      ],
      "methods": [
        "Loss Function",
        "ResNet-based architectures",
        "Ranking performance",
        "Grid relaxation",
        "Reconstruction error",
        "3D Shape Estimation",
        "Geometric Regression",
        "Pix2Vox\u7cfb\u5217",
        "3D annotation",
        "Categorical Cross Entropy Loss",
        "h2/h5 models",
        "Camera superpixel processing",
        "3D-R2N2",
        "Skip connections",
        "Anterior to posterior",
        "Generative Deep Learning",
        "Neural rendering",
        "fine-tuning",
        "Occupancy Prediction Network",
        "Multi-scale encoding",
        "3D IoU",
        "Mask R-CNN"
      ],
      "domains": [],
      "strengths": [],
      "limitations": [],
      "citations": [
        "20",
        "9",
        "34",
        "47",
        "53",
        "5",
        "31",
        "39",
        "36",
        "27",
        "26",
        "29",
        "4",
        "11",
        "18",
        "10",
        "6",
        "51",
        "25",
        "32",
        "28",
        "24",
        "15",
        "48",
        "44",
        "35",
        "21",
        "50",
        "13",
        "40",
        "45",
        "16",
        "52",
        "30",
        "38",
        "7",
        "23",
        "1",
        "33",
        "46",
        "8",
        "42",
        "37"
      ],
      "graph_data": {
        "paper_id": "2004.12989v2",
        "title": "CoReNet: Coherent 3D scene reconstruction from a single RGB image",
        "authors": [
          "Stefan Popov",
          "Pablo Bauszat",
          "Vittorio Ferrari"
        ],
        "chunk_ids": [
          "2004.12989v2_chunk_0",
          "2004.12989v2_chunk_12",
          "2004.12989v2_chunk_11",
          "2004.12989v2_chunk_10",
          "2004.12989v2_chunk_9",
          "2004.12989v2_chunk_8",
          "2004.12989v2_chunk_7",
          "2004.12989v2_chunk_6",
          "2004.12989v2_chunk_5",
          "2004.12989v2_chunk_4",
          "2004.12989v2_chunk_3",
          "2004.12989v2_chunk_2",
          "2004.12989v2_chunk_1"
        ],
        "chunk_sections": [
          "methodology",
          "other",
          "results"
        ],
        "chunk_citations": [
          "1804.05469v1"
        ],
        "themes": [
          "Performance comparison",
          "Translation equivariance",
          "Multivariable calculus",
          "Distance Metrics",
          "Realism",
          "Loss functions",
          "Diffusion Models",
          "contour detection",
          "Geometric Network (GN)",
          "3D evaluation metric",
          "3D Reconstruction",
          "Convolutional Neural Networks (CNNs)",
          "Synthetic dataset generation",
          "Tri-hybrid Representation",
          "Constraint-based modeling",
          "Cross-category generalization",
          "RGB-D Data",
          "Classifier Training",
          "semantic segmentation",
          "Photometric loss"
        ],
        "methods": [
          "3D-R2N2",
          "Generative Deep Learning",
          "Pix2Vox\u7cfb\u5217",
          "Ranking performance",
          "Occupancy Prediction Network",
          "Skip connections",
          "Categorical Cross Entropy Loss",
          "fine-tuning",
          "Multi-scale encoding",
          "Geometric Regression",
          "ResNet-based architectures",
          "Grid relaxation",
          "Neural rendering",
          "3D IoU",
          "Mask R-CNN",
          "h2/h5 models",
          "3D annotation",
          "Reconstruction error",
          "3D Shape Estimation",
          "Anterior to posterior",
          "Camera superpixel processing",
          "Loss Function"
        ],
        "domains": [
          "3D Computer Vision"
        ],
        "theme_related_papers": [
          "2105.05233v4",
          "1612.00496v2",
          "1411.6387v2",
          "2111.03098v1",
          "2401.04099v1",
          "2303.16509v2",
          "1612.00603v2",
          "1503.06465v2",
          "2010.03592v1",
          "1810.13049v2",
          "1704.02956v1",
          "1606.00373v2",
          "2007.13215v1",
          "1804.05469v1"
        ],
        "method_related_papers": [
          "2401.04099v1",
          "2303.16509v2",
          "2111.03098v1",
          "2105.05233v4",
          "2010.03592v1",
          "2007.13215v1",
          "1810.13049v2",
          "1804.05469v1",
          "1704.02956v1",
          "1612.00603v2",
          "1612.00496v2",
          "1606.00373v2",
          "1411.6387v2",
          "1503.06465v2"
        ],
        "domain_related_papers": [
          "2401.04099v1",
          "2303.16509v2",
          "2111.03098v1",
          "2105.05233v4",
          "2010.03592v1",
          "2007.13215v1",
          "1810.13049v2",
          "1804.05469v1",
          "1704.02956v1",
          "1612.00603v2",
          "1612.00496v2",
          "1606.00373v2",
          "1503.06465v2",
          "1411.6387v2"
        ]
      },
      "chunk_count": 6,
      "min_distance": 0.19979596138000488,
      "content": "5 Conclusions\n\nWe made three contributions to methods for reconstructing the shape of a sin- gle object given one RBG image as input: (1) ray-traced skip connections that propagate local 2D information to the output 3D volume in a physically correct manner; (2) a hybrid 3D volume representation that enables building transla- tion equivariant models, while at the same time producing \ufb01ne object details with limited memory; (3) a reconstruction loss tailored to capture overall ob- ject geometry. We then adapted our model to reconstruct multiple objects. By doing so jointly in a single pass, we produce a coherent reconstruction with all objects in one consistent 3D coordinate frame, and without intersecting in 3D space. Finally, we validated the impact of our contributions on synthetic data from ShapeNet as well as real images from Pix3D, including a full quantitative evaluation of 3D shape reconstruction of multiple objects in the same image.\n\nCoReNet: Coherent 3D scene reconstruction from a single RGB image\n\nReferences\n\n1. https://github.com/google-research/corenet\n\n2. Berman, M., Triki, A.R., Blaschko, M.B.: The lov\u00b4asz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In: CVPR (2018)\n\n3. Chang, A.X., Dai, A., Funkhouser, T.A., Halber, M., Nie\u00dfner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matterport3d: Learning from RGB-D data in indoor environments. In: 2017 International Conference on 3D Vision (2017)\n\n4. Chang, A.X., Funkhouser, T.A., Guibas, L.J., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: Shapenet: An information-rich 3d model repository. CoRR abs/1512.03012 (2015), http:// arxiv.org/abs/1512.03012\n\n5. Chen, Z., Tagliasacchi, A., Zhang, H.: Bsp-net: Generating compact meshes via binary space partitioning. In: CVPR (2020)\n\n6. Chen, Z., Zhang, H.: Learning implicit \ufb01elds for generative shape modeling. In: CVPR (2019)\n\n7. Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A uni\ufb01ed approach for single and multi-view 3d object reconstruction. In: ECCV (2016)\n\n8. Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d object recon- struction from a single image. In: CVPR (2017)\n\n9. Georgia Gkioxari, Jitendra Malik, J.J.: Mesh r-cnn. ICCV (2019)\n\n10. Girdhar, R., Fouhey, D.F., Rodriguez, M., Gupta, A.: Learning a predictable and generative vector representation for objects. In: ECCV (2016)\n\n11. Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: A papier-m\u02c6ach\u00b4e approach to learning 3d surface generation. In: CVPR (2018)\n\n12. Hartley, R.I., Zisserman, A.: Multiple View Geometry in Computer Vision. Cam- bridge University Press, ISBN: 0521623049 (2000)\n\n13. He, K., Gkioxari, G., Doll\u00b4ar, P., Girshick, R.B.: Mask R-CNN. In: ICCV (2017)\n\n14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)\n\n15. Izadinia, H., Shan, Q., Seitz, S.M.: IM2CAD. In: CVPR. pp. 2422\u20132431 (2017)\n\n2 Related work\n\nSingle object reconstruction. In the last few years there has been a surge of methods for reconstructing the 3D shape of one object from a single RGB image. Many of them [7,10,48,50,51] employ voxel grids in their internal repre- sentation, as they can be handled naturally by convolutional neural networks. Some works have tried to go beyond voxels: (1) by using a di\ufb00erentiable voxels- to-mesh operation (Liao, 2018); (2) by producing multiple depth-maps and/or silhouettes from \ufb01xed viewpoints that can be subsequently fused [38,35,33,52]; (3) by op- erating on point clouds [8,24], cuboidal primitives [45,30], and even directly on meshes [47,5]. A recent class of methods [31,25,6] use a continuous volume rep- resentation through implicit functions. The model receives a query 3D point as part of its input and returns the occupancy at that point.\n\nWe build on principles from these works and design a new type of hybrid rep- resentation that is both regular like voxels and continuous like implicit functions (sec. 3.1). We also address more complex reconstruction tasks: we reconstruct objects in the pose as depicted in the image and we also tackle scenes with multi- ple objects, predicting the semantic class of each object. Finally, we experiment with di\ufb00erent levels of rendering realism.\n\n3\n\n4\n\nStefan Popov, Pablo Bauszat, Vittorio Ferrari\n\nMulti-object reconstruction. IM2CAD (Izadinia, 2422) places multiple CAD models from a database in their appropriate position in the scene depicted in the in- put image. It only reconstructs the pose of the objects and copies over their whole CAD models, without trying to reconstruct their particular 3D shapes as they appear in the input image. 3D-RCNN (Kundu, 2018) learns a per-class linear shape basis from a training dataset of 3D models. It then uses a render-and-compare approach to \ufb01t the coe\ufb03cients of this basis to objects detected in the test im- age. This method only outputs 3D shapes that lie on a simple linear subspace spanning the training samples. Instead our model can output arbitrary shapes, and the mapping between image appearance and shape is more complex as it is modeled by a deep neural network. Tulsiani et. al. (Tulsiani, 2018) \ufb01rst detects object proposals (Zitnick, 2014) and then reconstructs a pose and a voxel grid for each, based on local features for the proposal and a global image descriptor. Mesh-RCNN (Gkioxari, 2019) extends Mask-RCNN (He, 2017) to predict a 3D mesh for each detected object in an image. It tries to predict the objects positions in the image plane correctly, but it cannot resolve the fundamental scale/depth ambiguity along the Z-axis.\n\nAll four methods [15,18,44,9] \ufb01rst detect objects in the 2D image, and then reconstruct their 3D shapes independently. Instead, we reconstruct all objects jointly and without relying on a detection stage. This allows us to enforce space exclusion constraints and thus produce a globally coherent reconstruction.\n\nThe concurrent work (Nie, 2020) predicts the 3D pose of all objects jointly (after 2D object detection). Yet, it still reconstructs their 3D shape independently, and so the reconstructions might overlap in 3D space. Moreover, in contrast to [29,44,15,18] our method is simpler as it sidesteps the need to explicitly predict per-object poses, and instead directly outputs a joint coherent reconstruction.\n\nImportantly, none of these works [15,18,9,29] o\ufb00ers true quantitative eval- uation of 3D shape reconstruction on multiple object scenes. One of the main reasons for this is the lack of datasets with complete and correct ground truth data. One exception is (Tulsiani, 2018) by evaluating on SunCG (Song, 2017), which is now banned. In contrast, we evaluate our method fully, including the 3D shape of multiple objects in the same image. To enable this, we create two new datasets of scenes assembled from pairs and triplets of ShapeNet objects, and we report perfor- mance with a full scene evaluation metric (sec. 4.3).\n\nFinally, several works tackle multiple object reconstruction from an RGB-D image [28,40], exploiting the extra information that depth sensors provides.\n\nNeural scene representations. Recent works [34,36,26,27,37,23] on neural scene representations and neural rendering extract latent representations of the scene geometry from images and share similar insights to ours. In particular, [16,46] use unprojection, a technique to accumulate latent scene information from multiple views, related to our ray-traced skip connections. Others [34,37] can also reconstruct (single-object) geometry from one RGB image.\n\nr\n\ne\n\nU o I model m e n a l p r i a h c n e b t e n i b a c r a c r i a h c y a l p s i d p m a l k a e p s d u o l e \ufb02 i r a f o s e l b a t e n o h p e l e t ONN\u2217 52.6 45.8 45.1 43.8 54.0 58.5 55.4 39.5 57.0 48.0 68.0 50.7 68.3 49.9 h2 53.0 46.9 44.3 44.7 56.4 57.4 53.8 35.9 58.1 53.4 67.2 49.7 70.9 49.9 h5 57.9 53.0 50.8 50.9 57.3 63.0 57.2 42.1 60.8 64.6 70.6 55.5 73.1 54.0 3D-R2N2 49.3 42.6 37.3 66.7 66.1 43.9 44.0 28.1 61.1 37.5 62.6 42.0 61.1 48.2 Pix2Mesh 48.0 42.0 32.3 66.4 55.2 39.6 49.0 32.3 59.9 40.2 61.3 39.5 66.1 39.7 ONN 57.1 57.1 48.5 73.3 73.7 50.1 47.1 37.1 64.7 47.4 68.0 50.6 72.0 53.0 l e s s e v\n\nTable 2: Comparison to state of the art. The \ufb01rst three rows compare ONN (Mescheder, 2019) to our models h2 and h5, all trained on our data. The next three rows are taken from (Mescheder, 2019) and report performance of 3D-R2N2 (Choy, 2016), Pix2Mesh (Wang, 2018), and ONN (Mescheder, 2019) on their data.\n\nComparison to state-of-the-art. We compare our models to state-of-the art single object reconstruction methods [25,50,7,47]. We start with an exact com- parison to ONN (Mescheder, 2019). For this we use the open source implementation provided by the authors to train and test their model on our low-realism images train and test sets. We then use our evaluation procedure on their output predictions. As table 2 shows, ONN achieves 52.6% mIoU on our data with our evaluation (and 51.5% with ONN\u2019s evaluation procedure). This number is expectedly lower than the 57.1% reported in (Mescheder, 2019) as we ask ONN to reconstruct each shape at the pose depicted in the input image, instead of the canonical pose. From ONN\u2019s per- spective, the training set contains 24 times more di\ufb00erent shapes, one for each rendered view of an object. Our best model for low-realism renderings h5 out- performs ONN on every class and achieves 57.9% mIoU. ONN\u2019s performance is comparable to h2, our best model that, like ONN, does not use skip connections.\n\nWe then compare to 3D-R2N2 (Choy, 2016), Pix2Mesh (Wang, 2018), and again ONN (Mescheder, 2019), using their mIoU as reported by (Mescheder, 2019) (table 2). Our model h5 clearly outperforms 3D- R2N2 (+8.6%) and Pix2Mesh (+9.9%). It also reaches a slightly better mIoU than ONN (+0.8%), while reconstructing in the appropriate pose for each input image, as opposed to a \ufb01xed canonical pose. We also compare on the Chamfer Distance surface metric, implemented exactly as in (Mescheder, 2019). We obtain 0.15, which is better than 3D-R2N2 (0.278), Pix2Mesh (0.216), and ONN (0.215), all compared with the same metric (as reported by (Mescheder, 2019)).1\n\nFinally, we compare to Pix2Vox (Xie, 2019) and its extension Pix2Vox++ (Xie, 2020) (con- current work to ours). For a fair comparison we evaluate our h5 model on a 323 grid of points, matching the 323 voxel grid output by [50,51]. We compare directly to the mIoU they report. Our model h5 achieves 68.9% mIoU in this case, +2.8% higher than Pix2Vox (66.1% for their best Pix2Vox-A model) and +1.9% higher than Pix2Vox++ (67.0% for their best Pix2Vox++/A model).\n\n1 Several other works [8,11], including very recent ones [5,52], report Chamfer Distance and not IoU. They adopt subtly di\ufb00erent implementations, varying the underlying point distance metric, scaling, point sampling, and aggregation across points. Thus, they report di\ufb00erent numbers for the same works, preventing direct comparison.\n\nCoReNet: Coherent 3D scene reconstruction from a single RGB image\n\nReconstructing at high resolutions. Our model can perform reconstruction at a higher resolution than the one used during training (sec. 3.1). We study this here by reconstructing at 2\u00d7 and 4\u00d7 higher resolution. We train one model (y1) using a 323 grid and one (y2) using a 643 grid, with ray-traced skip connections, images with low realism, and focal loss. We then reconstruct a 1283 discretization from each model, by running inference multiple times at di\ufb00erent grid o\ufb00sets (64 and 8 times, respectively, sec. 3.5). At test time, we always measure performance on the 1283 reconstruction, regardless of training resolution.\n\nFig. 4 shows example reconstructions. We compare performance to h4 from table 1, which was trained with same settings but at native grid resolution 1283. Our \ufb01rst model (trained on 323 and producing 1283) achieves 53.1% mIoU. The second model (trained on 643 and producing 1283) gets to 56.1%, comparable to h4 (56.6%). This demonstrates that we can reconstruct at substantially higher resolution than the one used during training."
    },
    {
      "id": "1612.00603v2",
      "chunks": [
        {
          "text": "6\n\n1\n\n0 2 c e D 7 ] V C . s c [ 2 v 3 0 6 0 0 . 2 1 6 1 :\n\nv\n\narXiv\n\ni\n\nX\n\nr\n\na\n\nA Point Set Generation Network for 3D Object Reconstruction from a Single Image\n\nHaoqiang Fan \u2217 Institute for Interdisciplinary Information Sciences Tsinghua University fanhqme@gmail.com\n\nHao Su\u2217 Leonidas Guibas Computer Science Department\n\nStanford University {haosu,guibas}@cs.stanford.edu\n\nAbstract\n\nGeneration of 3D data by deep neural network has been attracting increasing attention in the research com- munity. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output \u2013 point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our \ufb01nal solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of- the-art methods on single image based 3d reconstruction benchmarks; but it also shows strong performance for 3d shape completion and promising ability in making multiple plausible predictions.\n\nInput Reconstructed 3D point cloud\n\nFigure 1. A 3D point cloud of the complete object can be reconstructed from a single image. Each point is visualized as a small sphere. The reconstruction is viewed at two viewpoints (0\u25e6 and 90\u25e6 along azimuth). A segmentation mask is used to indicate the scope of the object in the image.\n\nfor weight sharing, etc. That is why the majority of extant works on using deep nets for 3D data resort to either volumetric grids or collections of images (2D views of the geometry). Such representations, however, lead to dif\ufb01cult trade offs between sampling resolution and net ef\ufb01ciency. Furthermore, they enshrine quantization artifacts that obscure natural invariances of the data under rigid motions, etc.",
          "section": "other",
          "distance": 0.22008919715881348
        },
        {
          "text": "3. Problem and Notations\n\nOur goal is to reconstruct the complete 3D shape of an object from a single 2D image (RGB or RGB-D). We represent the 3D shapes in the form of unordered point set S = {(xi,yi,zi)}N i=1 where N is a prede\ufb01ned constant. We observed that for most objects using N = 1024 is suf\ufb01cient to preserve the major structures.\n\nOne advantage of point set comes from its unordered- ness. Unlike 2D based representations like the depth map no topological constraint is put on the represented\n\nvanilla version two prediction branch version hourglass version\n\nFigure 2. PointOutNet structure\n\nobject. Compared to 3D grids, the point set enjoys higher ef\ufb01ciency by encoding only the points on the surface. Also, the coordinate values (xi,yi,zi) go over simple linear transformations when the object is rotated or scaled, which is in contrast to the case in volumetric representations.\n\nTo model the problem\u2019s uncertainty, we de\ufb01ne the groundtruth as a probability distribution P(\u00b7|I) over the shapes conditioned on the input I. In training we have access to one sample from P(\u00b7|I) for each image I.\n\nWe train a neural network G as a conditional sampler from P(\u00b7|I):\n\nS = G(I,r;\u0398) (1)\n\nnetwork with two prediction branches, one enjoys high \ufb02exibility in capturing complicated structures and the other exploits geometric continuity. Its representation power is further boosted by an hourglass structure. See Sec 4.2.\n\nLoss function for point set comparison: For our novel type of prediction, point set, it is unclear how to measure the distance between the prediction and groundtruth. We introduce two distance metrics for point sets \u2013 the Chamfer distance and the Earth Mover\u2019s distance. We show that both metrics are differentiable almost everywhere and can be used as the loss function, but has different properties in capturing shape space. See Sec 4.3.\n\nwhere \u0398 denotes network parameter, r \u223c N(0,I) is a random variable to perturb the input 1. During test time multiple samples of r could be used to generate different predictions.",
          "section": "other",
          "distance": 0.23189902305603027
        },
        {
          "text": "2. Related Work\n\n3D reconstruction from single images While most researches focus on multi-view geometry such as SFM and SLAM [11, 10], ideally, one expect that 3D can be reconstructed from the abundant single-view images.\n\nUnder this setting, however, the problem is ill-posed and priors must be incorporated. Early work such as ShapeFromX [13, 1] made strong assumptions over the shape or the environment lighting conditions. [12, 21] pioneered the use of learning-based approach for simple geometric structures. Coarse correspondences in an image collection can also be used for rough 3D shape estima- tion [15, 3]. As commodity 3D sensors become popular, RGBD database has been built and used to train learning- based systems [7, 9]. Though great progress has been made, these methods still cannot robustly reconstruct complete and quality shapes from single images. Stronger shape priors are missing.\n\nRecently, large-scale repositories of 3D CAD models, such as ShapeNet [4], have been introduced. They have great potential for 3D reconstruction tasks. For example, [22, 14] proposed to deform and reassemble existing shapes into a new model to \ufb01t the observed image. These systems rely on high-quality image-shape correspondence, which is a challenging and ill-posed problem itself.\n\nMore relevant to our work is [5]. Given a single image, they use a neural network to predict the underlying 3D object as a 3D volume. There are two key differences between our work and [5]: First, the predicted object in [5] is a 3D volume; whilst ours is a point cloud. As demonstrated and analyzed in Sec 5.2, point set forms a nicer shape space for neural networks, thus the predicted shapes tend to be more complete and natural. Second, we allow multiple reconstruction candidates for a single input image. This design re\ufb02ects the fact that a single image cannot fully determine the reconstruction of a 3D shape.\n\nDeep learning for geometric object synthesis In gen- eral, the \ufb01eld of how to predict geometries in an end-to-end fashion is quite a virgin land. In particular, our output, 3D point set, is still not a typical object in the deep learning community. A point set contains orderless samples from a metric-measure space. Therefore, equivalent classes are de\ufb01ned up to a permutation; in addition, the ground distance must be taken into consideration. To our knowledge, we are not aware of prior deep learning systems with the abilities to predict such objects.",
          "section": "other",
          "distance": 0.23935472965240479
        }
      ],
      "themes": [
        "3D evaluation metric",
        "measurement uncertainty",
        "3D Reconstruction",
        "Diffusion Models",
        "Constraint-based modeling",
        "Dense branch",
        "unordered set generation",
        "Synthetic dataset generation",
        "Convolutional Neural Networks (CNNs)",
        "RGB-D Data",
        "Loss functions",
        "Geometric Network (GN)",
        "Distance Metrics",
        "Geometric inference",
        "Quality assessment"
      ],
      "methods": [
        "Generative Deep Learning",
        "Loss Function",
        "Transformation layers",
        "Two-pass bootstrap",
        "Point-Voxel UNet",
        "Occupancy Prediction Network",
        "Machine learning",
        "Epsilon-scaling.",
        "Variational Autoencoder (VAE)",
        "Blinn-Phong shading",
        "3D Shape Estimation",
        "Interpolation",
        "Distance Metrics",
        "Hourglass architecture",
        "Sampling techniques"
      ],
      "domains": [],
      "strengths": [],
      "limitations": [],
      "citations": [
        " 3",
        "7",
        "15",
        "22",
        " 14",
        "5",
        "4",
        " 21",
        "12",
        "11",
        " 1",
        "13",
        " 10",
        " 9"
      ],
      "graph_data": {
        "paper_id": "1612.00603v2",
        "title": "A Point Set Generation Network for 3D Object Reconstruction from a Single Image",
        "authors": [
          "Haoqiang Fan",
          "Hao Su",
          "Leonidas Guibas"
        ],
        "chunk_ids": [
          "1612.00603v2_chunk_0",
          "1612.00603v2_chunk_12",
          "1612.00603v2_chunk_11",
          "1612.00603v2_chunk_10",
          "1612.00603v2_chunk_9",
          "1612.00603v2_chunk_8",
          "1612.00603v2_chunk_7",
          "1612.00603v2_chunk_6",
          "1612.00603v2_chunk_5",
          "1612.00603v2_chunk_4",
          "1612.00603v2_chunk_3",
          "1612.00603v2_chunk_2",
          "1612.00603v2_chunk_1"
        ],
        "chunk_sections": [
          "introduction",
          "other",
          "discussion",
          "methodology"
        ],
        "chunk_citations": [
          "1503.06465v2"
        ],
        "themes": [
          "Synthetic dataset generation",
          "unordered set generation",
          "Dense branch",
          "Loss functions",
          "Diffusion Models",
          "Geometric Network (GN)",
          "RGB-D Data",
          "Distance Metrics",
          "Constraint-based modeling",
          "Geometric inference",
          "Quality assessment",
          "3D evaluation metric",
          "3D Reconstruction",
          "measurement uncertainty",
          "Convolutional Neural Networks (CNNs)"
        ],
        "methods": [
          "Transformation layers",
          "Interpolation",
          "Occupancy Prediction Network",
          "Two-pass bootstrap",
          "Variational Autoencoder (VAE)",
          "Point-Voxel UNet",
          "Blinn-Phong shading",
          "Generative Deep Learning",
          "Distance Metrics",
          "Machine learning",
          "Epsilon-scaling.",
          "3D Shape Estimation",
          "Hourglass architecture",
          "Loss Function",
          "Sampling techniques"
        ],
        "domains": [
          "Predictive modeling",
          "3D Computer Vision"
        ],
        "theme_related_papers": [
          "2303.16509v2",
          "2111.03098v1",
          "2105.05233v4",
          "2007.13215v1",
          "2004.12989v2",
          "1612.00496v2",
          "1606.00373v2",
          "1503.06465v2",
          "1411.6387v2",
          "2010.03592v1",
          "1810.13049v2",
          "1704.02956v1",
          "2401.04099v1",
          "1804.05469v1"
        ],
        "method_related_papers": [
          "2401.04099v1",
          "2105.05233v4",
          "1606.00373v2",
          "2303.16509v2",
          "2111.03098v1",
          "2010.03592v1",
          "2007.13215v1",
          "2004.12989v2",
          "1810.13049v2",
          "1804.05469v1",
          "1704.02956v1",
          "1612.00496v2",
          "1411.6387v2",
          "1503.06465v2"
        ],
        "domain_related_papers": [
          "2401.04099v1",
          "2303.16509v2",
          "2111.03098v1",
          "2105.05233v4",
          "2010.03592v1",
          "2007.13215v1",
          "2004.12989v2",
          "1810.13049v2",
          "1804.05469v1",
          "1704.02956v1",
          "1612.00496v2",
          "1606.00373v2",
          "1503.06465v2",
          "1411.6387v2"
        ]
      },
      "chunk_count": 3,
      "min_distance": 0.22008919715881348,
      "content": "6\n\n1\n\n0 2 c e D 7 ] V C . s c [ 2 v 3 0 6 0 0 . 2 1 6 1 :\n\nv\n\narXiv\n\ni\n\nX\n\nr\n\na\n\nA Point Set Generation Network for 3D Object Reconstruction from a Single Image\n\nHaoqiang Fan \u2217 Institute for Interdisciplinary Information Sciences Tsinghua University fanhqme@gmail.com\n\nHao Su\u2217 Leonidas Guibas Computer Science Department\n\nStanford University {haosu,guibas}@cs.stanford.edu\n\nAbstract\n\nGeneration of 3D data by deep neural network has been attracting increasing attention in the research com- munity. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output \u2013 point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our \ufb01nal solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of- the-art methods on single image based 3d reconstruction benchmarks; but it also shows strong performance for 3d shape completion and promising ability in making multiple plausible predictions.\n\nInput Reconstructed 3D point cloud\n\nFigure 1. A 3D point cloud of the complete object can be reconstructed from a single image. Each point is visualized as a small sphere. The reconstruction is viewed at two viewpoints (0\u25e6 and 90\u25e6 along azimuth). A segmentation mask is used to indicate the scope of the object in the image.\n\nfor weight sharing, etc. That is why the majority of extant works on using deep nets for 3D data resort to either volumetric grids or collections of images (2D views of the geometry). Such representations, however, lead to dif\ufb01cult trade offs between sampling resolution and net ef\ufb01ciency. Furthermore, they enshrine quantization artifacts that obscure natural invariances of the data under rigid motions, etc.\n\n3. Problem and Notations\n\nOur goal is to reconstruct the complete 3D shape of an object from a single 2D image (RGB or RGB-D). We represent the 3D shapes in the form of unordered point set S = {(xi,yi,zi)}N i=1 where N is a prede\ufb01ned constant. We observed that for most objects using N = 1024 is suf\ufb01cient to preserve the major structures.\n\nOne advantage of point set comes from its unordered- ness. Unlike 2D based representations like the depth map no topological constraint is put on the represented\n\nvanilla version two prediction branch version hourglass version\n\nFigure 2. PointOutNet structure\n\nobject. Compared to 3D grids, the point set enjoys higher ef\ufb01ciency by encoding only the points on the surface. Also, the coordinate values (xi,yi,zi) go over simple linear transformations when the object is rotated or scaled, which is in contrast to the case in volumetric representations.\n\nTo model the problem\u2019s uncertainty, we de\ufb01ne the groundtruth as a probability distribution P(\u00b7|I) over the shapes conditioned on the input I. In training we have access to one sample from P(\u00b7|I) for each image I.\n\nWe train a neural network G as a conditional sampler from P(\u00b7|I):\n\nS = G(I,r;\u0398) (1)\n\nnetwork with two prediction branches, one enjoys high \ufb02exibility in capturing complicated structures and the other exploits geometric continuity. Its representation power is further boosted by an hourglass structure. See Sec 4.2.\n\nLoss function for point set comparison: For our novel type of prediction, point set, it is unclear how to measure the distance between the prediction and groundtruth. We introduce two distance metrics for point sets \u2013 the Chamfer distance and the Earth Mover\u2019s distance. We show that both metrics are differentiable almost everywhere and can be used as the loss function, but has different properties in capturing shape space. See Sec 4.3.\n\nwhere \u0398 denotes network parameter, r \u223c N(0,I) is a random variable to perturb the input 1. During test time multiple samples of r could be used to generate different predictions.\n\n2. Related Work\n\n3D reconstruction from single images While most researches focus on multi-view geometry such as SFM and SLAM [11, 10], ideally, one expect that 3D can be reconstructed from the abundant single-view images.\n\nUnder this setting, however, the problem is ill-posed and priors must be incorporated. Early work such as ShapeFromX [13, 1] made strong assumptions over the shape or the environment lighting conditions. [12, 21] pioneered the use of learning-based approach for simple geometric structures. Coarse correspondences in an image collection can also be used for rough 3D shape estima- tion [15, 3]. As commodity 3D sensors become popular, RGBD database has been built and used to train learning- based systems [7, 9]. Though great progress has been made, these methods still cannot robustly reconstruct complete and quality shapes from single images. Stronger shape priors are missing.\n\nRecently, large-scale repositories of 3D CAD models, such as ShapeNet [4], have been introduced. They have great potential for 3D reconstruction tasks. For example, [22, 14] proposed to deform and reassemble existing shapes into a new model to \ufb01t the observed image. These systems rely on high-quality image-shape correspondence, which is a challenging and ill-posed problem itself.\n\nMore relevant to our work is [5]. Given a single image, they use a neural network to predict the underlying 3D object as a 3D volume. There are two key differences between our work and [5]: First, the predicted object in [5] is a 3D volume; whilst ours is a point cloud. As demonstrated and analyzed in Sec 5.2, point set forms a nicer shape space for neural networks, thus the predicted shapes tend to be more complete and natural. Second, we allow multiple reconstruction candidates for a single input image. This design re\ufb02ects the fact that a single image cannot fully determine the reconstruction of a 3D shape.\n\nDeep learning for geometric object synthesis In gen- eral, the \ufb01eld of how to predict geometries in an end-to-end fashion is quite a virgin land. In particular, our output, 3D point set, is still not a typical object in the deep learning community. A point set contains orderless samples from a metric-measure space. Therefore, equivalent classes are de\ufb01ned up to a permutation; in addition, the ground distance must be taken into consideration. To our knowledge, we are not aware of prior deep learning systems with the abilities to predict such objects."
    },
    {
      "id": "1804.05469v1",
      "chunks": [
        {
          "text": "6. Conclusion\n\nWe have proposed a deep learning framework that di- rectly recovers 3D shape structures from single 2D images. Our network joins a structure masking network for decern- ing the object structure and a structure recovery network for inferring 3D cuboid structure. The recovered 3D struc- tures achieve both \ufb01delity with respect to the input image and plausibility as a 3D shape structure. To the best of our knowledge, our work is the \ufb01rst that recovers detailed 3D shape structures from single 2D images.\n\nOur method fails to recover structures for object cate- gories unseen from the training set. For such cases, it would be interesting to learn an incremental part assembler. Our method currently recovers 3D cuboids only but not the un- derlying part geometry. A worthy direction is to synthesize detailed part geometry matching the visual appearance of the input image. Another interesting topic is to study the profound correlation between 2D features and 3D structure, so as to achieve a more explainable 3D structure decoding.\n\nReferences\n\n[1] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 3\n\n[2] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d- r2n2: A uni\ufb01ed approach for single and multi-view 3d ob- ject reconstruction. In European Conference on Computer Vision, pages 628\u2013644. Springer, 2016. 1, 3\n\n[3] D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolu- tional architecture. In Proceedings of the IEEE International Conference on Computer Vision, pages 2650\u20132658, 2015. 3\n\n[4] H. Fan, H. Su, and L. Guibas. A point set generation net- work for 3d object reconstruction from a single image. arXiv preprint arXiv:1612.00603, 2016. 1, 3\n\n[5] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta. Learning a predictable and generative vector representation for objects. In European Conference on Computer Vision, pages 484\u2013499. Springer, 2016. 1, 3\n\n[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014. 3\n\n[7] Q. Huang, H. Wang, and V. Koltun. Single-view reconstruc- tion via joint analysis of image and shape collections. ACM Transactions on Graphics (TOG), 34(4):87, 2015. 3, 7\n\n[8] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri. 3d shape segmentation with projective convolutional networks. arXiv preprint arXiv:1612.02808, 2016. 1\n\n[9] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n\n[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012. 1\n\n[11] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab. Deeper depth prediction with fully convolutional residual networks. In 3D Vision (3DV), 2016 Fourth Interna- tional Conference on, pages 239\u2013248. IEEE, 2016. 3\n\n[12] J. Li, R. Klein, and A. Yao. A two-streamed network for estimating \ufb01ne-scaled depth maps from single rgb images. In ICCV, 2017. 3, 4, 6\n\n[13] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, and L. Guibas. Grass: Generative recursive autoencoders for shape structures. arXiv preprint arXiv:1705.02090, 2017. 2, 3, 4, 5\n\n[14] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, and R. Wang. 3d shape reconstruction from sketches via multi-view convo- lutional networks. arXiv preprint arXiv:1707.06375, 2017. 3\n\n[15] N. Mitra, M. Wand, H. R. Zhang, D. Cohen-Or, V. Kim, and Q.-X. Huang. Structure-aware shape processing. In SIG- GRAPH Asia 2013 Courses, page 1. ACM, 2013. 1\n\n[16] J. Shotton, A. Blake, and R. Cipolla. Contour-based learning for object detection. In Proc. ICCV, volume 1, pages 503\u2013 510. IEEE, 2005. 3\n\n[17] H. Su, Q. Huang, N. J. Mitra, Y. Li, and L. Guibas. Estimat- ing image depth using shape collections. ACM Transactions on Graphics (TOG), 33(4):37, 2014. 3\n\n[18] A. Toshev, B. Taskar, and K. Daniilidis. Shape-based object detection via boundary structure segmentation. International journal of computer vision, 99(2):123\u2013146, 2012. 3\n\n[19] S. Tulsiani, H. Su, L. J. Guibas, A. A. Efros, and J. Malik. Learning shape abstractions by assembling volumetric prim- itives. arXiv preprint arXiv:1612.00404, 2016. 2, 3, 7\n\n[20] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural networks for matlab. In Proceedings of the 23rd ACM inter- national conference on Multimedia, pages 689\u2013692. ACM, 2015. 5\n\n[21] Y. Wang, K. Xu, J. Li, H. Zhang, A. Shamir, L. Liu, Z. Cheng, and Y. Xiong. Symmetry hierarchy of man-made objects. Computer Graphics Forum, 30(2):287\u2013296, 2011. 5\n\n[22] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural In- formation Processing Systems, pages 82\u201390, 2016. 1, 3, 8\n\n[23] K. Xu, H. Zhang, D. Cohen-Or, and B. Chen. Fit and di- verse: set evolution for inspiring 3d shape galleries. ACM Transactions on Graphics (TOG), 31(4):57, 2012. 5\n\n[24] K. Xu, H. Zheng, H. Zhang, D. Cohen-Or, L. Liu, and Y. Xiong. Photo-inspired model-driven 3d object modeling. ACM Transactions on Graphics (TOG), 30(4):80, 2011. 3, 6, 8\n\n[25] Y. Zheng, X. Chen, M.-M. Cheng, K. Zhou, S.-M. Hu, and N. J. Mitra. Interactive images: Cuboid proxies for smart image manipulation. ACM Transactions on Graphics, 31(4):99:1\u201399:11, 2012. 1, 8\n\n[26] Y. Zheng, H. Fu, D. Cohen-Or, O. K.-C. Au, and C.-L. Tai. Component-wise controllers for structure-preserving shape manipulation. In Computer Graphics Forum, volume 30, pages 563\u2013572. Wiley Online Library, 2011. 5",
          "section": "conclusion",
          "distance": 0.23191207647323608
        },
        {
          "text": "2. Related work\n\nReconstructing 3D shapes from a single image has been a long-standing pursue in both vision and graphics \ufb01elds. Due to its ill-posedness, many priors and assumptions have\n\nbeen attempted, until the proliferation of high-capacity deep neural networks. We will focus only on those deep learning based models and categorize the fast-growing literature in three different dimensions.\n\nDepth estimation vs. 3D reconstruction. Depth estima- tion is perhaps the most straightforward solution for recov- ering 3D information from single images. Deep learning has been shown to be highly effective for depth estima- tion [3, 11]. Compared to depth estimation, reconstructing a full 3D model is much more challenging due to the re- quirement of reasoning about the unseen parts. The latter has to resort to shape or structure priors. Using deep neu- ral networks to encode shape priors of a speci\ufb01c category has received much attention lately, under the background of fast growing large 3D shape repositories [1]. Choy et al. [2] develop a 3D Recurrent Reconstruction Neural Network to generate 3D shapes in volumetric representation, given a single image as input. A point set generation network is pro- posed for generating from a 2D image a 3D shape in point cloud [4]. We are not aware of any previous works that can generate part-based structures directly from a single image.\n\nDiscriminative vs. Generative. For the task of 3D mod- eling from 2D images, discriminative models are learned to map an input image directly to the output 3D representation, either by a deep CNN for one-shot generation or a recurrent model for progressive generation [2]. The advantages of such approach include ease of training and high-quality re- sults. With the recent development of deep generative mod- els such as variational auto-encoder (VAE) [9], generative adversarial nets (GAN) [6] and their variants. Learning a generative model for 3D shape generation has gained ex- tensive research [22, 5, 13]. For generative models, the in- put image can be used to condition the sampling from the prede\ufb01ned parameter space or learned latent space [22, 5]. Generative models are known hard to train. For the task of cross-modality mapping, we opt to train a discriminative model with a moderate size of training data.\n\nGeometry reconstruction vs. Structure recovery. The existing works based on deep learning models mostly utilize volumetric 3D shape representation [22, 5]. Some notable exceptions include generating shapes in point clouds [4], cuboid primitives [19] and manifold surfaces [14]. How- ever, none of these representations contains structural in- formation of parts and part relations. Interestingly, struc- ture recovery is only studied with non-deep-learning ap- proaches [24, 17, 7]. This is largely because of the lack of a structural 3D shape representation suitable for deep neu- ral networks. Recently, Li et al. [13] propose to use recur- sive neural networks for structural representation learning,\n\nnicely addressing the encoding/decoding of arbitrary num- ber of shape parts and various types of part relations. Our method takes the advantage of this and integrate it into a cross-modality mapping architecture for structure recovery from an RGB image.",
          "section": "other",
          "distance": 0.23927223682403564
        }
      ],
      "themes": [
        "3D Reconstruction",
        "Symmetry-aware processing",
        "Classifier Training",
        "Convolutional Neural Networks (CNNs)",
        "RGB-D Data",
        "Geometric Network (GN)",
        "semantic segmentation"
      ],
      "methods": [
        "Generative Deep Learning",
        "Hierarchical Structure Parsing",
        "Loss Function",
        "Conditional Random Field (CRF)",
        "VGG-16 Initialization & Fine-Tuning with MatConvNet/VLFeat",
        "Individual controllers",
        "Gradient-based optimization",
        "Occupancy Prediction Network",
        "Skip connections",
        "Multi-scale encoding",
        "Hausdorff absolute error",
        "Feature Fusion",
        "Reconstruction error",
        "3D Shape Estimation",
        "Mask R-CNN",
        "Bayesian Inference"
      ],
      "domains": [],
      "strengths": [],
      "limitations": [],
      "citations": [
        "20",
        "9",
        "17",
        " 13",
        "5",
        "12",
        "14",
        "26",
        " 17",
        "4",
        "11",
        "18",
        "10",
        "6",
        " 11",
        "19",
        "25",
        "24",
        "15",
        "3",
        " 5",
        " 7",
        "21",
        "13",
        "16",
        "2",
        "23",
        "1",
        "7",
        "22",
        "8"
      ],
      "graph_data": {
        "paper_id": "1804.05469v1",
        "title": "Im2Struct: Recovering 3D Shape Structure from a Single RGB Image",
        "authors": [
          "Chengjie Niu",
          "Jun Li",
          "Kai Xu"
        ],
        "chunk_ids": [
          "1804.05469v1_chunk_0",
          "1804.05469v1_chunk_1",
          "1804.05469v1_chunk_2",
          "1804.05469v1_chunk_3",
          "1804.05469v1_chunk_4",
          "1804.05469v1_chunk_5",
          "1804.05469v1_chunk_6",
          "1804.05469v1_chunk_7",
          "1804.05469v1_chunk_8",
          "1804.05469v1_chunk_9"
        ],
        "chunk_sections": [
          "introduction",
          "methodology",
          "conclusion",
          "other"
        ],
        "chunk_citations": [],
        "themes": [
          "Symmetry-aware processing",
          "Geometric Network (GN)",
          "RGB-D Data",
          "Classifier Training",
          "semantic segmentation",
          "3D Reconstruction",
          "Convolutional Neural Networks (CNNs)"
        ],
        "methods": [
          "Feature Fusion",
          "Occupancy Prediction Network",
          "Multi-scale encoding",
          "Mask R-CNN",
          "Bayesian Inference",
          "Conditional Random Field (CRF)",
          "Individual controllers",
          "Generative Deep Learning",
          "Reconstruction error",
          "Skip connections",
          "Gradient-based optimization",
          "VGG-16 Initialization & Fine-Tuning with MatConvNet/VLFeat",
          "3D Shape Estimation",
          "Hausdorff absolute error",
          "Hierarchical Structure Parsing",
          "Loss Function"
        ],
        "domains": [
          "3D Computer Vision",
          "ImageNet"
        ],
        "theme_related_papers": [
          "2004.12989v2",
          "1810.13049v2",
          "1612.00603v2",
          "2111.03098v1",
          "2010.03592v1",
          "1704.02956v1",
          "1606.00373v2",
          "2401.04099v1",
          "2303.16509v2",
          "2105.05233v4",
          "2007.13215v1",
          "1503.06465v2",
          "1411.6387v2",
          "1612.00496v2"
        ],
        "method_related_papers": [
          "2401.04099v1",
          "2303.16509v2",
          "2111.03098v1",
          "2007.13215v1",
          "1810.13049v2",
          "1606.00373v2",
          "1411.6387v2",
          "2004.12989v2",
          "1612.00603v2",
          "1612.00496v2",
          "1503.06465v2",
          "2105.05233v4",
          "2010.03592v1",
          "1704.02956v1"
        ],
        "domain_related_papers": [
          "2401.04099v1",
          "2303.16509v2",
          "2111.03098v1",
          "2105.05233v4",
          "2010.03592v1",
          "2007.13215v1",
          "2004.12989v2",
          "1810.13049v2",
          "1704.02956v1",
          "1612.00603v2",
          "1612.00496v2",
          "1606.00373v2",
          "1503.06465v2",
          "1411.6387v2"
        ]
      },
      "chunk_count": 2,
      "min_distance": 0.23191207647323608,
      "content": "6. Conclusion\n\nWe have proposed a deep learning framework that di- rectly recovers 3D shape structures from single 2D images. Our network joins a structure masking network for decern- ing the object structure and a structure recovery network for inferring 3D cuboid structure. The recovered 3D struc- tures achieve both \ufb01delity with respect to the input image and plausibility as a 3D shape structure. To the best of our knowledge, our work is the \ufb01rst that recovers detailed 3D shape structures from single 2D images.\n\nOur method fails to recover structures for object cate- gories unseen from the training set. For such cases, it would be interesting to learn an incremental part assembler. Our method currently recovers 3D cuboids only but not the un- derlying part geometry. A worthy direction is to synthesize detailed part geometry matching the visual appearance of the input image. Another interesting topic is to study the profound correlation between 2D features and 3D structure, so as to achieve a more explainable 3D structure decoding.\n\nReferences\n\n[1] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 3\n\n[2] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d- r2n2: A uni\ufb01ed approach for single and multi-view 3d ob- ject reconstruction. In European Conference on Computer Vision, pages 628\u2013644. Springer, 2016. 1, 3\n\n[3] D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolu- tional architecture. In Proceedings of the IEEE International Conference on Computer Vision, pages 2650\u20132658, 2015. 3\n\n[4] H. Fan, H. Su, and L. Guibas. A point set generation net- work for 3d object reconstruction from a single image. arXiv preprint arXiv:1612.00603, 2016. 1, 3\n\n[5] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta. Learning a predictable and generative vector representation for objects. In European Conference on Computer Vision, pages 484\u2013499. Springer, 2016. 1, 3\n\n[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014. 3\n\n[7] Q. Huang, H. Wang, and V. Koltun. Single-view reconstruc- tion via joint analysis of image and shape collections. ACM Transactions on Graphics (TOG), 34(4):87, 2015. 3, 7\n\n[8] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri. 3d shape segmentation with projective convolutional networks. arXiv preprint arXiv:1612.02808, 2016. 1\n\n[9] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n\n[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012. 1\n\n[11] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab. Deeper depth prediction with fully convolutional residual networks. In 3D Vision (3DV), 2016 Fourth Interna- tional Conference on, pages 239\u2013248. IEEE, 2016. 3\n\n[12] J. Li, R. Klein, and A. Yao. A two-streamed network for estimating \ufb01ne-scaled depth maps from single rgb images. In ICCV, 2017. 3, 4, 6\n\n[13] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, and L. Guibas. Grass: Generative recursive autoencoders for shape structures. arXiv preprint arXiv:1705.02090, 2017. 2, 3, 4, 5\n\n[14] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, and R. Wang. 3d shape reconstruction from sketches via multi-view convo- lutional networks. arXiv preprint arXiv:1707.06375, 2017. 3\n\n[15] N. Mitra, M. Wand, H. R. Zhang, D. Cohen-Or, V. Kim, and Q.-X. Huang. Structure-aware shape processing. In SIG- GRAPH Asia 2013 Courses, page 1. ACM, 2013. 1\n\n[16] J. Shotton, A. Blake, and R. Cipolla. Contour-based learning for object detection. In Proc. ICCV, volume 1, pages 503\u2013 510. IEEE, 2005. 3\n\n[17] H. Su, Q. Huang, N. J. Mitra, Y. Li, and L. Guibas. Estimat- ing image depth using shape collections. ACM Transactions on Graphics (TOG), 33(4):37, 2014. 3\n\n[18] A. Toshev, B. Taskar, and K. Daniilidis. Shape-based object detection via boundary structure segmentation. International journal of computer vision, 99(2):123\u2013146, 2012. 3\n\n[19] S. Tulsiani, H. Su, L. J. Guibas, A. A. Efros, and J. Malik. Learning shape abstractions by assembling volumetric prim- itives. arXiv preprint arXiv:1612.00404, 2016. 2, 3, 7\n\n[20] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural networks for matlab. In Proceedings of the 23rd ACM inter- national conference on Multimedia, pages 689\u2013692. ACM, 2015. 5\n\n[21] Y. Wang, K. Xu, J. Li, H. Zhang, A. Shamir, L. Liu, Z. Cheng, and Y. Xiong. Symmetry hierarchy of man-made objects. Computer Graphics Forum, 30(2):287\u2013296, 2011. 5\n\n[22] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural In- formation Processing Systems, pages 82\u201390, 2016. 1, 3, 8\n\n[23] K. Xu, H. Zhang, D. Cohen-Or, and B. Chen. Fit and di- verse: set evolution for inspiring 3d shape galleries. ACM Transactions on Graphics (TOG), 31(4):57, 2012. 5\n\n[24] K. Xu, H. Zheng, H. Zhang, D. Cohen-Or, L. Liu, and Y. Xiong. Photo-inspired model-driven 3d object modeling. ACM Transactions on Graphics (TOG), 30(4):80, 2011. 3, 6, 8\n\n[25] Y. Zheng, X. Chen, M.-M. Cheng, K. Zhou, S.-M. Hu, and N. J. Mitra. Interactive images: Cuboid proxies for smart image manipulation. ACM Transactions on Graphics, 31(4):99:1\u201399:11, 2012. 1, 8\n\n[26] Y. Zheng, H. Fu, D. Cohen-Or, O. K.-C. Au, and C.-L. Tai. Component-wise controllers for structure-preserving shape manipulation. In Computer Graphics Forum, volume 30, pages 563\u2013572. Wiley Online Library, 2011. 5\n\n2. Related work\n\nReconstructing 3D shapes from a single image has been a long-standing pursue in both vision and graphics \ufb01elds. Due to its ill-posedness, many priors and assumptions have\n\nbeen attempted, until the proliferation of high-capacity deep neural networks. We will focus only on those deep learning based models and categorize the fast-growing literature in three different dimensions.\n\nDepth estimation vs. 3D reconstruction. Depth estima- tion is perhaps the most straightforward solution for recov- ering 3D information from single images. Deep learning has been shown to be highly effective for depth estima- tion [3, 11]. Compared to depth estimation, reconstructing a full 3D model is much more challenging due to the re- quirement of reasoning about the unseen parts. The latter has to resort to shape or structure priors. Using deep neu- ral networks to encode shape priors of a speci\ufb01c category has received much attention lately, under the background of fast growing large 3D shape repositories [1]. Choy et al. [2] develop a 3D Recurrent Reconstruction Neural Network to generate 3D shapes in volumetric representation, given a single image as input. A point set generation network is pro- posed for generating from a 2D image a 3D shape in point cloud [4]. We are not aware of any previous works that can generate part-based structures directly from a single image.\n\nDiscriminative vs. Generative. For the task of 3D mod- eling from 2D images, discriminative models are learned to map an input image directly to the output 3D representation, either by a deep CNN for one-shot generation or a recurrent model for progressive generation [2]. The advantages of such approach include ease of training and high-quality re- sults. With the recent development of deep generative mod- els such as variational auto-encoder (VAE) [9], generative adversarial nets (GAN) [6] and their variants. Learning a generative model for 3D shape generation has gained ex- tensive research [22, 5, 13]. For generative models, the in- put image can be used to condition the sampling from the prede\ufb01ned parameter space or learned latent space [22, 5]. Generative models are known hard to train. For the task of cross-modality mapping, we opt to train a discriminative model with a moderate size of training data.\n\nGeometry reconstruction vs. Structure recovery. The existing works based on deep learning models mostly utilize volumetric 3D shape representation [22, 5]. Some notable exceptions include generating shapes in point clouds [4], cuboid primitives [19] and manifold surfaces [14]. How- ever, none of these representations contains structural in- formation of parts and part relations. Interestingly, struc- ture recovery is only studied with non-deep-learning ap- proaches [24, 17, 7]. This is largely because of the lack of a structural 3D shape representation suitable for deep neu- ral networks. Recently, Li et al. [13] propose to use recur- sive neural networks for structural representation learning,\n\nnicely addressing the encoding/decoding of arbitrary num- ber of shape parts and various types of part relations. Our method takes the advantage of this and integrate it into a cross-modality mapping architecture for structure recovery from an RGB image."
    },
    {
      "id": "2111.03098v1",
      "chunks": [
        {
          "text": "3.3 Coarse-to-Fine 3D Reconstruction\n\nOur reconstruction module is based on a coarse-to-\ufb01ne shape representation, which consists of two components: coarse-level voxelization and \ufb01ne-level local PCA-SDF.\n\nCoarse-Level Voxelization. Based on the extracted 3D voxel features G, we \ufb01rst estimate a coarse-level voxelization \u02dcV by a speci\ufb01c branch. The coarse-level voxelization holistically represents the whole 3D surface with binary occupancy values, where the unoccupied voxels cover \u201cair\" in the scene, and occupied voxels can be either fully occupied ones inside the object, or voxels intersecting with the object\u2019s surface. For the occupied voxels of both types, we further reconstruct a \ufb01ne-level local shape via the local PCA-SDF.\n\nLocal PCA-SDF Shape Representation. Recent works such as DeepSDF [59] aims to learn global implicit functions to represent shapes (see Fig. 3(a)). However, representing the entire objects with a single latent code often results in loss of details, which limits its application for scene-level object reconstruction. DeepLS [30] represents 3D surfaces by a set of independent latent codes on a regular\n\n5\n\ngrid (see Fig. 3(b)). Each latent code zi, concatenated with any point location x, can be decoded into a SDF value si by the learned implicit network f: si = f(zi,x). However, this shape representation has two limitations. i) Inference is inef\ufb01cient (in the order of seconds) since every point of a test voxel (e.g., 2563 points) is required to be sent to f for SDF calculation, making it unsuitable for real-time applications. ii) Our key observation is that local voxels, either within an object instance or across different categories, share similar local shapes, e.g., voxels across a table\u2019s surface all have planar shapes. However, DeepLS treats voxels as independent training samples, without fully leveraging such local shape priors in training. To address these issues, we propose a novel local PCA-SDF shape representation, which represents each voxel shape as a linear combination of a set of implicit volumetric prototypes, leading to signi\ufb01cantly \ufb01ner reconstruction and 10\u00d7 faster inference speed than DeepLS (see Tab. 4).\n\nFormally, as shown in Fig. 3(c), for each occupied voxel V, we de\ufb01ne a regular lattice q \u2208 Rk\u00d7k\u00d7k\u00d73\n\nand compute their SDFs s \u2208 Rk\u00d7k\u00d7k\u00d71 toward the surface. By collecting SDFs of NS occupied voxels from the training surfaces, we apply Principal Component Analysis (PCA) to \ufb01nd lB (lB << k3) local shape bases, SB \u2208 Rk\u00d7k\u00d7k\u00d7lb. As such, given the learned SB and the latent code zi, any local shape Si of the underlying surface can be implicitly represented by Si = SBzi. The latent code zi for Si can be generated by the corresponding voxel feature Gi via zi = MLP(Gi). MLP is a mapping network, implemented with two fully-connected layers. By combining the contributions of all the occupied voxels, we can infer a global iso-surface from the SDF \ufb01eld. Similar to DeepLS [30], we apply a 1.5 times receptive \ufb01eld strategy to mitigate the inconsistent surface predictions at the voxel boundaries (Fig. 3(c)). Accordingly, during inference, we could apply average pooling to combine the SDF values for the boundary area. It is worth mentioning that our local PCA-SDF also allows reconstruction at resolutions higher than the one used during training by simply applying trilinear interpolation on the learned local shape bases SB.",
          "section": "other",
          "distance": 0.23839271068572998
        }
      ],
      "themes": [
        "PCA-SDF Shape Representation",
        "3D Reconstruction",
        "Multivariable calculus",
        "Cross-category generalization",
        "Synthetic dataset generation",
        "RGB-D Data",
        "Distance Metrics"
      ],
      "methods": [
        "Loss Function",
        "Gradient-based optimization",
        "Reconstruction error",
        "3D Shape Estimation",
        "Geometric Regression",
        "Variational Autoencoder (VAE)",
        "Categorical Cross Entropy Loss",
        "Scan2CAD dataset",
        "Camera superpixel processing",
        "Machine learning",
        "Generative Deep Learning",
        "Average Error",
        "Linear regression",
        "Multi-scale encoding",
        "3D IoU",
        "Feature Fusion",
        "Principal Component Analysis (PCA)",
        "Interpolation",
        "Distance Metrics"
      ],
      "domains": [],
      "strengths": [],
      "limitations": [],
      "citations": [
        "59",
        "30"
      ],
      "graph_data": {
        "paper_id": "2111.03098v1",
        "title": "Voxel-based 3D Detection and Reconstruction of Multiple Objects from a Single Image",
        "authors": [
          "Feng Liu",
          "Xiaoming Liu"
        ],
        "chunk_ids": [
          "2111.03098v1_chunk_0",
          "2111.03098v1_chunk_11",
          "2111.03098v1_chunk_10",
          "2111.03098v1_chunk_9",
          "2111.03098v1_chunk_8",
          "2111.03098v1_chunk_7",
          "2111.03098v1_chunk_6",
          "2111.03098v1_chunk_5",
          "2111.03098v1_chunk_4",
          "2111.03098v1_chunk_3",
          "2111.03098v1_chunk_2",
          "2111.03098v1_chunk_1"
        ],
        "chunk_sections": [
          "methodology",
          "other"
        ],
        "chunk_citations": [
          "2004.12989v2",
          "1810.13049v2"
        ],
        "themes": [
          "PCA-SDF Shape Representation",
          "Synthetic dataset generation",
          "Cross-category generalization",
          "Multivariable calculus",
          "RGB-D Data",
          "Distance Metrics",
          "3D Reconstruction"
        ],
        "methods": [
          "Linear regression",
          "Distance Metrics",
          "Generative Deep Learning",
          "Feature Fusion",
          "Interpolation",
          "Variational Autoencoder (VAE)",
          "Categorical Cross Entropy Loss",
          "Multi-scale encoding",
          "Geometric Regression",
          "Scan2CAD dataset",
          "Principal Component Analysis (PCA)",
          "3D IoU",
          "Average Error",
          "Reconstruction error",
          "Machine learning",
          "Gradient-based optimization",
          "3D Shape Estimation",
          "Camera superpixel processing",
          "Loss Function"
        ],
        "domains": [
          "3D Computer Vision",
          "Autonomous technology",
          "Vehicle detection"
        ],
        "theme_related_papers": [
          "2303.16509v2",
          "2105.05233v4",
          "2007.13215v1",
          "2004.12989v2",
          "1612.00603v2",
          "1612.00496v2",
          "1606.00373v2",
          "1503.06465v2",
          "1411.6387v2",
          "2010.03592v1",
          "1804.05469v1",
          "1704.02956v1",
          "2401.04099v1",
          "1810.13049v2"
        ],
        "method_related_papers": [
          "2010.03592v1",
          "1612.00496v2",
          "1411.6387v2",
          "2401.04099v1",
          "1612.00603v2",
          "1503.06465v2",
          "2303.16509v2",
          "2105.05233v4",
          "2007.13215v1",
          "2004.12989v2",
          "1810.13049v2",
          "1804.05469v1",
          "1704.02956v1",
          "1606.00373v2"
        ],
        "domain_related_papers": [
          "2401.04099v1",
          "2303.16509v2",
          "2105.05233v4",
          "2010.03592v1",
          "2007.13215v1",
          "2004.12989v2",
          "1810.13049v2",
          "1804.05469v1",
          "1704.02956v1",
          "1612.00603v2",
          "1612.00496v2",
          "1606.00373v2",
          "1503.06465v2",
          "1411.6387v2"
        ]
      },
      "chunk_count": 1,
      "min_distance": 0.23839271068572998,
      "content": "3.3 Coarse-to-Fine 3D Reconstruction\n\nOur reconstruction module is based on a coarse-to-\ufb01ne shape representation, which consists of two components: coarse-level voxelization and \ufb01ne-level local PCA-SDF.\n\nCoarse-Level Voxelization. Based on the extracted 3D voxel features G, we \ufb01rst estimate a coarse-level voxelization \u02dcV by a speci\ufb01c branch. The coarse-level voxelization holistically represents the whole 3D surface with binary occupancy values, where the unoccupied voxels cover \u201cair\" in the scene, and occupied voxels can be either fully occupied ones inside the object, or voxels intersecting with the object\u2019s surface. For the occupied voxels of both types, we further reconstruct a \ufb01ne-level local shape via the local PCA-SDF.\n\nLocal PCA-SDF Shape Representation. Recent works such as DeepSDF [59] aims to learn global implicit functions to represent shapes (see Fig. 3(a)). However, representing the entire objects with a single latent code often results in loss of details, which limits its application for scene-level object reconstruction. DeepLS [30] represents 3D surfaces by a set of independent latent codes on a regular\n\n5\n\ngrid (see Fig. 3(b)). Each latent code zi, concatenated with any point location x, can be decoded into a SDF value si by the learned implicit network f: si = f(zi,x). However, this shape representation has two limitations. i) Inference is inef\ufb01cient (in the order of seconds) since every point of a test voxel (e.g., 2563 points) is required to be sent to f for SDF calculation, making it unsuitable for real-time applications. ii) Our key observation is that local voxels, either within an object instance or across different categories, share similar local shapes, e.g., voxels across a table\u2019s surface all have planar shapes. However, DeepLS treats voxels as independent training samples, without fully leveraging such local shape priors in training. To address these issues, we propose a novel local PCA-SDF shape representation, which represents each voxel shape as a linear combination of a set of implicit volumetric prototypes, leading to signi\ufb01cantly \ufb01ner reconstruction and 10\u00d7 faster inference speed than DeepLS (see Tab. 4).\n\nFormally, as shown in Fig. 3(c), for each occupied voxel V, we de\ufb01ne a regular lattice q \u2208 Rk\u00d7k\u00d7k\u00d73\n\nand compute their SDFs s \u2208 Rk\u00d7k\u00d7k\u00d71 toward the surface. By collecting SDFs of NS occupied voxels from the training surfaces, we apply Principal Component Analysis (PCA) to \ufb01nd lB (lB << k3) local shape bases, SB \u2208 Rk\u00d7k\u00d7k\u00d7lb. As such, given the learned SB and the latent code zi, any local shape Si of the underlying surface can be implicitly represented by Si = SBzi. The latent code zi for Si can be generated by the corresponding voxel feature Gi via zi = MLP(Gi). MLP is a mapping network, implemented with two fully-connected layers. By combining the contributions of all the occupied voxels, we can infer a global iso-surface from the SDF \ufb01eld. Similar to DeepLS [30], we apply a 1.5 times receptive \ufb01eld strategy to mitigate the inconsistent surface predictions at the voxel boundaries (Fig. 3(c)). Accordingly, during inference, we could apply average pooling to combine the SDF values for the boundary area. It is worth mentioning that our local PCA-SDF also allows reconstruction at resolutions higher than the one used during training by simply applying trilinear interpolation on the learned local shape bases SB."
    },
    {
      "id": "2401.04099v1",
      "chunks": [
        {
          "text": "2.3. Explicit 3D Representations\n\nExplicit 3D representations have been widely studied for decades. Many works have attempted to construct 3D assets through 3D voxels [9, 16, 54], point clouds [13, 22, 49, 55, 62] and meshes [15, 18, 48]. Since pure implicit radiance fields are operationally slow, often needing millions of neu- ral network queries to render large-scale scenes, great ef- forts have been put into integrating explicit representations with implicit radiance fields to combine their advantages. Many works looked into employing tensor factorization to achieve efficient explicit representations [5, 6, 14, 31, 58]. Multi-scale hash grids [31] and block decomposition [43] are introduced to extend to city-scale scenes. Another pop- ular direction focuses on empowering explicit structures with additional implicit attributes. Point NeRF [53] utilizes neural 3D points to represent and render a continuous radiance volume. Similarly, NU-MCC [22] uses latent point features, specifically focusing on shape completion tasks. 3D Gaussian splatting [20] introduces point-based \u03b1-blending along with an efficient point-based rasterizer and has attracted great attention due to their outstanding ability in reconstruction [20] and generation [8, 44, 59] tasks. However, existing works present 3D Gaussians through optimization, while our work takes one step further and focuses on building a amortized framework that generates 3D Gaussians without per-instance optimization.\n\n3. Amortized Generative 3D Gaussians\n\nUnlike the existing methods that optimize Gaussians, initialized from structure-from-motion (SfM) or random blobs, our work pursues a more challenging objective: gen- erating 3D Gaussians from a single image using a neural\n\nnetwork in one shot. Specifically, our cascaded approach begins with constructing a hybrid generator that maps input image features into a concise set of 3D Gaussian attributes. Subsequently, a UNet architecture with point-voxel layers is employed to super-resolve the 3D Gaussian representa- tion, improving its fidelity. In the following subsections, we first provide the background on 3D Gaussian splatting, then we delve deep into details about our proposed compo- nents, and finally, we illustrate how we address the rising challenges with amortized generation.",
          "section": "other",
          "distance": 0.24312720929465248
        }
      ],
      "themes": [
        "Matrix factorization",
        "Attention mechanisms",
        "Neural network architectures",
        "3D Reconstruction",
        "Waveform generation pipeline",
        "Diffusion Models",
        "Tri-hybrid Representation",
        "Data-driven view consistency",
        "Reflectance Estimation",
        "Surface Normal",
        "Classifier Training",
        "Convolutional Neural Networks (CNNs)",
        "Super-resolution techniques",
        "setup",
        "Upsampling",
        "Distance Metrics",
        "Parameter optimization"
      ],
      "methods": [
        "Orientation Estimation",
        "grid-based renderer",
        "Sparse Representation",
        "Gradient-based optimization",
        "Data augmentation",
        "diffusion term",
        "Reconstruction error",
        "3D Shape Estimation",
        "Matrix factorization",
        "Intrinsic Image Decomposition",
        "Surface Normal Optimization",
        "Transformation layers",
        "Diffusion Models",
        "Attention Mechanisms",
        "Hybrid generation pipeline",
        "3D-R2N2",
        "Fused Adam GeLU ops",
        "Generative Deep Learning",
        "Point-Voxel UNet",
        "NeRF",
        "Multi-scale encoding",
        "Feature Fusion",
        "Transformer-based models",
        "Multivariate Gaussian modeling",
        "Principal Component Analysis (PCA)",
        "Distance Metrics",
        "Sampling techniques"
      ],
      "domains": [],
      "strengths": [],
      "limitations": [],
      "citations": [
        "20",
        "9",
        "53",
        "5",
        "31",
        " 58",
        " 18",
        " 44",
        " 49",
        "43",
        " 31",
        " 6",
        " 22",
        "15",
        " 54",
        " 55",
        "13",
        " 59",
        " 62",
        " 14",
        "22",
        " 16",
        " 48",
        "8"
      ],
      "graph_data": {
        "paper_id": "2401.04099v1",
        "title": "AGG: Amortized Generative 3D Gaussians for Single Image to 3D",
        "authors": [
          "Dejia Xu",
          "Ye Yuan",
          "Morteza Mardani",
          "Sifei Liu",
          "Jiaming Song",
          "Zhangyang Wang",
          "Arash Vahdat"
        ],
        "chunk_ids": [
          "2401.04099v1_chunk_0",
          "2401.04099v1_chunk_13",
          "2401.04099v1_chunk_12",
          "2401.04099v1_chunk_11",
          "2401.04099v1_chunk_10",
          "2401.04099v1_chunk_9",
          "2401.04099v1_chunk_8",
          "2401.04099v1_chunk_7",
          "2401.04099v1_chunk_6",
          "2401.04099v1_chunk_5",
          "2401.04099v1_chunk_4",
          "2401.04099v1_chunk_3",
          "2401.04099v1_chunk_2",
          "2401.04099v1_chunk_1"
        ],
        "chunk_sections": [
          "introduction",
          "other"
        ],
        "chunk_citations": [
          "2303.16509v2",
          "1612.00603v2"
        ],
        "themes": [
          "Tri-hybrid Representation",
          "Neural network architectures",
          "Super-resolution techniques",
          "Diffusion Models",
          "Attention mechanisms",
          "Upsampling",
          "Parameter optimization",
          "Distance Metrics",
          "Matrix factorization",
          "Surface Normal",
          "Classifier Training",
          "Data-driven view consistency",
          "3D Reconstruction",
          "Reflectance Estimation",
          "Waveform generation pipeline",
          "Convolutional Neural Networks (CNNs)",
          "setup"
        ],
        "methods": [
          "Surface Normal Optimization",
          "3D-R2N2",
          "Distance Metrics",
          "Matrix factorization",
          "Generative Deep Learning",
          "diffusion term",
          "Data augmentation",
          "Sampling techniques",
          "Sparse Representation",
          "Feature Fusion",
          "Transformation layers",
          "Fused Adam GeLU ops",
          "Diffusion Models",
          "grid-based renderer",
          "Intrinsic Image Decomposition",
          "NeRF",
          "Hybrid generation pipeline",
          "Multi-scale encoding",
          "Point-Voxel UNet",
          "Principal Component Analysis (PCA)",
          "Orientation Estimation",
          "Transformer-based models",
          "Attention Mechanisms",
          "Reconstruction error",
          "Gradient-based optimization",
          "3D Shape Estimation",
          "Multivariate Gaussian modeling"
        ],
        "domains": [
          "3D Computer Vision",
          "Autonomous technology",
          "ImageNet"
        ],
        "theme_related_papers": [
          "2303.16509v2",
          "2004.12989v2",
          "2105.05233v4",
          "1606.00373v2",
          "1411.6387v2",
          "2010.03592v1",
          "1612.00603v2",
          "2111.03098v1",
          "1503.06465v2",
          "2007.13215v1",
          "1704.02956v1",
          "1804.05469v1",
          "1810.13049v2",
          "1612.00496v2"
        ],
        "method_related_papers": [
          "2105.05233v4",
          "2010.03592v1",
          "2007.13215v1",
          "1704.02956v1",
          "1411.6387v2",
          "2303.16509v2",
          "2004.12989v2",
          "2111.03098v1",
          "1612.00603v2",
          "1503.06465v2",
          "1810.13049v2",
          "1804.05469v1",
          "1612.00496v2",
          "1606.00373v2"
        ],
        "domain_related_papers": [
          "2303.16509v2",
          "2111.03098v1",
          "2105.05233v4",
          "2010.03592v1",
          "2007.13215v1",
          "2004.12989v2",
          "1810.13049v2",
          "1804.05469v1",
          "1704.02956v1",
          "1612.00603v2",
          "1612.00496v2",
          "1606.00373v2",
          "1503.06465v2",
          "1411.6387v2"
        ]
      },
      "chunk_count": 1,
      "min_distance": 0.24312720929465248,
      "content": "2.3. Explicit 3D Representations\n\nExplicit 3D representations have been widely studied for decades. Many works have attempted to construct 3D assets through 3D voxels [9, 16, 54], point clouds [13, 22, 49, 55, 62] and meshes [15, 18, 48]. Since pure implicit radiance fields are operationally slow, often needing millions of neu- ral network queries to render large-scale scenes, great ef- forts have been put into integrating explicit representations with implicit radiance fields to combine their advantages. Many works looked into employing tensor factorization to achieve efficient explicit representations [5, 6, 14, 31, 58]. Multi-scale hash grids (uller, 2201) and block decomposition (Tancik, 8248) are introduced to extend to city-scale scenes. Another pop- ular direction focuses on empowering explicit structures with additional implicit attributes. Point NeRF (Xu, 5438) utilizes neural 3D points to represent and render a continuous radiance volume. Similarly, NU-MCC (Lionar, 2307) uses latent point features, specifically focusing on shape completion tasks. 3D Gaussian splatting (Kerbl, 2023) introduces point-based \u03b1-blending along with an efficient point-based rasterizer and has attracted great attention due to their outstanding ability in reconstruction (Kerbl, 2023) and generation [8, 44, 59] tasks. However, existing works present 3D Gaussians through optimization, while our work takes one step further and focuses on building a amortized framework that generates 3D Gaussians without per-instance optimization.\n\n3. Amortized Generative 3D Gaussians\n\nUnlike the existing methods that optimize Gaussians, initialized from structure-from-motion (SfM) or random blobs, our work pursues a more challenging objective: gen- erating 3D Gaussians from a single image using a neural\n\nnetwork in one shot. Specifically, our cascaded approach begins with constructing a hybrid generator that maps input image features into a concise set of 3D Gaussian attributes. Subsequently, a UNet architecture with point-voxel layers is employed to super-resolve the 3D Gaussian representa- tion, improving its fidelity. In the following subsections, we first provide the background on 3D Gaussian splatting, then we delve deep into details about our proposed compo- nents, and finally, we illustrate how we address the rising challenges with amortized generation."
    }
  ],
  "images": [
    {
      "path": "output\\images\\07bba42c-f096-488f-9b71-c826c89315c7.jpg",
      "paper_id": "1606.00373v2",
      "description": "This figure illustrates a comparison of 3D reconstructions generated by different models: AlexNet, VGG, a proposed model, and the ground truth. Each sub-image represents the output of one approach applied to the same scene, providing a visual evaluation of performance. The proposed model appears to produce a reconstruction closer to the ground truth, suggesting its effectiveness over the other models. This figure is important for understanding the comparative performance and accuracy of the prop...",
      "themes": [
        "Synthetic dataset generation",
        "Conditional Random Fields (CRFs)",
        "Regularization",
        "Loss functions",
        "Diffusion Models",
        "upsampling",
        "Upsampling",
        "RGB-D Data",
        "Geometric inference",
        "3D Reconstruction",
        "Super-resolution techniques",
        "Convolutional Neural Networks (CNNs)",
        "Photometric loss"
      ],
      "methods": [
        "Feature Fusion",
        "Interpolation",
        "3D projection",
        "Probabilistic Graphical Models",
        "Data augmentation",
        "Transformation layers",
        "ResNet-based architectures",
        "Conditional Random Field (CRF)",
        "Generative Deep Learning",
        "Machine learning",
        "VGG-16 Initialization & Fine-Tuning with MatConvNet/VLFeat",
        "Gradient-based optimization",
        "3D Shape Estimation",
        "Camera superpixel processing",
        "Loss Function",
        "Sampling techniques"
      ],
      "domains": [],
      "distance": 0.21588683128356934
    },
    {
      "path": "output\\images\\5595bf9d-d30e-4f8a-b472-fb595cfadc9e.jpg",
      "paper_id": "1612.00603v2",
      "description": "This figure presents a comparative visualization of 3D object reconstruction from single input images. The columns illustrate different stages and methods of reconstruction:\n\n- **Input Image**: The original 2D image used as the basis for reconstruction.\n- **Ours**: The initial 3D reconstruction result from the authors' proposed method.\n- **Ours (post-processed)**: The refined 3D model after applying post-processing techniques to enhance the reconstruction.\n- **Ground Truth**: The actual, accurat...",
      "themes": [
        "Synthetic dataset generation",
        "unordered set generation",
        "Dense branch",
        "Loss functions",
        "Diffusion Models",
        "Geometric Network (GN)",
        "RGB-D Data",
        "Distance Metrics",
        "Constraint-based modeling",
        "Geometric inference",
        "Quality assessment",
        "3D evaluation metric",
        "3D Reconstruction",
        "measurement uncertainty",
        "Convolutional Neural Networks (CNNs)"
      ],
      "methods": [
        "Transformation layers",
        "Interpolation",
        "Occupancy Prediction Network",
        "Two-pass bootstrap",
        "Variational Autoencoder (VAE)",
        "Point-Voxel UNet",
        "Blinn-Phong shading",
        "Generative Deep Learning",
        "Distance Metrics",
        "Machine learning",
        "Epsilon-scaling.",
        "3D Shape Estimation",
        "Hourglass architecture",
        "Loss Function",
        "Sampling techniques"
      ],
      "domains": [],
      "distance": 0.22101229429244995
    },
    {
      "path": "output\\images\\b2a47eca-6118-4c5d-b5b3-ec9449ef4775.jpg",
      "paper_id": "1612.00603v2",
      "description": "This figure illustrates a process for reconstructing 3D point clouds from 2D images. The left column shows the input images, which include a car and a bottle. The right columns display the corresponding reconstructed 3D point clouds for each object. This visualization demonstrates the capability of a method or algorithm to transform 2D visual data into a 3D representation, highlighting its effectiveness in capturing the shape and structure of different objects. The figure is important as it visu...",
      "themes": [
        "Synthetic dataset generation",
        "unordered set generation",
        "Dense branch",
        "Loss functions",
        "Diffusion Models",
        "Geometric Network (GN)",
        "RGB-D Data",
        "Distance Metrics",
        "Constraint-based modeling",
        "Geometric inference",
        "Quality assessment",
        "3D evaluation metric",
        "3D Reconstruction",
        "measurement uncertainty",
        "Convolutional Neural Networks (CNNs)"
      ],
      "methods": [
        "Transformation layers",
        "Interpolation",
        "Occupancy Prediction Network",
        "Two-pass bootstrap",
        "Variational Autoencoder (VAE)",
        "Point-Voxel UNet",
        "Blinn-Phong shading",
        "Generative Deep Learning",
        "Distance Metrics",
        "Machine learning",
        "Epsilon-scaling.",
        "3D Shape Estimation",
        "Hourglass architecture",
        "Loss Function",
        "Sampling techniques"
      ],
      "domains": [],
      "distance": 0.23725593090057373
    },
    {
      "path": "output\\images\\915321dc-4b45-4253-ae51-c1e59a7a396e.jpg",
      "paper_id": "2111.03098v1",
      "description": "This figure illustrates the architecture and process flow of a 3D reconstruction system from 2D images. It is crucial for understanding the methodology used in the research. The figure is divided into several key components:\n\n- **Image Input:** Starts with an input image \\( I \\) with dimensions \\( W \\times H \\times 3 \\).\n\n- **Feature Backbone:** Extracts feature maps \\( F \\) from the input image, transforming it into a higher-dimensional space \\( W_F \\times H_F \\times D \\).\n\n- **2D-to-3D Feature...",
      "themes": [
        "PCA-SDF Shape Representation",
        "Synthetic dataset generation",
        "Cross-category generalization",
        "Multivariable calculus",
        "RGB-D Data",
        "Distance Metrics",
        "3D Reconstruction"
      ],
      "methods": [
        "Linear regression",
        "Distance Metrics",
        "Generative Deep Learning",
        "Feature Fusion",
        "Interpolation",
        "Variational Autoencoder (VAE)",
        "Categorical Cross Entropy Loss",
        "Multi-scale encoding",
        "Geometric Regression",
        "Scan2CAD dataset",
        "Principal Component Analysis (PCA)",
        "3D IoU",
        "Average Error",
        "Reconstruction error",
        "Machine learning",
        "Gradient-based optimization",
        "3D Shape Estimation",
        "Camera superpixel processing",
        "Loss Function"
      ],
      "domains": [],
      "distance": 0.2516941036548037
    },
    {
      "path": "output\\images\\09cdd616-142f-42a9-9846-156b437a079f.jpg",
      "paper_id": "2010.03592v1",
      "description": "This figure presents a comparative analysis of different methods for reconstructing 3D shapes from images under various lighting conditions. It is divided into three sections based on the type of image and lighting: Grayscale Image with Laboratory Illumination, Color Image with Laboratory Illumination, and Color Image with Natural Illumination.\n\nEach section includes the following columns:\n- **Image**: The input image used for reconstruction.\n- **Shape**: The 3D shape reconstruction results.\n- *...",
      "themes": [
        "Loss functions",
        "boundary",
        "Mean Value",
        "Diffusion Models",
        "contour detection",
        "Parameter optimization",
        "RGB-D Data",
        "Smoothness priors",
        "Matrix factorization",
        "Surface Normal",
        "Geometric inference",
        "Quality assessment",
        "3D Reconstruction",
        "Reflectance Estimation",
        "Color constancy",
        "Ambiguous Bas-Relief",
        "Photometric loss"
      ],
      "methods": [
        "Histogram-based signal entropy calculation",
        "Surface Normal Optimization",
        "Iterative guided morphable model fitting",
        "Linear regression",
        "Simple Baseline",
        "Generative Deep Learning",
        "Hausdorff absolute error",
        "Sparse Representation",
        "Gradient matrix",
        "Retinex",
        "Interpolation",
        "Expectation-Maximization (EM)",
        "Geometric prior",
        "pre-calculating",
        "Intrinsic Image Decomposition",
        "Quadratic Parzen window",
        "SIRFS-based shape estimation",
        "Cost aggregation",
        "Geometric Regression",
        "Bayesian Inference",
        "Grid relaxation",
        "Principal Component Analysis (PCA)",
        "Mean curvature variation",
        "Average Error",
        "Sparse feature selection",
        "Gradient-based optimization",
        "3D Shape Estimation",
        "Multivariate Gaussian modeling",
        "Optimization algorithms"
      ],
      "domains": [],
      "distance": 0.26136314868927
    }
  ]
}